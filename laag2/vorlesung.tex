\documentclass[a4paper, 10pt]{scrbook}
\usepackage{mathe-vorlesung}
\usepackage[all]{xy}
\usepackage{array}
%\usepackage[a4paper,left=1 cm,right= 1cm, top=1cm, bottom=2cm] {geometry}

% Für Text zwischen enumerate-items aber ohne Bezug zu einem bestimmten item.
\makeatletter
\newcommand{\interitemtext}[1]{%
\begin{list}{}
{\itemindent=0mm\labelsep=0mm
\labelwidth=0mm\leftmargin=0mm
\addtolength{\leftmargin}{-8mm}}
\item #1
\end{list}}
\makeatother

\ifthenelse{\isundefined{\psmallmatrix}}{
	\newenvironment{psmallmatrix}{\left(\begin{smallmatrix}}{\end{smallmatrix}\right)}
}{}


\title{Lineare Algebra und Analytische Geometrie 2}
\author{}
\date{\today}

\begin{document}

\maketitle
\tableofcontents

\setcounter{section}{7}
\chapter{Inhomogene lineare Gleichungssysteme und affine Räume}

\begin{df}[Inhomogenes lineares Gleichungssystem] 
	\label{df: 8.1}
	Sei $A$ eine $m\times n$-Matrix und $b$ ein $m\times 1$-Vektor.
	Man nennt
	\[
		Ax = b
	\]
	ein \emph{inhomogenes lineares Gleichungssystem}.
	\begin{note}
		Falls $b\ne 0$, dann
		\begin{enumerate}[a)]
			\item muss keine Lösung existieren (z.B. $A=0, b\ne 0$)
			\item ist $x=0$ nie eine Lösung
		\end{enumerate}
	\end{note}
\end{df}

\begin{df}[Affiner Unterraum] 
	\label{df: 8.2}
	Sei $V$ ein $K$-Vektorraum und $W<V$ ein Unterraum von $V$.
	Eine Teilmenge $X\subset V$ heißt \emph{affiner Unterraum} von $V$ genau dann, wenn
	\[
		X=\emptyset \quad \lor \quad \exists v_0\in V:X=v_0+W=\{x\in V | \exists w\in W:x=v_0+w\}
	\]
	\begin{ex}[Gerade im $\R^2$]
		\[ g=\{(u_1,u_2)+\lambda(w_1,w_2):\lambda\in\R\} \]
		$g$ ist affiner Unterraum von $\R^2$.
	\end{ex}
\end{df}

\begin{df}[Affiner Raum] \label{df: 8.3}
Sei K ein Körper und V ein K-Vektorraum. Eine Menge $A \neq \emptyset$ heißt \textit{affiner Raum über K bezüglich V}
genau dann, wenn eine Funktion $V \times A \rightarrow A, (x,p) \mapsto p+x$ (Verschiebung eines Punktes um einen Vektor) existiert, so dass gilt:
\begin{enumerate}
\item[(A1)] \label{itm: aff1} $\forall x,y \in V, p \in A : p+(x+y)=(p+x)+y \qquad $
\item[(A2)] \label{itm: aff2} $\forall p \in A : p+0=p \qquad $
\item[(A3)] \label{itm: aff3} $\forall x \in V, p \in A: p+x=p \implies x = 0$
\item[(A4)] \label{itm: aff4} $\forall p, q \in A \;\exists x\in V : q=p+x$
\end{enumerate}
$A=\emptyset$ wird ebenfalls als affiner Raum bezeichnet.
\end{df}


\begin{ex}
\begin{itemize}
\item Eine Gerade $g$ in $\mathbb R ^2$ ist ein affiner Raum bezüglich $\R$.
\item Jeder affine Unterraum ist ein affiner Raum

	\begin{proof}
		Sei $V$ ein $K$-Vektorraum, $U$ ein Unterraum von $V$, $W:=V/U$ und $A = v_0 + U \subset W$ ein affiner Unterraum.

Definiere die Operation $+: U\times A\to A,\; (x,p)\mapsto p+x$ für $p = v_0 + u$ mit $u\in U$ durch
\[
p+x := v_0 + (u+x) 
\]
Sie ist wohldefiniert, denn $x,u\in U$, also auch $x+u\in U$.

$A$ mit dieser Funktion erfüllt die Axiome, denn für $p=v_0+u_1\in A, x,y\in V$ gilt:
\begin{enumerate}[{A}1:]
	\item $p+(x+y) = (v_0+u)+(x+y) = ((v_0+u)+x)+y = (p+x)+y$
	\item $p+0 = (v_0+u)+0=v_0+(u+0)=v_0+u=p$
	\item $v_0+u+x=p+x=p=v_0+u \implies x=0$
	\item Sei $p=v_0+u_1, q=v_0+u_2$ und wähle $x:= u_2-u_1$, dann ist
		\[
			p+x = v_0+u_1+u_2-u_1 = v_0+u_2 = q
		\]
\end{enumerate}
\end{proof}

\item Jeder Vektorraum ist selbst ein affiner Raum

Sei $V$ ein $K$-Vektorraum, $A := V$. Definiere die Operation
\[
V\times A \rightarrow A : (x,p) \rightarrow p+x
\]
als die Summe der Vektoren $x$ und $p$ in $V=A$.
\end{itemize}
\end{ex}


\begin{prop} 
	\label{prop:8.4}
Seien $A$ und $B$ affine Räume über $K$ bezüglich $V$.
Dann existiert eine bijektive Abbildung $f:A\rightarrow B$ mit 
\[
	f(p+x) = f(p)+x \qquad \forall p\in A, x\in V
\]
Insbesondere haben alle affinen Räume bezüglich $V$ gleich viele Elemente wie $V$.
\begin{proof}
	\begin{seg}{Eindeutige Darstellung von $p=v_0+x$}
		Seien $p=v_0+x=v_0+y$ zwei Darstellungen von $p$, dann ist
		\[
			v_0+(x-y) \stackrel{A1}= (v_0+x)-y = p-y = (a+y)-y \stackrel{A1}= v_0+(y-y) = v_0
		\]
		Also ist nach A3 $x-y=0$ und $x=y$.
	\end{seg}
	Seien $A\neq \emptyset \neq B$ und wähle beliebige, feste $a\in A, b\in B$.
	Für $p=a+v\in A$ definieren wir $f:A\to B$ durch
	\begin{align*}
		f(a) &:= b\\
		f(p) &:= b +v
	\end{align*}
	$f$ ist wohldefiniert, da $b,b+v\in B$ und die Darstellung $p=a+v$ eindeutig ist (siehe oben).
	Die gewünschte Eigenschaft $f(p+x)=f(p)+x$ ist ebenfalls erfüllt:
	\[
		f(p+x) = f(a+(v+x)) = b + (v+x) = (b+v)+x = f(p) + x
	\]
	Es bleibt zu zeigen, dass $f$ bijektiv ist.

	\begin{seg}{$f$ surjektiv}
		Sei $d\in B$ und $d=b+y$ mit eindeutigem $y$.
		Setze $c := a+y$, dann ist
		\[
			f(c) = f(a+y) = f(a)+y = b+y = d
		\]
	\end{seg}
	\begin{seg}{$f$ injektiv}
		Sei $p=a+x,q=a+y\in A$ und $f(p)=f(q)$.
		Es gilt
		\[
			b + x = f(a+x) = f(p) = f(q) = f(a+y) = b+y
		\]
		Also $x=y$ und damit $p=q$.
	\end{seg}
\end{proof}
\begin{note}
	Für den Spezialfall $A=V$, besagt die Proposition, dass wir einen Isomorphismus zwischen jedem affinen Raum $B$ und seinem zugrundeliegenden Vektorraum $V$ finden.
	$B$ ist sozusagen $V$ an $b$ „verschoben“.
\end{note}
\end{prop}

Da wir gezeigt haben, dass für festes $a$ der Ausdruck $p=a+x$ den Vektor $x$ eindeutig bestimmt, führen wir eine neue Notation ein.

\begin{df*}
	Sei $A$ ein affiner Raum bezüglich einem Vektorraum $V$.
	Für $a,p\in A, x\in V$ und $p=a+x$ bezeichnen wir
	\[
		\vec{ap} = x
	\]
	auch als \emph{Verbindungsvektor} zwischen $a$ und $p$.
\end{df*}

Wir können jetzt Koordinaten in einem affinen Raum $A$ bezüglich $V$ einführen.

Man wähle einen Basispunkt $p_0\in A$ und eine Basis $v_1,\dotsc,v_n$ von $V$.
Setze jetzt $p_i := p_0 + v_i$.

Ein Punkt $p = p_0 +x\in A$ ist eindeutig bestimmt durch
\[
	p = p_0 + x = p_0 + \sum_{i=1}^n\lambda_i v_i = p_0 + \sum_{i=1}^n\lambda_i \vec{p_0p_i}
\]
Bezüglich dem Basispunkt $p_0$ und der Basis $\vec{p_0p_1},\dotsc,\vec{p_0p_n}$ ist $p$ damit eindeutig durch den Koordinatenvektor $(\lambda_1,\dotsc,\lambda_n)$ bestimmt.

Das affine Koordinatensystem besteht auch dem Anfangspunkt $p_0$ und den Einheitspunkten $p_1,\dotsc,p_n$.

\begin{df}
	\label{df: 8.5}
Sei $A$ affiner Raum bezüglich $V$. 
Eine Teilmenge $\emptyset \neq A_0 \subset A$ heißt \emph{affiner Teilraum} von $A$ genau dann, wenn $p \in A_0$ und ein Untervektorraum $U\subset V$ existieren, so dass 
\[
	A_0=p + U = \{q\in A\big| \exists x\in U: q=p+x\}
\]
Außerdem ist $A_0 = \emptyset$ auch eine affine Teilmenge.
\end{df}


\begin{lem} 
	\label{df: 8.6}
Sei $A$ ein affiner Raum mit affinen Teilräumen $A_0=p_0+V_0$ und $A_1=p_1+V_1$.
\begin{enumerate}[(a)]
\item Für $q\in A$ gilt: $q\in A_0 \iff q+V_0 = p_0+V_0$
\item $V_0=\{\vec{p_0q}:q\in A_0\}$, d.h. $V_0$ ist durch $A_0$ bestimmt
\item $\forall q\in A, x\in V_0 :\; q+x\in A_0 \implies q\in A_0$
\item $A_0=A_1 \iff (V_0=V_1 \land p_0\in A_1)$
\item $A_0$ ist selbst ein affiner Raum über $K$ bezüglich $V_0$.
\end{enumerate}
\begin{proof}
\begin{enumerate}[(a)]
\item
	Sei $q\in A_0 = p+V_0 \implies \exists x\in V_0: q=p+x$.
	\begin{align*}
		q+V_0=\{q+y:y\in V_0\}&=\{(p+x)+y:y\in V_0\}\\
	&=\{p+(x+y):y\in V_0\}=\{p+z:z\in V_0\}=p+V_0
	\end{align*}
	Sei $A=p+V_0=q+V_0\ni q=q+0 \implies q\in A_0$
\item
	Sei $x\in V_0$, also auch $p_0+x\in A_0$.
	Wir finden ein $q\in A_0$, sodass $\vec{p_0q}=x$ ist, also ist $x\in \{\vec{pq}:q\in A_0\}$

	Sei $q\in A_0$, also auch $q=p+\vec{pq}\in A_0$.
	Da $A_0 =\{p_0+x:x\in V_0\}$ mit jeweils eindeutigem $x$ für jedes Element ist $\vec{pq}=x\in V_0$ und damit $\vec{pq}\in V_0$.
\item
	Sei $q\in A, x\in V_0, q+x\in A_0$.
	Dann ist
	\[
		q = \underbrace{(q+x)}_{\in A_0}-\underbrace x_{V_0} \in A_0
	\]
\item
	Sei $p_0+V_0=A_0=A_1=p_1+V_1$.
	\[
		p_0=p_0+0\in A_0=A_1 \stackrel{(c)}\implies p_0\in A_1
	\]
	Wegen $p_0\in A_1$ ist
	\[
		p_0 + V_0 = A_0 = A_1 = p_1 + V_1 = p_0 + V_1
	\]
	und damit nach (b) $V_0 = \{\vec{p_0q}:q\in A_0\} = V_1$.

	Sei umgekehrt $V_0=V_1$ und $p_0\in A_1$.
	Dann ist $A_1 = p_0 + V_1$ und
	\[
		A_1=p_0+V_1=p_0+V_0=A_0
	\]
\item
	Schränke die Operation $V\times A \to A$ auf $V_0\times A_0\to A_0$ ein.
	Die Eindeutigkeit der Zuordnung bleibt erhalten.
	Sei $p = p_0 + x \in A, x,y\in V_0$, dann wird folgendermaßen abgebildet:
	\[
		(y,p) = (y,p_0+x) \quad \mapsto \quad p_0 + \underbrace{x + y}_{\in V_0} \in A_0
	\]
	Das Bild liegt also tatsächlich wieder in $A_0$.

	Die Axiome A1, A2, A3 bleiben bei der Einschränkung erhalten.
	A4 gilt wegen $A_0=p+V_0$.
\end{enumerate}
\end{proof}
\end{lem}

\begin{df}[Dimension eines affinen Vektorraums] \label{df: 8.7}
	Sei $A$ ein affiner Raum über $K$ bezüglich $V$.
	Die Dimension von $A$ ist definiert als: 
	\begin{align*}
		\dim A &:=\dim V
	\intertext{Die Dimension der leeren Menge ist:}
		\dim \emptyset &:= -1
	\end{align*}
	Nulldimensionale affine Teilräume heißen \emph{Punkte}.
	Eindimensionale affine Teilräume heißen \emph{Geraden}.
	Zweidimensionale affine Teilräume heißen \emph{Ebenen}.
	Für $\dim A=n$, heißen $(n-1)$-dimensionale affine Teilräume \emph{Hyperebenen}.
\end{df}

\begin{prop} \label{prop: 8.8}
	Sei $A$ ein affiner Raum mit affinen Teilräumen $A_i$ ($i\in I$). Dann ist der Durchschnitt
	\[
	D:= \bigcap_{i\in I}A_i
	\]
	selbst ein affiner Teilraum.
	\begin{note}
		Für $I=\emptyset$ setzt man $D:=A$
	\end{note}
	\begin{proof}
		Für $D=\emptyset$ ist die Behauptung trivial.
		Sei also $D\neq \emptyset$. 
		Dann existiert ein Punkt $p\in D=\bigcap_{i\in I} A_i$, also ist $p\in A_i$ für alle $i\in I$ und wir können alle $A_i$ schreiben als
		\begin{align*}
			A_i=p+V_i
		\end{align*}
		mit passenden Untervektorräumen $V_i$.
		Da die Darstellung $q=p+x$ mit $x\in V_i$ für jedes $q\in A_i$ ($i\in I$) eindeutig ist, gilt
		\[
			D=\bigcap_{i\in I}A_i=\bigcap_{i\in I}(p+V_i) = p+\bigcap_{i\in I}V_i
		\]
		Da $\bigcap_{i\in I}V_i$ wieder ein Untervektorraum von $V$ ist, ist $D$ ein affiner Teilraum.
	\end{proof}
\end{prop}

\begin{df}[Affine Hülle eines affinen Teilraumes] \label{df: 8.9}
	Sei $A$ ein affiner Raum und $M\subset A$ eine Teilmenge von $A$.
	Die \emph{affine Hülle} $[M]$ von $M$ ist definiert als
	\[
	[M]= \bigcap_{\substack{A_0 \text{ affiner Teilraum von } A \\ M\subset A_0}}A_0
	\]
	also der Schnitt aller affinen Teilräume von $A$, die $M$ enthalten und damit der kleinste affine Teilraum, der $M$ enthält.
	\begin{note}
		Wenn $M=\{p_1,\dotsc,p_n\}$ endlich ist, schreiben wir oft
		\[
		[p_1,\dotsc,p_n] \quad \text{ statt }\quad [\{p_1,\dotsc,p_n\}]
		\]
		\begin{ex}
			\begin{align*}
			[\emptyset] &=\emptyset\\
			[p_1] &=\{p_1\}\\
			[p_1,p_2] &=\begin{cases}
			\{p_1\} &\text{falls } p_1=p_2 \\
			\text{Gerade durch $p_1$ und $p_2$}
			\end{cases}
			\end{align*}
		\end{ex}
	\end{note}
\end{df}

\begin{thm}[Dimensionsformel für affine Räume] 
	\label{thm:8.10}
Sei $A$ ein affiner Raum mit Teilräumen $A_1=p_1+V_1$ und $A_2=p_2+V_2$.
Dann gilt:
\[
	\boxed{
\dim A_1 +\dim A_2 = \begin{cases}
\dim [A_1\cup A_2] + \dim (A_1 \cap A_2) &\quad\text{für } A_1\cap A_2 \neq \emptyset \\
\dim [A_1 \cup A_2] + \dim (V_1 \cap V_2) - 1 &\quad\text{für } A_1\cap A_2 = \emptyset
\end{cases}
}
\]
\begin{ex}
\begin{enumerate}
\item
Für $A_1=\{p_1\}, A_2=\{p_2\}, V_1=\{0\}, V_2=\{0\}$ gilt:
\begin{itemize}
\item Falls $A_1\cap A_2 \neq \emptyset$, dann ist $p_1=p_2$ und
\begin{align*}
	0+0=\dim [A_1\cup A_2] + 0 \implies \dim[A_1\cup A_2] = 0
\end{align*}
\item Falls $A_1\cap A_2 = \emptyset$, dann ist $p_1\neq p_2$ und
\begin{align*}
	0+0= \dim [A_1\cup A_2] + 0 - 1 \implies \dim[A_1\cup A_2] = 1
\end{align*}

\end{itemize}

\item
Sei $A_1=[p_1,p_2], A_2=\{p_3\}$. 
\begin{itemize}
 \item Für $A_1\cap A_2 \neq \emptyset$ gilt $[A_1\cup A_2]=A_1 \implies [A_1 \cup A_2]$ ist Gerade .

\item Für $A_1\cap A_2=\emptyset$:
\[
1+0=\dim [A_1\cup A_2] + 0-1 \implies [A_1\cup A_2] \text{ ist eine Ebene}
\]
\end{itemize}
\item
Seien $A_1=p_1+V_1, A_2=p_2+V_1$ Geraden mit $A_1\cap A_2 = \emptyset$ und $\dim V_1 = 1$, dann ist
\[
	1+1=\dim [A_1 \cup A_2] + \underbrace{\dim (V_1 \cap V_2)}_{=1} - 1 \implies [A_1\cup A_2] \text{ ist eine Ebene}
\]
Der zweite Fall ist trivial, da gerade dann die beiden affinen Unterräume identisch sind. 
\end{enumerate}
\end{ex}

\begin{proof}
	\fixme[Prüfen und etwas mehr Prosa]
1. Fall:
\[
[A_1\cap A_2] \neq \emptyset \implies \exists p\in A_1 \cap A_2 \]\[
A_1=p+V_1, A_2=p+V_2 \]\[
A_1\cap A_2 = p+V_1\cap V_2\]\[
A_1\cap A_2 \ni q=p+x, x\in V_1 \land x\in V_2 \]\[
\dim A_1 \cap A_2 = \dim V_1 \cap V_2
\]
\[
[A_1\cup A_2] = p+(V_1+ V_2)
\]

Also ist der erste Teil der Dimensionsformel für affine Räume aus der Dimensionsformel für Vekorräume.
\begin{align*}
 \stackrel{6.8}\implies  \dim[A_1\cup A_2] = \dim(V_1 + V_2)&=\dim V_1+\dim V_2 - \dim(V_1\cap V_2)\\
&=\dim A_1 + \dim A_2 -\dim(A_1\cap A_2)
\end{align*}


2. Fall: $A_1\cap A_2=\emptyset$\\

$A_1=p_1+V_1,A_2=p_2+V_2,\langle\vec{p_1p_2}\rangle=:W$ eindimensional.

Wir zeigen:
\begin{enumerate} %r schaut ein wenig blöd aus
\item $(V_1+V_2)\cap W=\{0\}$:
Angenommen, $(V_1+V_2)\cap W\neq \{0\} \implies (V_1+V_2)\cap W=W$
\begin{align*}
&\implies W\subset V_1+V_2, \vec{p_1p_2}\in W \\
&\implies \exists x\in V_1, y\in V_2:\vec{p_1p_2}=x+y
\end{align*}
\[
A_1=p_1+V_1, V_1=\{\vec{p_1a_1}:a_1\in A_1\} \implies \exists a_1:x=\vec{p_1a_1}
\]
Analog ergibt sich: $ \exists a_2: y= \vec{a_2p_2}$
\begin{align*}
&\implies \vec{p_1p_2}=\vec{p_1a_1}+\vec{a_1a_2}+\vec{a_2p_2} \\
&\vec{a_1a_2}=\vec{a_1p_1}+\vec{p_1p_2}+\vec{p_2a_2} \\
&=\vec{a_1p_1}+\vec{p_1a_1}+\vec{a_2p_2}+\vec{p_2a_2} \\
&=\vec{a_1a_1}+\vec{a_2a_2}=0+0=0\\
&\implies\vec{a_1a_2}=0 \implies a_1=a_2
\end{align*}
Dies ist ein Widerspruch dazu, dass der Schnitt $A_1 \cap A_2$ leer ist.
\item $[A_1\cup A_2]=p_1+(V_1+V_2+W)$:
\begin{align*}
A_1&=p_1+V_2\subset p_1+(V_1+V_2+W)\\
A_2&=p_2+V_2=p_1+\vec{p_1p_2}+V_2\subset p_1+(V_1+V_2+W)\\
&\implies A_1\cup A_2\subset p_1(V_1+V_2+W)\\
&\implies [A_1\cup A_2]\subset p_1+(V_1+V_2+W)\\
p_1\in{A_1\cup A_2}&\implies[A_1\cup A_2]=p_1+V', V' \text{ ein VR}\\
A_1\subset[A_1\cup A_2] &\implies V'\supset V_1\\
W&=\langle\vec{p_1p_2}\rangle, \qquad p_1,p_2\in[A_1\cup A_2]\\
&\implies \vec{p_1p_2}\in V'\implies W\subset V'\\
p_2\in[a_1\cup A_2]&\implies [A_1\cup A_2]=p_2+V'\\ &\implies V_2\subset V'
\end{align*}
\end{enumerate}
Wenn das gezeigt ist, folgt die Dimensionsformel.
\begin{align*}
\dim[A_1\cup A_2]&=\dim (V_1+V_2+W)\\
&=\dim(V_1+V_2)+\dim W -\underbrace{\dim((V_1+V_2)\cap W)}_{=0}\\
&=\dim(V_1+V_2)+\dim W \\
&=\dim(V_1) +\dim(V_2)-\dim(V_1\cap V_2)+\dim W\\
&=\dim{A_1}+\dim(A_2)-\dim(V_1\cap V_2) +1\\
&\implies \dim[A_1\cup A_2]=\dim A_1+\dim A_2-\dim(V_1\cap V_2)+1
\end{align*}
\end{proof}
\end{thm}

\begin{df} \label{df:8.11}
Sei $A$ ein affiner Raum und $A_1=p_1+V_1$ und $A_2=p_2+V_2$ affine Teilräume von $A$.

$A_1$ und $A_2$ heißen \emph{parallel} genau dann, wenn 
\[
	V_1\subset V_2 \quad \lor \quad V_2 \subset V_1
\]
\begin{ex}
\begin{itemize}
\item Zwei parallele Geraden
\item Eine Gerade parallel zu einer Ebene
\item Zwei parallele Ebenen
\item Jeder affine Unterraum $A_1$ von $A$ ist parallel zu $A$
\item Jeweils zwei Punkte sind parallel zueinander
\end{itemize}
\end{ex}
\end{df}

Aus der Dimensionsformel \ref{thm:8.10} folgt für zwei geraden in der Ebene:
\[ \dim[A_1\cup A_2]\le \dim A = 2\]
Falls $A_1\cap A_2\neq \emptyset$:
\[ 1+1 = \dim[A_1\cup A_2]+\dim(A_1\cap A_2)\]
Falls $A_1\cap A_2=\emptyset$:
\[ 1+1=\dim[A_1\cup A_2]+\dim(V_1\cap V_2)-1\]
Für zwei Ebenen im dreidimensionalen Raum $A$ gilt:
\[  2\le\dim[A_1\cup A_2]\le3\]
Falls $A_1\cap A_2\neq\emptyset$:
\[ 2+2=\dim[A_1\cup A_2]+\dim(A_1\cap A_2)\]
Falls $A_1\cap A_2=\emptyset$:
\[ 2+2=\dim[A_1\cup A_2]+\dim(V_1\cap V_2)-1\]

\section{Konkrete Beschreibung affiner Teilräume durch inhomogene lineare Gleichungssysteme}

Sei $A=p+V$ ein affiner Raum und ein Koordinatensystem bezüglich $p$ gegeben.
Sei weiterhin $A_1 = p_1 + V_1$ ein affiner Teilraum von $A$.
$V_1$ können wir wie bereits bekannt als Lösungsmenge eines homogenen linearen Gleichungssystems schreiben:
\[
	V_1 = \{x\in V:\; Ax = 0\} \qquad A \in \Mat(n,\R)
\]
Ein Punkt $q\in A_1$ kann geschrieben werden als
\[
	q = p_1 + x
\]
mit $x=\vec{p_1q}$.
Wenn wir in Koordinaten rechnen, schreiben wir dann $x=q-p_1$ ($q$ und $p_1$ in Koordinaten bezüglich $p$ gegeben).
Die Gleichung $Ax=0$ wird dann zu
\[
	A(q-p_1)=0 \quad \iff \quad Aq =Ap_1
\]
und für bekanntes $p_1$ ist dann mit $b:=Ap_1$
\[
	q\in \{\tilde q\in A:\; A\tilde q = b\}
\]
Diese $\tilde q$ sind die Koordinaten der Punkte von $A_1$

\begin{kor} \label{kor:8.12}
	Sei $A$ ein endlich-dimensionaler affiner Raum über dem Körper $K$.

	Affine Teilräume sind (in Koordinaten geschrieben) genau die Lösungsmenge von inhomogenen linearen Gleichungsystemen (mit Koeffizienten in $K$).
	
	Für eine lineare Funktion $\phi:V\to V$ und $b\in V$ sind affine Teilräume also genau die Mengen der Form 
	\[
		\phi^{-1}(b)
	\]
	Falls $\phi^{-1}\neq\emptyset$, ist für beliebiges $v_0\in \phi^{-1}(b)$
	\[
		\phi^{-1}(b)=v_0+W_0
	\]
	wobei $W_0=\ker\phi$.
\begin{proof}
	Die eine Richtung haben wir schon gezeigt. 
	Sei umgekehrt
\[ 
	\phi^{-1}(b)=\emptyset \quad\text{ oder }\quad v_0\in \phi^{-1}(b)
\]
Im ersten Fall ist es trivialerweise ein affiner Teilraum.
Sei also $\phi^{-1}(b)\neq \emptyset$ und $u_1,u_2\in \phi^{-1}(b)$ beliebig.
Dann ist in Koordinaten geschrieben
\[
	\phi(u_2 - u_1) = \phi(u_2) - \phi(u_1) = b - b = 0 \implies u_2 - u_1 \in \ker\phi
\]
und damit
\[
	v_0 + \ker\phi = \phi^{-1}(b)
\]
Also $V_0:=\ker(\phi)$
\end{proof}
\end{kor}

\section{Lösbarkeit von inhomogenen linearen Gleichungssystemen}

Die Lösungsmenge eines linearen inhomogenen Gleichungssystems $Ax=b$ ist entweder leer oder von der Form $p_0+V_0$, wobei $V_0$ die Lösungsmenge des zugehörigen homogenen Gleichungssystems $Ax=0$ ist.

Die Lösung eines inhomogenen Gleichungssystems ergibt sich also aus einer speziellen (oder \emph{partikulären}) Lösung $p_0$ in $Ap_0 = b$ und der allgemeinen Lösung von $Ax=0$.

\begin{seg}{Existenz einer speziellen Lösung von $Ax=b$}
	Wir fragen uns, ob es ein $x$ gibt, sodass $\phi(x) = b$.
	Äquivalent wäre die Frage: Gilt $b\in\im\phi$?

	$\im\phi$ ist als Vektorraum erzeugt von den $\phi(v_1)\dotsc\phi(v_n)$ für eine Basis $v_1,\dotsc,v_n$.
	Das entspricht den Spalten der Matrix $A$.
\[
b\in\im\phi \quad \iff\quad \exists \lambda_i:\;b=\sum_{i=1}^n\lambda_i\phi(v_i)
\]
Man definiert die erweiterte Koeffizientenmatrix folgendermaßen:
\[
	\begin{pmatrix}[c|c] A&b\end{pmatrix}=\begin{pmatrix}[ccc|c]\phi(v_1)&\cdots&\phi(v_n)&b\end{pmatrix}
\]
Es gilt damit
\[
b\in\im\phi \iff \rg A = \rg(A|b)
\]
Der Rang und damit die Existenz einer spezielle Lösung lässt sich also durch Rangbestimmung mit dem Gauß-Verfahren entscheiden.
\end{seg}
\begin{thm} \label{thm:8.13}
Sei $A \in \Mat(m\times n,K)$ und $b\in K^m$. Dann hat das inhomogene lineare Gleichungssystem $Ax=b$ eine Lösung genau dann, wenn gilt:
\[
\rg A=\rg(A|b)
\]
\begin{proof}
	Es gilt stets $\rg A \le \rg(A|b)$.
	Gleichheit herscht genau dann, wenn $b$ als Linearkombination der Spaltenvektoren von $A$ darstellbar ist, also $b\in \im \phi$.
\end{proof}
\end{thm}

\begin{kor}
	\label{kor:8.14}
	Sei $A\in \Mat(m\times n, K)$, dann existiert eine Lösung von $Ax=b$ für jede Wahl von $b\in K^m$ genau dann, wenn
	\[
		\rg A = m
	\]
\begin{proof}
$A$ definiert $\phi:K^n\to K^m$.
Es gilt die Äquivalenzkette
\begin{align*}
Ax=b \; \text{ lösbar $\;\forall b\in K^m$} 
	\iff b\in\im\phi \quad \forall b\in K^m
	\iff \im \phi = K^m
	\iff \phi\; \text{ surjektiv} 
	\iff \rg A =m
\end{align*}
\end{proof}
\end{kor}

Gibt es eine Formel für die Lösung $x$ von $Ax=b$, falls $x$ existiert und eindeutig ist?

$Ax=0$ hat eine eindeutige Lösung $x\neq 0$ genau dann, wenn $A$ invertierbar ist.
In diesem Fall existiert auch eine eindeutige Lösung für $Ax=b$, nämlich
\[
	x=A^{-1}b
\]
Das Inverse von $A$ lässt sich folgendermaßen durch ihre Adjunkte und ihre Determinante bestimmen
\[
	A^{-1} = \f{\adj A}{\det A}
\]
Also
\begin{align*}
\implies x=A^{-1}b&=\frac{\adj A}{\det A}b_j
\end{align*}
Für die einzelnen Einträge ergibt sich dann
\[
x_i
=\sum_{j=1}^n\frac{\det(A_{ji})}{\det A}b
=\frac 1{\det A}\det\begin{pmatrix}[ccccccc]a^1&\cdots&a^{i-1}&b&a^{a+1}&\cdots&a^n\end{pmatrix}
\]
\begin{kor}[Cramersche Regel] \label{kor: 8.15}
Wenn $A$ invertierbar ist, dann ist die eindeutige Lösung von $Ax=b$ gegeben durch den Vektor
\begin{align*}
x=\begin{pmatrix}x_1\\ \vdots \\ x_n\end{pmatrix} \qquad \text{ wobei }\qquad
x_i=\frac{
\det\begin{pmatrix}
a^1 & \cdots & a^{i-1} & b & a^{i+1} & \cdots & a^n
\end{pmatrix}
}{\det A}
\end{align*}
Die $a^i$ seien dabei die Spaltenvektoren von $A$.\end{kor}
\begin{note}[Konsequenz]
Die Koeffizieneten der Lösung hängen stetig differenzierbar von den Koeffizienten von $A$ und $b$ ab. \fixme{stetig differenzierbar??}
\end{note}




\chapter{Dualräume}

Seien $V$ und $W$ $K$-Vektorräume. 
Wir hatten
\[
	\Hom(V,W)=\{\phi:V\to W \;\big|\; \phi\; K\text{-linear}\}
\]
definiert als die Menge der $K$-linearen Abbildungen von $V$ nach $W$.

$\Hom(V,W)$ ist selbst ein $K$-Vektorraum. Für $\phi,\psi\in \Hom(V,W)$ gilt
\begin{align*}
\phi+\psi&:x\mapsto (\phi+\psi)(x):=\phi(x)+\psi(x)\\
\lambda\phi&:x\mapsto (\lambda\phi)(x):=\lambda\phi(x)
\end{align*}
Die äquivalenten Operationen auf den Matrizen sind die geläufigen Matrizenaddition und Matrizen-Skalarmultiplikation.

Für die Dimension von $\Hom(V,W)$ gilt
\[
	\dim V=n \;\land\; \dim W=l \quad \implies\quad \dim \Hom(V,W)=n\cdot l
\]
\begin{proof}
Sei $v_1,\dotsc,v_n$ eine Basis von $V$ und $w_1,\dotsc,w_n$ eine Basis von W.
Jede Matrix ist Linearkombination von Matrizen der Form 
\[
	\begin{pmatrix}0&\cdots&0\\\vdots&1&\vdots\\0&\cdots&0\end{pmatrix}
\]
mit einer 1 an der $i,j$-ten Stelle.
Wir definieren die zugehörige lineare Abbildung $f_{ij}$ durch
\begin{align}
\label{eq: hom_basis}
f_{ij}(v_k)=\begin{cases}
w_j & k=i\\
0 & k\neq i
\end{cases}
\end{align}
Die $f_{ij}$ bilden nach (3.3) eine Basis von $\Hom(V,W)$
\end{proof}

\begin{df}
\label{df:9.1}
Sei $V$ ein $K$-Vektorraum.
Der Raum 
\[
	V^*:=\Hom(V,K)
\]
der linearen Abbildungen von $V$ nach $K$ heißt der \emph{zu $V$ duale Vektorraum} (oder der \emph{Dualraum} von V).

Die Elemente $\phi\in V^*$ heißen \emph{Linearformen} (oder \emph{lineare Funktionale}).
\begin{ex}
	Sei $x=(x_1,\dotsc,x_n)\in V$ und $\phi$ gegeben durch
	\[
		\phi(x) = \sum_{k=1}^n a_ix_i
	\]
	für $a_i \in K$.
	Für $V$ und $W$ wählen wir die Einheitsbasis und für $V^*$ so wie in \eqref{eq: hom_basis}.
	Dann ist
	\[
	\phi(e_i)=a_i\cdot1=a_i
	\]
Wir zeigen jetzt, dass wir $\phi$ als Linearkombination
\[
	\phi=\sum_{i=1}^na_if_{1i}
\]
schreiben können.
\begin{proof}
Sei $v=\sum_{j=1}^n\lambda_je_j\in V$.
Dann gilt
\begin{align*}
\left(\sum_{i=1}^na_if_{1i}\right)(v)
&=\sum_{i=1}^na_if_{1i}\left(\sum_{j=1}^n\lambda_je_j\right)\\
&=\sum_{i=1}^na_i\lambda_if_{1i}(e_i)\\
&=\sum_{i=1}^na_i\lambda_i= \phi\left(\sum_{j=1}^n \lambda_je_j\right) = \phi(v)
\end{align*}
\end{proof}
Man verwendet die Bezeichnung $e_{i}^*:=f_{1i}$, also
\begin{align*}
e_{i}^*(e_j):=\begin{cases}
1&i=j\\
0&i\neq j
\end{cases}
\end{align*}
\end{ex}
\end{df}

\begin{prop} \label{prop: 9.2}
Sei $V$ ein $n$-dimensionaler Vektorraum und $v_1,\dotsc,v_n$ eine Basis.
Dann hat $V^*$ eine Basis $v_1^*,\dotsc,v_n^*$ mit
\[
 \boxed{v_i^*(v_j)=\begin{cases}1&i=j\\0&i\neq j\end{cases}}
\]
Insbesondere gilt
\[
	\dim V^*=n = \dim V
\]
Die Basis $v_1^*,\dotsc,v_n^*$ von $V^*$ heißt die zu $v_1,\dotsc,v_n$ \emph{duale Basis}.

Die Abbildung $v_i\mapsto v_i^*$ definiert einen Isomorphismus von $V$ zu $V^*$.
\begin{note}[Kronecker-delta]
Man verwendet die Schreibweise:
\[
\delta_{ij} := \begin{cases} 1 & \text{für } i=j\\ 0 &\text{sonst}\end{cases}
\]
Also kann man schreiben:
\[
	v_{i}^*(v_j):=\delta_{ij}v_j
\]
\end{note}


\begin{proof}
Sei $v_1,\dotsc,v_n$ eine Basis von V und $1$ eine Basis von $W=K$.
Die linearen Abbildungen von $V$ nach $W$ sind festgelegt durch die Bilder der Basisvektoren
(und für jede Wahl der Bilder existiert genau eine lineare Abbildung, die die Basisvektoren so abbildet).

Also existiert eine Abbildung $v_i^*: (v_1\dotsc,v_{i-1},v_i,v_{i+1},\dotsc,v_n) \mapsto (0,\dotsc,1,\dotsc,0)$.
Die $v_{1}^*,\dotsc,v_{n}^*$ sind Elemente von $V^*$, es bleibt zu zeigen, dass sie eine Basis bilden.

Sei $\phi\in V^*$, definiere $a_i :=\phi(v_i)\in K$.
Dann ist $\phi=\sum_{i=1}^na_iv_{i}^*$, denn
\[
\left(\sum_{i=1}^na_iv_{i}^*\right)(v_j)
=\sum_{i=1}^na_iv_{i}^*(v_j)
=a_jv_j^*(v_j) = a_j = \phi(v_j)
\]
Also bilden die $v_{i}^*$ ein Erzeugendensystem.
Zeige jetzt die lineare Unabhängigkeit.

Sei $0=\sum_{i=1}^n\lambda_iv_i^* \in V^*$
\[
\forall v\in V: 0=0(v)=\left(\sum_{i=1}^n\lambda_iv_i^*\right)(v)
\]
Wähle $v=v_j$ dann ergibt sich
\[
0=\sum_{i=1}^n\lambda_iv_i^*(v_j)=\lambda_j
\]
Damit sind alle Koeffizienten $\lambda_j=0$ ($j=1,\dotsc,n$) und die $v_1^*,\dotsc,v_n^*$ linear unabhängig. 
Also bilden $v_1^*,\dotsc,v_n^*$ eine Basis von $V^*$ und $\dim V^*=n$.

Damit existiert auch der Isomorphismus $V\to V^*$, definiert durch $v_i\mapsto v_i^*$.
\end{proof}
\end{prop}

\begin{ex}
Sei $V=\R^2, K=\R$ und lineare Gleichung $ax+by=0$ gegeben.
Dann ist
\[
W:=\left\{(x, y) \in \R^2:ax+by=0\right\}
\]
ist ein Untervektorraum von $V$. 
Für $(a,b)=0$ ist $W=\R^2$.
Für $(a,b)\neq (0,0)$ ist $W$ eindimensional.

$(a,b)$ definiert also eine Linearform $\phi:V\to\R:(x,y)\mapsto ax+by\in\R$.
Wir schreiben auch in Anlehnung an die Matrizenschreibweise (lineare Funktionale sind $1\times n$-Matrizen):
\[
	\phi=\begin{pmatrix}a&b\end{pmatrix}\in V^*
\]
In $V^*$ erzeugt der Vektor $\phi$ einen Unterraum $W^0:=\<\phi\>=\{\lambda\phi:\lambda\in\R\}$.
Entweder:
\begin{align*}
	\phi=\begin{pmatrix}a&b\end{pmatrix}=\begin{pmatrix}0&0\end{pmatrix} \implies \dim W^0=0\\
\phi\neq \begin{pmatrix}a&b\end{pmatrix}=\begin{pmatrix}0&0\end{pmatrix}\implies \dim W^0=1
\end{align*}
In beiden Fällen gilt: 
\[
	\dim W +\dim W^0 =2=\dim V
\]
$W_0$ beschreibt das Gleichungssystem. 
$W$ beschreibt die Lösungsmenge.
\end{ex}
\begin{ex}
\begin{itemize}
\item
Sei $I=[0,1]$ ein Intervall und $V=\{f:I\to\R, \text{ stetig}\}$ ein Vektorraum über $\R$.
Dann sind
\begin{align*}
	\phi\in W^*:f &\mapsto \int_0^1f(x)dx\\
	\text{und}\quad \psi\in V^*:f&\mapsto f\left(\frac 12\right)
\end{align*}
Linearformen.
\item
Sei $I=(0,1)$ ein Intervall und $W=D(I):=\{f:I\to\R, \text{differenzierbar}\}$ ein Vektorraum über $\R$.
Dann ist
\[
	\phi\in W^*:f\mapsto f'\left(\frac 13\right)
\]
eine Linearform.
\end{itemize}
\end{ex}

\begin{kor} \label{kor: 9.3}
Sei $V$ ein endlich-dimensionaler Vektorraum und $0\neq v_0\in V$.
Dann existiert ein $\phi\in V^*$ mit $\phi(v_0)\neq 0$.

Sei $v_1,v_2\in V$ mit $v_1\neq v_2$. 
Dann exstiert ein $\psi\in V^*$ mit $\psi(v_1)\neq \psi(v_2)$.
\begin{proof}
Sei $v_0\in V \setminus \{0\}$.
Ergänze $v_0$ zu einer Basis $v_0,\dotsc,v_n$ von $V$.
Dann hat $v^*$ eine Basis $v_{0}^*,\dotsc,v_{n}^*$.
Wähle jetzt $\phi=v_0^*$, dann ist $\phi(v_0)=1$.

Ähnlich kann man die zweite Aussage zeigen.
\end{proof}
\end{kor}
\begin{note}
Für irgendein $u\in V$ ist $u^*$ nicht definiert!
Erst wenn die ganze Basis $u_1,\dotsc,u_n$ existiert, gibt es die Basis $u_1^*,\dotsc,u_n^*$.
Sie hängt von allen Vektoren $u_1,\dotsc,u_n$ ab.
Ebenso ist der Isomorphismus $V\to V^*$ abhängig von einer Basis von $V$.
\end{note}

\section{Bidualräume}

\begin{df} \label{df:9.4}
	Der Vektorraum $V^{**} := (V^*)^* = \Hom(V^*,K)$ heißt \emph{Bidualraum} von V.
\end{df}

Die Elemente von $V^{**}$ sind lineare Abbildungen von $V^*$ nach $K$.

Sei $\phi\in V^*$ (also $\phi: V\to K$ lineare Abbildung).
Sei $v\in V$, dann ist $\phi(v)\in K$.

Also definiert die Auswertung einer linearen Abbildung an einem fest gewähltem $v\in V$ eine Abbildung von $V^*\to K: \phi \mapsto \phi(v)$.

Wir bezeichnen diese Abbildung mit $\jota_v$.
\[
	\jota_v(\phi)=\phi(v)
\]
Weiter definieren wir die Abbildung $\jota: V\to V^{**}$ durch
\[
	V\to V^{**}:v\mapsto \jota_v
\]

Zu zeigen: $\jota_v$ ist lineare Abbildung
\begin{proof}
	Sei $\phi,\psi\in V^*$, dann gilt
\begin{align*}
\jota_v(\phi+\psi) &=(\phi+\psi)(v)=\phi(v)+\psi(v)=\jota_v(\phi)+\jota_v(\psi)\\
\jota_v(\lambda\phi) &=(\lambda\phi)(v)=\lambda\cdot\phi(v)=\lambda\jota_v(\phi)
\end{align*}
Also ist $\jota_v$ linear und $\jota_v\in V^{**}$.
\end{proof}


\begin{thm}
\label{thm:9.5}
Die Abbildung $\jota: V\to V^{**}$ ist linear.

Wenn $\dim V<\infty$, dann ist $\jota$ ein Isomorphismus $V\to V^{**}$.
\begin{note}
	$\jota$ hängt nicht von der Basiswahl ab (im Gegensatz zum Isomorphismus $V\to V^*$)!
\end{note}
\begin{proof}
	Die Wohldefiniertheit hatten wir schon gezeigt, denn $\jota(v) = \jota_v \in V^{**}$ und $\jota_v$ ist eindeutig für gegebenes $v$.
	Wir zeigen jetzt die Linearität. 
	Sei $v,w\in V$, $\phi\in V^*$, dann gilt
\begin{align*}
\jota_{v+w}(\phi)&=\phi(v+w)=\phi(v)+\phi(w)=\jota_v(\phi)+\jota_w(\phi)\\
\jota_{\lambda v}(\phi) &= \phi(\lambda v) = \lambda \phi(v) = \lambda \jota_v(\phi)
\end{align*}
Sei $\dim V=n$, dann ist $\dim V^{**}=n$ (man wende \ref{prop: 9.2} zweimal an).
Damit genügt es, die Injektivität von $\jota$ nachzuweisen.

Sei $v\in V, v\neq 0$.
Nach \ref{kor: 9.3} existiert $\phi$ mit 
\[
	0 \neq \phi(v) = \jota_v(\phi) = (\jota(v))(\phi)
\]
Also ist $\ker \jota = (0)$ und $\jota$ injektiv.
\end{proof}
\begin{note}
Falls $V$ nicht endlich-dimensional ist, gilt \ref{kor: 9.3} trotzdem (Beweis möglich durch transfinite Induktion).
Also ist $\jota$ immer injektiv.

Wenn $V$ nicht endlich-dimensional ist, ist $V$ nicht isomorph zu $V^*$ und nicht zu $V^{**}$.
\end{note}
\end{thm}

Sei $V=K^n$, und die Basis die Einheitsbasis.
$V^*$ hat die duale Basis $e_1^*,\dotsc,e_n^*$ mit $e_i^*(e_j)=\delta_{ij}$.
\[
e_i^*(v)=e_i^*\left (\sum_{j=1}^n\lambda_je_j\right )=\sum_{j=1}^n\lambda_je_i^*(e_j)=\lambda_i
\]
$e_i^*$ ist also die $i$-te Koordinatenfunktion (liefert beim Anwenden auf einen Vektor die $i$-te Koordinate).
\[
	v^*(v)=\left(\sum_{i=1}^na_ie_i^*\right)(v)=\sum_{i=1}^na_i(e_i^*(v))=\sum_{i=1}^n a_i\lambda_i
\]
Auswerten von Funktionalen ist also die Multiplikation von Matrizen, wenn man
Vektoren in $V$ als Spaltenvektoren und Vektoren in $V^*$ (Funktionale) als Zeilenvektoren schreibt.

\section{Unterräume und ihre dualen Gegenstücke}

\begin{df}
\label{df: 9.6}
Sei $V$ ein $K$-Vektorraum und $W<V$ ein Untervektorraum.
Dann heißt
\[
W^0:=\left\{\phi\in V^*\;\big|\;\forall w\in W:\phi(w)=0 \right\} \subset V^*
\]
der zu $W$ \emph{orthogonale Raum}.
Man schreibt auch für $W^0=W^\orth$ ($W$ senkrecht).

\begin{ex}
\begin{align*}
W=\{0\} &\implies W^0=V^*\\
W=V &\implies W^0=\{0_{V^*}\}
\end{align*}
\end{ex}

\begin{proof}[$W^0$ ist tatsächlich Untervektorraum]
$W^0$ ist nicht-leer, denn $0_{V^*}\in W^0$. 
Sei also $\phi,\psi\in W^0, \lambda\in K$:
\begin{align*}
(\phi+\psi)(w)&=\phi(w)+\psi(w)=0_K\\
(\lambda\phi)(w)&=\lambda\phi(w)=0_K
\end{align*}
\end{proof}
\end{df}

\begin{ex}
	Für $V=\R^2$ beschreibt $ax+by=0$ die Gleichung einer Gerade in der Ebene (für $(a,b)\neq (0,0)$).

	Die Idee dahinter: Die Elemente von $W^0$ sind Gleichungen, deren Lösungen die Elemente von $W$ sind.
\end{ex}


\begin{prop}
\label{prop:9.7}
Sei $W<V$, $w_1,\dotsc,w_l$ eine Basis von $W$, ergänzt zu einer Basis $w_1,\dotsc,w_l,v_1,\dotsc,v_t$ von $V$.
Dann bilden $v_1^*,\dotsc,v_t^*$ von der dualen Basis $w_1^*,\dotsc,v_l^*,v_1^*,\dotsc,v_t^*$ eine Basis von $W^0$.

Insbesondere gilt also
\[
\dim W +\dim W^0=\dim V
\]
\begin{proof}
Aus $w_1,\dots,w_l,v_1,\dotsc,v_t$ Basis von $V$ folgt, dass $v_1^*,\dotsc,v_t^*$ sind linear unabhängig.\\
Zeige: $v_1^*,\dotsc,v_t^* \in W^0$ und erzeugen $W^0$:

Sei $w=\sum_{k=1}^l\lambda_kw_k \in W$.
Wegen
\begin{align*}
v_i^*(w_j)=0 \qquad \forall j=1,\dotsc,l
\end{align*}
gilt
\[
v_i^*(w)=v_i^*\left(\sum_{k=1}^l\lambda_kw_k\right)=\sum_{k=1}^l\lambda_kv_i^*(w_k)=0
\]
Damit ist $v_i^*\in W^0$.
Sei $\phi\in W^0\subset V^*$.
\[
	\phi= \sum_{j=1}^t\lambda_jv_j^*+\sum_{k=1}^l\my_kw_k^*
\]
Da $w_i\in W$ ist $\phi(w_i)=0$ und
\[
	0  = \phi(w_i)= \sum_{j=1}^t\lambda_j\underbrace{v_j^*(w_i)}_{=0}+\sum_{k=1}^l\my_k\underbrace{w_k^*(w_i)}_{=\delta_{ki}} = \my_i
\]
Damit ist $\my_i=0$ für beliebiges $i$.
Also ist
\[
\phi=\sum_{j=1}^t\lambda_jv_j^*
\]
und $v_1^*,\dotsc,v_t^*$ erzeugen $W^0$.
Die Dimensionsformel ergibt sich sofort.
\end{proof}

\end{prop}

\begin{kor}
\label{kor:9.8}
Sei $\dim V<\infty, W<V$. Dann gilt:
\[
\jota(W)\isomorph(W^0)^0
\]
Der Isomorphismus $\jota: V\to V^{**}$ induziert also durch Einschränkung einen Isomorphismus
\[
	\jota\big|_W: W\xrightarrow{\sim} (W^0)^0
\]
\begin{proof}
	\begin{seg}{Wohldefiniertheit}
		Die eindeutige Zuordnung ist durch $\jota$ bereits gegeben.
		Zeige $\jota(w)=\jota_w\in(W^0)^0$.
\begin{align*}
(W^0)^0=\{\phi\in V^{**}:\phi(W^0)=0\}
\end{align*}
Wähle also $\psi\in W^0$ (dann gilt $\psi(W)=0$).
Dann ist
\begin{align*}
\jota_w(\psi)=\psi(w)=0
\end{align*}
Also ist $\jota_w\in (W^0)^0$ und damit $\jota\big|_W$ wohldefiniert.
\end{seg}

Da $\jota\big|_W:W\to (W^{0})^0$ injektiv (und deshalb $\ker\jota\big|_W=(0)$), ist nach der Dimensionsformel $\dim\jota(W) = \dim W$.
Laut \ref{prop:9.7} ist $\dim W=\dim V-\dim W^0$.
Also
\begin{align*}
\dim W^0 + \dim (W^0)^0 = \dim V^{**} = \dim V^* = \dim V
\end{align*}
und damit
\[
\implies \dim(W^0)^0=\dim V- \dim W^0
\]
\end{proof}
\begin{note}
Also ist $W$ durch $W^0$ bestimmt:
\[
W=\jota^{-1}((W^0)^0)
\]
\end{note}
\end{kor}


\begin{ex}[Anwendung auf homogene lineare Gleichungssysteme]
$a_1x_1+\dotsb+a_nx_n=0$, $V=K^n, V^*\ni \phi, [\phi]<V^*, W:=\ker(\phi)$ %unklar, was das original ist
Dann ist $W$ die Lösungsmenge und
\[
W^0=[\phi]
\]
\end{ex}

\begin{kor}
\label{kor:9.9}
Sei $V$ ein eindlich-dimensionaler Vektorraum, $W<V$, $\phi_1,\dotsc,\phi_r\in V^*$.
Dann sind äquivalent:
\begin{enumerate}
\item $W$ ist die Lösungsmenge des homogenen linearen Gleichungssystems $\phi_1(x)=\dotsb=\phi_r(x)=0$:
	\[
		W=\{v\in V:\phi_1(v)=\dotsb=\phi_r(v)=0\}
	\]
\item $\phi_1,\dotsc,\phi_r$ erzeugen $W^0$:
	\[
		W^0=[\phi_1,\dotsc,\phi_r]
	\]
\end{enumerate}
Die minimale Anzahl von Abbildungen (Gleichungen) $\phi$ die das LGS liefern, ist $\dim W^0=\dim V-\dim W$.

\begin{proof}
Sei $U:=[\phi_1,\dotsc,\phi_r]<V^*$.
\begin{align*}
V^{**}>U^0&=\{x\in V^{**}:x(\phi_1)=\dotsb=x(\phi_r)=0\}\\
&=\{\jota_v:v\in V, \jota_v(\phi_1)=\dotsb=\jota_v(\phi_r)=0\}\\
&=\{\jota_v:v\in V, \phi_1(v)=\dotsc=\phi_r(v)=0\}\\
&=\jota(W)
\end{align*}
Zeige: $W=\jota^{-1}(U^0)$, bzw. $\jota(W)=U^0$ genau dann wenn $W^0=U$.
\begin{description}
\item[``$\Longleftarrow$''] Es gilt
\[
W^0=U \implies (W^0)^0=U^0 \implies \jota(W)=U^\circ
\]
\item[``$\Longrightarrow$'']
	\fixme[Prüfen und verständlich machen]
Es gilt:
\begin{align*}
\jota(W)=U^0 \qquad U^0&=(W^0)^0\\
\implies (U^0)^0 &= ((W^0)^0)^0
\end{align*}
$\jota_v:V\to V^{**}$, analog $\jota_{v^*}:V^*\to V^{***}=(V^*)^{**}$.
Also:
\begin{align*}
\jota_{v^*}(U)=\jota_{v^*}(W^*) &\implies \jota_{v^*}^{-1}\jota_{v^*}(U)=\jota_{v^*}^{-1}\jota_{v^*}(W^0) \\
&\implies U=W^0
\end{align*}
\end{description}
\end{proof}
\end{kor}

Gegeben $W<V$: $W^0<V^*$, Erzeugende $\phi_1,\dotsc,\phi_n$ von $W^0$.
Dann ist $W$ die Lösungsmenge der Gleichungen $\phi_i(x)=0$.

Wenn umgekehrt ein lineares Gleichungssystem gegeben durch die $\phi_i$ ist.
Dann ist die Lösungsmenge gegeben durch $\jota^{-1}(W^0)$.

Das Gauß-Eliminationsverfahren schreibt das Erzeugendensystem $\phi_1,\dotsc,\phi_r$ von $W^0$ um in ein
anderes Erzeugendensystem von $W^0$.
Dabei bleibt $W^0$ unverändert, also auch $\jota^{-1}(U^0)$, was der Lösungsmenge entspricht.

\section{Duale Abbildungen}

\begin{df}
\label{df:9.10}
Sei $V$ und $W$ $K$-Vektorräume und $f:V\to W$ linear.
Die zu $f$ \emph{duale Abbildung} $f^*:W^*\to V^*$ ist definiert durch
\begin{align*}
	\boxed {f^*(\phi):=\phi\circ f}
\end{align*}
\end{df}

\begin{center}
\makebox[0pt]{
\begin{xy}
 (0,0)*+{V}; (20,0)*+{W}; (20, -20)*+{K};
{\ar (0,0)*+{V}; (20,0)*+{W}}?*!/_2mm/{\scriptstyle f};
{\ar (20,0)*+{W}; (20,-20)*+{K}}?*!/_2mm/{\scriptstyle\phi};
{\ar (0,0)*+{V}; (20,-20)*+{K}}?*!/^6mm/{\scriptstyle\phi\circ f = f^*(\phi)};

\end{xy}  }      \end{center}


\begin{proof}[$f^*$ ist linear]
Sei $\phi,\psi\in W^*$ und $v\in V$
\begin{align*}
f^*(\phi+\psi)(v) &= ((\phi+\psi)\circ f)(v) = (\phi+\psi)(f(v))=\phi(f(v))+\psi(f(v))=f^*(\phi)(v)+f^*(\psi)(v)\\
f^*(\lambda\phi)(v)&= ((\lambda\phi)\circ f)(v)=(\lambda\phi)(f(v))=\lambda\phi(f(v))=\lambda f^*(\phi)(v)
\end{align*}
\end{proof}

\begin{prop}
\label{prop:9.11}
Sei $f:V\to W$ durch eine Matrix $A$ gegeben.
Dann ist $f^*:W^*\to V^*$ (bezüglich der dualen Basen) durch die Matrix $A^T$ gegeben.
\begin{proof}
Seien $f:V\to W$ und $f^*:W^*\to V^*$ definiert durch Matrizen:
\begin{align*}
f(v_j)&=Av_j\\
f^*(w_i^*)&=Bw_i^*
\end{align*}
Es gilt
\begin{align*}
	(w_i^*\circ f)(v_j)&=w_i^*\left(\sum_{k=1}^m a_{kj}w_k\right)=a_{ij}\\
  (f^*(w_i^*))(v_j) &=\left(\sum_{k=1}^n b_{ki}v_k^*\right)(v_j)=b_{ji}
\end{align*}
Aber nach Definition ist $f^*(w_i^*)=w_i^*\circ f$, also 
\[
	a_{ij}=w_i^*(f(v_j))=f^*(w_i^*)(v_j)=b_{ji}
\]
Damit ist $B=A^T$.
\end{proof}
\end{prop}

\begin{note}
Die Transposition von Matrizen ist bijektiv und linear.
Es herrscht also ein Isomorphismus zwischen $\Mat(l\times n,K)\to \Mat(n\times l,K)$.
\end{note}

\begin{kor}
\label{kor:9.12}
Seien $V$ und $W$ endlich dimensionale $K$-Vektorräume.
Dann gilt:
\begin{align*}
\Hom(V,W)&\isomorph \Hom(W^*,V^*)\\
\qquad\qquad f&\mapsto f^*
\end{align*}
\begin{proof}
Nach Basiswahl entspricht $f$ der Matrix $A$ und $f^*$ der Matrix $A^T$ nach \ref{prop:9.11}.
Also ist $f\mapsto f^*$ ein $K$-Vektorraum-Isomorphismus.
\end{proof}

\begin{note}[Erinnerung zum Beweis der Dimensionsformel]
Gegeben $f:V\to W$, $\ker f <V, \im f <W$.
Dimensionsformel:
\[
	\dim V = \dim \ker f + \dim \im f
\]
sogar:
\[
\im f \isomorph V/\ker f
\]
Wähle Basis $u_1,\dotsc,u_r$ von $\ker f$, ergänze zu einer Basis von $V$:
\[
u_1,\dotsc,u_r,v_1,\dotsc,v_t \qquad \text{Basis von $V$}
\]
Also $\dim V=r+t$.
Da $0=f(u_1)=f(u_2)=\dotsb=f(u_n)$ erzeugt $f(v_1),\dotsc,f(v_r)$ das Bild $\im(f)$ und sind außerdem nach Konstruktion linear unabhängig.
Sie bilden also eine Basis von $\im f$.
Also $\dim \im f = t$.
Wegen $\dim \ker f = r$ ergibt sich die Dimensionsformel
\[
	\dim V = \dim \ker f + \dim \im f
\]
\end{note}
\end{kor}

\begin{prop}
\label{prop:9.13}
Seien $V$ und $W$ endlich dimensionale $K$-Vektorräume und $f:V\to W$ $K$-linear.
Dann gilt:
\[
\underbrace{\im(f^*)}_{< V^*}=\underbrace{(\ker f)^0}_{<V^*} =\{\phi:V\to K, \phi|_{\ker f}=0\}
\]
\begin{proof}
	\fixme[Verständlich und übersichtlich machen]
\begin{align*}
\im(f^*) &=\{\phi\in V^*:\exists \psi\in W^*:f^*(\psi)=\psi\circ f=\phi\}\\
&=\{\psi\circ f:\psi\in W^*\}
\end{align*}
Sei $\phi\in\im(f^*)$, also $\exists \psi\in W^*$.
Zeige: $\phi\in(\ker f)^0$, d.h. $\phi(\ker f)=0$.

Sei $v\in \ker(f)$, d.h. $f(v)=0$.
\[
\phi(v)=(\psi\circ f)(v)=\psi(f(v))=\psi(0_V)=0_K
\]
Also ist $\im(f^*)\subset (\ker f)^0$.

Sei $\phi\in (\ker f)^0$, d.h. $\phi(\ker f)=0$.
Zeige: $\phi\in\im(f^*)$, dh. $\exists\psi\in W^*$ mit $\phi=f^*(\psi)=\psi\circ f$.

\begin{center}
\makebox[0pt]{
\begin{xy}
(0,3)*+{\overbrace{V}^{v_1, ... v_r}}; (20, -20)*+{K}; (32,3)*+{\overbrace{W}^{w_1, ..., w_s}\supset \overbrace{\im f}^{f(v_1),...,f(v_r)}};
{\ar (2,0)*+{}; (17,0)*+{}}?*!/_2mm/{\scriptstyle f};
{\ar (20, -2)*+{}; (20,-20)*+{K}}?*!/_2mm/{\scriptstyle\psi};
{\ar (1,-1)*+{}; (20,-20)*+{K}}?*!/^6mm/{\scriptstyle\psi\circ f = f^*(\psi)};
{\ar@/_1.75pc/(-10,2.5)*+{\overbrace{\ker(f)}^{u_1, ..., u_n} \in}; (12, -20)*+{0 \in}};


\end{xy}  }      \end{center}

Um $\psi$ zu definieren, wählen wir Basen: $u_1,\dotsc,u_n$ für $\ker f$ und ergänze durch $v_1,\dotsc,v_r$ zu Basis von V.
Also ist
\[
f(v_1),\dotsc,f(v_r)
\]
ist Basis von $\im(f)$.

Eränze $f(v_1),\dotsc,f(v_n)$ durch $w_1,\dotsc,w_s$ zu Basis von $W$.
Um $\psi$ zu definieren, geben wir Bilder der Basisvektoren vor.
\begin{align*}
w_1,\dotsc,w_s&:\psi(w_j):=0 \\
f(v_1),\dotsc,f(v_r)&:\psi(f(v_i)):=\phi(v_i)
\end{align*}
Das definiert die lineare Abbildung: $\psi: W\to K$.

Zeige: $\phi=\psi\circ f$, d.h. $\phi(v)=(\psi\circ f)(v)\forall v\in V$ (genügt für $v$ Basisvektor $u_i$ oder $v_j$).
\begin{align*}
\phi(u_i)&=0= \psi(0)=\psi(f(v_i))\\
\phi(v_j)&=\psi(f(v_j))
\end{align*}
Also gilt $\phi\in \im(f^*)$.
\end{proof}
\end{prop}

\begin{kor}[Zeilenrang gleich Spaltenrang]
\label{kor:9.14}
Sei $A\in\Mat(m\times m,K)$.
Dann stimmen Zeilen und Spaltenrang von $A$ überein.
\begin{proof}
Sei $f:V\to W$ die zu $A$ lineare Abbildung.
Man wählt $V=K^n, W=K^m$.
\[
\text{Spaltenrang}(A)=\dim\im(f)
\]
Denn Spalten von $A$ sind die Bilder der Basisvektoren.
$f^*:W^*\to V^*$ ist gegeben durch $A^T$.
\[
\implies \dim \im(f^*)=\text{Spaltenrang}(A^T)=\text{Zeilenrang}(A)
\]
Es gilt
\[
\dim\im(f^*)=\dim(\ker(f))^0=\dim V-\dim\ker(f) = \dim \im f
\]
Also
\[
\underbrace{\dim\im(f)}_{\text{Spaltenrang von $A$}}=\underbrace{\dim\im(f^*)}_{\text{Zeilenrang von $A$}}
\]
\end{proof}
\end{kor}

\begin{thm}
\label{thm:9.15}
Sei $V$ ein $K$-Vektorraum mit $\dim V=n$ und $X\subset V$ ein $l$-dimensionaler affiner Unterraum von $V$.
Setze $r:=n-l$.
Dann existieren Linearformen $\phi_1,\dotsc,\phi_r \in V^*$ und Skalare $b_1,\dotsc,b_r \in K$, sodass gilt:
\[
X=\{u\in V:\phi_1(u)=b_1,\dotsc,\phi_r(u)=b_r\}
\]
wobei $r$ die kleinste Zahl mit dieser Eigenschaft ist.

$X$ ist der Lösungsraum eines inhomogenen linearen Gleichungssystems $Ax=b$ (mit $A \in \Mat(r\times n,K), b\in K^r$).
A ist durch die Wahl einer Basis durch die Linearformen festgelegt.

\begin{note}
Vergleiche \ref{kor:8.12}.
\end{note}

\begin{proof}
Sei $X=v+W$ für ein $v\in V$, $\dim W=l$, $W<V$.
\[
\dim W=l \implies \dim W^0=n-l=r
\]
Wähle Basis $\phi_1,\dotsc,\phi_r$ von $W^0$.
Diese beschreibt $W$:
\[
W=\{u\in V:0=\phi_1(u)=\dotsb=\phi_r(u)\}
\]
Gesucht sind $v+W$.
Setze \[b:=\begin{pmatrix}\phi_1(v)\\\vdots\\\phi_r(v)\end{pmatrix}\]
Dann ist, weil $\phi_1(v+u)=\phi_1(v)+\phi_1(u)$:
\begin{align*}
	\{x\in V: \phi_1(x)=\phi_1(v),\dotsc,\phi_r(x)=\phi_r(v)\}
	&= \{x\in V : \phi_1(x-v)=0,\dotsc,\phi_r(x-v)=0\}\\
	&= v + \{x\in V : \phi_1(x)=0,\dotsc,\phi_r(x)=0\} = v + W
\end{align*}
\end{proof}
\end{thm}

%\setcounter{section}{9}
\chapter{Eigenwerte und Eigenvektoren}

\section{Motivation}

Sei $\dim V=n, \dim W=l, \phi:V\to W$ linear.
Dann existiert nach dem Gauß-Verfahren eine Basis $v_1,\dotsc,v_n$ von $V$ und eine Basis $w_1,\dotsc,w_l$ von W, sodass
$\phi$ bezüglich dieser Basen folgende darstellende Matrix hat:

\[
 A= \begin{pmatrix}
     1 &\ 0 &\ \cdots &\ \cdots &\ \cdots &\ 0\\
     0 &\ \ddots &\ \ddots &\ &\  &\ \vdots  \\
     \vdots &\ \ddots &\ 1 &\ \ddots &\ &\ \vdots \\
     \vdots &\  &\ \ddots &\ 0 &\ \ddots &\ \vdots \\
     \vdots &\  &\  &\ \ddots &\ \ddots &\  0 \\
     0 &\ \cdots &\ \cdots &\  \cdots &\ 0 &\ 0
    \end{pmatrix}
\]


Ist also eine Matrix $B$ gegeben, dann existieren invertierbare Matrizen $S$ und $T$, sodass
\[
SBT^{-1}=A
\]
Wir suchen im Fall $V=W$ den Ausdruck $B^n$ für große $n$ oder für $n\to\infty$.
\[
A=SBT^{-1} \implies A^2=SBT^{-1}SBT^{-1}
\]
Falls $S=T$, könnten wir leicht $A^n=SB^nT$ berechnen.
$S\neq T$ hilft uns nicht weiter.

$S=T$ bedeutet: gleiche Basis in $V$ und $W$.

Wir suchen also eine Basis in $V$, so dass aus der Matrix $A$ die darstellende Matrix
\[
SAS^{-1}
\]
bezüglich der neuen Basis wird mit Basiswechselmatrix $S$.

Der Vorteil dieser Wahl liegt klar auf der Hand.
\[
(SAS^{-1})^n = SA^nS^{-1}
\]
Können wir $S$ so wählen, dass $SAS^{-1}$ in Diagonalform ist?
\begin{ex}
\begin{itemize}
\item Sei $\phi=\Id$ mit beliebiger Basis $v_1,\dotsc,v_n$.
Dann ist A die Einheitsmatrix
\item Sei $\lambda\in K\setminus\{0, 1\}$ und $\phi=\lambda\Id$ mit beliebiger Basis $v_1,\dotsc,v_n$.
Da $\phi(v_i)=\lambda v_i$ gelten muss, können wir keine $1$ oder $0$ in der Diagonalen erhalten.
\end{itemize}
\end{ex}
Sei $\phi$ gegeben durch $A=\begin{psmallmatrix}1&0\\0&2\end{psmallmatrix}$.
\[
\phi(v_1+v_2)=\phi(v_1)+\phi(v_2)=v_1+2v_2\neq\lambda(v_1+v_2)
\]
wähle neue Basis $v_1,v_1+v_2$.
\begin{align*}
\phi(v_1)&=v_1 \\
\phi(v_2)&=v_1+2v_2=2(v_1+v_2)-v_1
\end{align*}
Also sieht die neue Matrix folgendermaßen aus:
\[
B=\begin{pmatrix}1&-1\\0&2\end{pmatrix}
\]
$B$ beschreibt die selbe Abbildung, aber ist keine Diagonalmatrix.
Bei $A$ gilt für ein $\lambda_i\in K$:
\[
\phi(v_i)=\lambda_i v_i
\]
Wir suchen also eine Matrix zu $\phi$ gegeben durch
\[
\begin{pmatrix}\lambda_1&0&\dotsc&0\\0&\lambda_2&\dotsc&0\\\vdots&\vdots&\ddots&\vdots\\0&0&\cdots&\lambda_n\end{pmatrix}
\]
Also $\phi(v_i)=\lambda_i v_i$

\section{Diagonalisierbarkeit}

\begin{df}
\label{df:10.1}
Sei $V$ ein $K$-Vektorraum und $\phi:V\to V$ eine lineare Abbildung.
Dann heißt $\phi$ ein \emph{Endomorphismus} von $V$.

Ein Skalar $\lambda\in K$ heißt \emph{Eigenwert} von $\phi$ genau dann, wenn
\[
\exists v_0\in V\setminus\{0\}:\; \phi(v_0)=\lambda v_0
\]
Ein solches $v_0$ heißt \emph{Eigenvektor} von $\phi$ zum Eigenwert $\lambda$.
\end{df}

\begin{prop}
\label{prop:10.2}
Sei $\dim V < \infty$ und $\phi:V\to V$ linear.
Dann existiert eine Basis von V bezüglich derer $\phi$ durch eine Diagonalmatrix beschrieben ist
\emph{genau dann}, wenn es eine Basis von $V$ aus Eigenvektoren von $\phi$ gibt.

In diesem Fall heißt $\phi$ \emph{diagonalisierbar} und jede darstellende Matrix zu $\phi$ (bezüglich beliebiger Basis)
heißt ebenfalls diagonalisierbar.

Mit anderen Worten:
Eine Matrix $A\in\Mat(n\times n, K)$ ist also diagonalisierbar genau dann, wenn
\[
\exists S\in \GL(n,K):\; SAS^{-1} \text{ Diagonalmatrix}
\]
\begin{proof}
Die Diagonalmatrix ergibt sich genau aus den Eigenwerten zu den Eigenvektoren von $\phi$.
\end{proof}
\end{prop}

Sei $V=\R^2$ und $\phi$ eine Drehung um $\alpha\neq 0,\pi,\dotsc$ .
Dann gilt:
\[
v\neq \lambda\phi(v) \qquad \forall\lambda\in K,v\neq 0
\]
$\phi$ kann also nicht durch eine Diagonalmatrix beschrieben werden.

Es können also offensichtlich nicht alle Matrizen diagonalisierbar sein.
Wann ist eine Matrix $A$ diagonalisierbar und wenn ja, wie sehen ihre Eigenvektoren und Eigenwerte von $A$ aus?

\begin{prop}
\label{prop:10.3}
Sei $\phi:V\to V$ linear und $v_1,\dotsc,v_l$ Eigenvektoren zu paarweise verschiedenen Eigenwerten $\lambda_1,\dotsc,\lambda_l$.
Dann sind $v_1,\dotsc,v_l$ linear unabhängig.

Wenn $\dim V=l$, dann ist $\phi$ diagonalisierbar und die Diagonalmatrix ergibt sich durch die Eigenwerte auf der Hauptdiagonalen.
\begin{proof}
	\fixme[Diese Induktion stimmt immernoch nicht]

Sei $0=\sum_{k=1}^l\my_kv_k$ für $\my_i\in K$.
Induktion nach $l$:
Für $l=1$ ist: $0=\my_1v_1 \implies \my_1=0$, da $v_1\neq 0$.
Weiter gilt
\[
0=\phi(0)=\phi\left(\sum_{k=1}^l\my_kv_k\right)=\sum_{k=1}^l\my_k\phi(v_k)=\sum_{k=1}^l\my_k\lambda_kv_k
\]
Es ist also
\[
	\sum_{k=1}^l\my_k\lambda_kv_k = 0 = \lambda_l 0= \sum_{k=1}^l\lambda_l\my_kv_k = \sum_{k=1}^l\my_k\lambda_{l}v_k
\]
Subtrahiert man links von rechts erhält man
\[
	0= \sum_{k=1}^{l}\my_k(\lambda_{l}-\lambda_k)v_k 
\]
Die $v_i$ sind nach Induktion linear unabhängig und $\forall k\neq l:\lambda_{l}-\lambda_k \neq 0$.
Also:
\[
0=\my_1=\my_2=\dotsb=\my_{l}
\]
%na gut, ich habe grade gesehen, dass da was fehlt :)  Wie kommst du aber auch auf l+1 
%Wenn wir die Induktionsvorraussetzung, dass v_1,…,v_l linear unabhängig sind verwenden, müssen wir zeigen, dass
% v_1,…,v_{l+1} linear unabhängig sind. Durch diesen „Beweis“ haben wir nichts gewonnen, wir zeigen nur nochmal die Induktionsvorraussetzung
\end{proof}
\end{prop}

\begin{prop}
\label{prop:10.4}
Sei $\phi:V\to V$ linear und $V$ endlich dimensional.
Ein Skalar $\lambda\in K$ ist genau dann Eigenwert von $\phi$ genau dann, wenn
\[
\det(\phi-\lambda\Eins)=0
\]
\begin{proof}
\begin{align*}
&\det(\phi-\lambda\Eins)=0\\
\iff& \ker(\phi-\lambda\Eins) \text{ nicht trivial}\\
\iff& \exists v_0\neq 0:v_0\in \ker(\phi-\lambda\Eins)\\
\iff& \exists v_0\neq 0:(\phi-\lambda\Eins)(v_0)=0\\
\iff& \exists v_0\neq 0:\phi(v_0)=\lambda v_0
\end{align*}
\end{proof}
\end{prop}

\begin{note}
Betrachte bei Matrizen einfach $\det(A-\lambda I)$.
Die Basiswahl für die Matrix spielt dabei keine Rolle, denn
\begin{align*}
\det(SAS^{-1}-\lambda I)&= \det(SAS^{-1}-\lambda SIS^{-1})=\det(S(A-\lambda I)S^{-1}) \\
&=\det S\cdot\det (A-\lambda I)\cdot \det S^{-1} = \det(A-\lambda I)
\end{align*}
\end{note}

\begin{alg*}[Bestimmung von Eigenwerten und Eigenvektoren]
	\begin{algorithmic}
		\Input Matrix $A\in \Mat(n\times n,K)$
		\Output Eigenwerte $\lambda_i$ mit zugehörigen Eigenvektoren $v_i$
		\Statex
		\State Bestimme in $\det(A-\lambda I)=0$ alle möglichen Lösungen für $\lambda$ als Eigenwerte
		\State Betrachte $\lambda$ in $\det(A-\lambda I)=0$ als Vektor der Eigenwerte.
		\State Die Eigenvektoren sind die Lösungen des homogenen Gleichungssytems $\det(A-\lambda I)=0$
	\end{algorithmic}
\end{alg*}

\begin{note}
Noch ist zu klären, was denn $\det(A-\lambda I)$ mit Unbekannten $\lambda$ ist; eine sogenannte „Polynomiale Gleichung für $\lambda$“
\end{note}

\begin{ex}
Sei $V=\R^2$.
\begin{itemize}
\item
 Sei $\phi$ die Funktion, die eine Drehung um Winkel $\alpha \in [0,2\pi)$ beschreibt.
\begin{align*}
A&=\begin{pmatrix}\cos \alpha & -\sin\alpha\\ \sin \alpha &\cos \alpha\end{pmatrix}\\
A-x I&=\begin{pmatrix}\cos \alpha -x  & -\sin\alpha\\ \sin \alpha &\cos \alpha -x\end{pmatrix}\\
\det(A-xI)&=(\cos \alpha -x)^2+\sin^2\alpha\\
&=\cos^2\alpha +x^2-2\cos x+\sin^2 \alpha\\
&=x^2-2x\cos \alpha + 1\\
&=(x-\cos\alpha)^2+1-\cos^2\alpha=0\\
&\iff \underbrace{(x-\cos \alpha)^2}_{\ge 0}=\cos^2\alpha - 1
\end{align*}
Aber
\[
\cos^2\alpha-1\ge 0 \iff \alpha\in\{0,\pi\}
\]
Also existieren für $\alpha\not\in\{0,\pi\}$ keine (reellen) Eigenwerte.
Sonst gilt
\begin{align*}
\alpha=0&\implies \phi=\Id \implies \lambda=1 \qquad \text{ und jeder Vektor ist Eigenvektor} \\
\alpha=\pi&\implies \phi=-\Id \implies \lambda=-1 \qquad \text{ und jeder Vektor ist Eigenvektor}
\end{align*}

\item
\begin{align*}
A&=\begin{pmatrix}\cos\alpha & \sin \alpha\\\sin \alpha&-\cos\alpha\end{pmatrix}\\
A-xI&=\begin{pmatrix}\cos\alpha-x & \sin \alpha\\\sin \alpha&-\cos\alpha-x\end{pmatrix}\\
\det(A-xI)&=-(\cos^2\alpha - x^2)-\sin^2\alpha=x^2-1=(x-1)(x+1)
\end{align*}
Also hat $A$ die Eigenwerte $\lambda_1=1, \lambda_2=-1$.
Nach \ref{prop:10.3} gibt es also zwei linear unabhängige Eigenvektoren, die eine Basis bilden.
Also ist $A$ nach \ref{prop:10.2} diagonalisierbar, so mit einer Basiswechselmatrix $S$ gilt:
\[
SAS^{-1}=\begin{pmatrix}1&0\\0&-1\end{pmatrix}
\]
\underline{Bestimmung der Eigenvektoren:}\\
Löse folgendes homogene lineare Gleichungssystem:
\begin{align*}
A-\lambda I=\begin{pmatrix}\cos\alpha-1&\sin\alpha \\ \sin\alpha & -\cos\alpha -1\end{pmatrix}
\begin{pmatrix}x\\y\end{pmatrix}=
\begin{pmatrix}0\\0\end{pmatrix}
\end{align*}
Sei $\sin \alpha\neq 0$
\begin{align*}
x=\frac{1+\cos \alpha}{\sin \alpha}y
\end{align*}
Es gilt:
\begin{align*}
\sin\alpha &= 2\sin\frac{\alpha}2\cos\frac{\alpha}2\\
\cos\alpha &= \cos^2\frac\alpha2-\sin\frac\alpha2\\
\cos^2\frac\alpha2&+\sin^2\frac\alpha2=1
\end{align*}
Also
\begin{align*}
x&=\frac{1+\cos\alpha}{\sin\alpha}\\
&=\frac{(\cos^2\frac\alpha2+\sin^2\frac\alpha2)+(\cos^2\frac\alpha2-\sin^2\frac\alpha2)}{2\sin\frac\alpha2\cos\frac\alpha2}\\
&=\frac {2\cos\frac\alpha2\cos\frac\alpha2}{2\sin\frac\alpha2\cos\frac\alpha2}y
\end{align*}
Setze $y=\sin\frac\alpha2\implies x=\cos\frac\alpha2$.
Also:
\[
\begin{pmatrix}x\\y\end{pmatrix}=\begin{pmatrix}\cos\frac \alpha2\\\sin\frac\alpha2\end{pmatrix}
\]
Eigenvektoren zu $\lambda_1=1$ sind:
\[
\my\cdot \begin{pmatrix}\cos\frac\alpha2\\ \sin\frac\alpha2\end{pmatrix}
\]
und zu $\lambda_2=-1$ sind:
\[
\my\cdot \begin{pmatrix}\cos\frac{\alpha+\pi}2 \\ \sin\frac{\alpha +\pi}2\end{pmatrix}
\]
Bezüglich der Basis $v_1,v_2$ ergibt sich die abbildende Matrix:
\[
\begin{pmatrix}1&0\\0&-1\end{pmatrix}
\]
Die Abbildung ist also die Spiegelung an der Geraden durch alle Eigenvektoren zum Eigenwert $\lambda_1$.
\end{itemize}
\end{ex}

\begin{prop}
\label{prop:10.5}
Sei $\phi:V\to V$ linear und $\lambda$ Eigenwert von $\phi$.
Dann ist die Menge der Eigenvektoren (hier inklusive dem Nullvektor):
\[
\{v\in V:\phi(v)=\lambda v\}
\]
ein Untervektorraum von $V$.
Dieser Raum heißt \emph{Eigenraum} von $\phi$ zum Eigenwert $\lambda$.

\begin{proof}
Sei $\phi(v_1)=\lambda v_1$ und $\phi(v_2)=\lambda v_2$
\begin{alignat*}{3}
	\phi(v_1+v_2)&=\phi(v_1)+\phi(v_2)&&=\lambda v_1+\lambda v_2 &&= \lambda(v_1+v_2)\\
 \phi(\my v_1)&=\my\phi(v_1)&&=\my\lambda v_1&&=\lambda (\my v_1)
\end{alignat*}
\end{proof}
\begin{note}
Vereinigungen von Eigenräumen zu verschiedenen Eigenwerten bilden keine Unterräume von $V$.
\end{note}
\end{prop}

\section{Über Polynome und Ringe}

Was ist $\det(A-xI)$?
Die Determinante ist bisher nur definiert für Matrizen mit Einträgen im Körper $K$.
\[
\det(A-xI)=a_nx^n+a_{n-1}x^{n-1}+\dotsb+a_1x+a_0 \qquad \text{ mit }a_i \in K
\]
ist ein Polynom in $x$.
Ein Polynom $f(x)$ ist im Allgemeinen nicht das selbe wie eine Funktion $f:K\to K$.
$f(x)$ definiert $f:K\to K$ für $\lambda\in K$, wenn man setzt:
\[
f(\lambda):= a_n\lambda^n+\dotsb+a_1\lambda+a_0
\]

\begin{ex}
Sei $K=\F_2=\{0,1\}$.
\begin{align*}
f_1(x)&=0\\
f_2(x)&=x^2+x\\
f_3(x)&=x^3+x\\
f_4(x)&=x^{12}+x^{7}+x^5+x^2\\
\end{align*}
\begin{table}[h]
\begin{tabular}{l|r|r}
 & 0 & 1\\
\hline
$f_1(\lambda)$ & 0 & 0\\
$f_2(\lambda)$ & 0 & 0\\
$f_3(\lambda)$ & 0 & 0\\
$f_4(\lambda)$ & 0 & 0
\end{tabular}
\end{table}

Die zugehörigen Funktionen sind gleich, obwohl die Polynome verschieden sind.
\end{ex}

\begin{df}
\label{df:10.6}
Sei $K$ ein Körper.
Ein \emph{Polynom} mit Koeffizienten in $K$ ist eine Folge $(a_0, a_1, \dotsc)\in K^{\N_0}$ mit
$a_i\in K$ ($i\in \N_0$), sodass
\[
\exists N\in \N_0 \forall n\ge N:\; a_n=0
\]
Die Nullfolge ($a_i=0 \quad\forall i\in\N_0$) heißt \emph{Nullpolynom}.

Für ein Polynom $P\neq$ Nullpolynom, definieren wir den \emph{Grad} des Polynoms
\[
\deg P:= \max\{i:a_i\neq 0\}
\]
Für das Nullpolynom $P$ definiere
\[
\deg P:= -\infty
\]

Sei $P$ kein Nullpolynom, $\deg P=n$, dann schreiben wir
\[
P=a_nx^n + a_{n-1}x^{n-1} + \dotsb + a_1x + a_0
\]
mit einem Symbol $x$.
Für das Nullpolynom schreiben wir:
\[
P=0
\]

Falls für ein Polynom $P$ gilt:
\[
a_{\deg P}=1
\]
dann heißt $P$ \emph{normiert}.

Falls
\[
\deg P\le 0
\]
dann heißt $P$ \emph{konstantes Polynom}.

Die Menge aller Polynome mit Koeffizienten in $K$ wird mit:
\[
K[x]
\]
bezeichnet (mit Symbol $x$).
\end{df}

\subsection{Was ist die algebraische Struktur von $K[x]$?}

\begin{prop}
\label{prop:10.7}
Die Menge $K^\N$ der Folgen mit Einträgen in $K$ ist ein $K$-Vektorraum mit Addition
\[
(a_i)_{i\in\N} + (a_i)_{i\in\N} = (a_i+b_i)_{i\in\N}
\]
und skalarer Multiplikation
\[
\lambda (a_i)_{i\in\N}=(\lambda a_i)_{i\in \N}
\]
Das neutrale Element ist die Nullfolge $(0, 0, \dotsc)$.

Die Menge $K[x]$ ist ein Unterraum.
Für $P,Q\in K[x]$ gilt:
\[
\deg(P+Q)\le \max\{\deg P, \deg Q\}
\]
\end{prop}

Multiplikation bei Folgen:\\
Eintragsweise funktioniert nicht auf Polynomen.
Definere:
\[
(a_i)_{i\in\N}\cdot (b_i)_{i\in \N}=(c_i)_{i\in \N} \qquad \text{ mit } c_i=\sum_{h+j=i}a_hb_j
\]
Das ergibt die bekannte Multiplikation von Polynomen:
\[
\left(\sum a_hx^h\right)\cdot\left(\sum b_jx^j\right)=\sum c_ix^i \qquad \text{ mit } c_i=\sum_{h+j=i}a_hb_j
\]

$K[x]$ hat Addition, also ist $K[x]$ abelsche Gruppe.
Für die Multiplikation existiert das neutrale Element $1_{K[x]}=1=(1,0,\dotsc)$.
aber es existieren keine inversen Elemente, denn
\begin{align*}
x^2\cdot f(x)\neq 1 \qquad \forall f(x)\in K[x]
\end{align*}
$K[x]$ kann also kein Körper sein.

Sei $f$ ein Polynom mit Koeffizienten aus K $(a_0, a_1, ..., \overbrace{a_n}^{\neq 0}, \overbrace{a_{n+1}}^{=0}, ...)$.
Wir schreiben dafür
\[
	f(x)=a_nx^n+...+a_1x+a_0
\]
mit $\deg f=n$ für $f \neq 0$, sonst $\deg 0 = - \infty$.

$K[x]$ ist ein K-Vektorraum bezüglich Polynomaddition und skalarer Multiplikation auf Polynome.
Wir haben nun zusätzlich auch eine Polynommultiplikation eingeführt. 
Jedoch erfüllt $K[x]$ nicht alle Körperstrukturen ($(K[x],+)$ ist abelsche Gruppe, $(K[x], \cdot)$ ist kommutative Halbgruppe mit Einselement).
So gibt es aber Polynome die kein Inverses besitzen.  
Wir brauchen also eine andere Struktur, um $K[x]$ zu beschreiben.

\begin{df}
\label{df:10.8}
Ein \emph{Ring (mit Einselement)} ist eine Menge $R$ mit zwei Verknüpfungen:
\[
+:R\times R\to R \quad \text{ und }\quad  \cdot:R\times R\to R
\]
und zwei ausgezeichneten Elementeen $0$ und $1\neq 0$, so dass gilt:
\begin{enumerate}[({R}1)]
\item
$(R,+)$ ist eine abelsche Gruppe mit neutralem Element $0$, d.h.:
\begin{alignat*}{3}
	\forall a\in R&: &a+0&=a\\
\forall a,b\in R&: &a+b&=b+a\\
\forall a\in R\exists b\in R&: &a+b&=0 \qquad (-a:=b) \\
					  \forall a,b,c\in R&:&(a+b)+c&=a+(b+c)
\end{alignat*}
\item
	\begin{alignat*}{3}
		\forall a\in R&:&a\cdot 1&=a\\
\forall a,b,c\in R&:&a\cdot(b\cdot c)&=(a\cdot b)\cdot c
\end{alignat*}
\item
\begin{align*}
\forall a,b,c\in R: a\cdot(b+c)&=a\cdot b + a\cdot c\\
(a+b)\cdot c&=a\cdot c+b\cdot c
\end{align*}
\end{enumerate}
Falls zusätzlich
\[
\forall a,b\in R: a\cdot b=b\cdot a
\]
dann heißt $R$ \emph{kommutativ}
\end{df}

\begin{ex}
\begin{itemize}
\item
Ein Körper $K$ ist ein kommutativer Ring
\item
$R=\Z$ ist ein kommutativer Ring
\item
$R=\Z/n\Z$ ist ein kommutativer Ring ($n\ge 2$)
\item
$R=K[x]$ ist kommutativer Ring ($K$ Körper oder selber kommutativer Ring)
\item
$R=\Mat(n\times n, K)$ ist ein \emph{nicht} kommutativer Ring
\item
$R=\Abb(A,A)$ ist kommutativer Ring ($A$ kommutativer Ring) mit:
\begin{align*}
f+g:\lambda &\mapsto f(\lambda) +g(\lambda)\\
f\cdot g :\lambda &\mapsto f(\lambda) \cdot g(\lambda)
\end{align*}
\end{itemize}
\end{ex}

\begin{df}
\label{df:10.9}
Eine Abbildung $\phi:R\to S$ ($R, S$ Ringe) heißt \emph{Ringhomomorphismus} genau dann, wenn
\begin{align*}
\phi(a+_R b) &= \phi(a) +_S \phi(b) \qquad \forall a,b\in R\\
\phi(a \cdot_R b) &= \phi(a) \cdot_S \phi(b) \qquad \forall a,b\in R \\
\phi(1_R)&= 1_S
\end{align*}
$\phi$ heißt \emph{Ringisomorphismus}, falls er zusätzlich bijektiv ist.
\end{df}

\begin{ex}
\begin{itemize}
\item $\phi:\Z\to \Z/n\Z:a\mapsto [a]=\{a+n\Z\}$
\item $\phi: K[x]\to \Abb(K,K):f(x)\mapsto (\lambda\mapsto f(\lambda))$ ist ein Ringhomomorphismus, aber für endliche Körper nicht Ringisomorphismus.
\end{itemize}
\end{ex}

$R=\Z/4\Z$:
\begin{align*}
\bar2\neq \bar0, \bar2\cdot \bar2=\bar4=\bar0
\end{align*}
Im Körper dagegen folgt für $a\neq 0\neq b$, dass $a\cdot b\neq 0$.

\begin{df}
\label{df:10.10}
Sei $R$ ein kommutativer Ring.
Ein Element $0\neq r\in R$ heißt \emph{Nullteiler} genau dann, wenn
\[
\exists s\in R, s\neq 0:\; r\cdot s=0
\]
\end{df}

\begin{ex}
\begin{itemize}
\item
$\Z/p\Z$ für $p$ Primzahl ist ein Körper und es existiert kein Nullteiler.

Falls $p$ nicht Primzahl, so existieren $a,b$ mit $a,b\neq n=0$, sodass
\[
a\cdot b=n=0
\]
\item
Sei $f(x)$ Polynom von Grad $n$ und $g(x)$ Polynom von Grad $l$.
Dann ist
\begin{align*}
f(x)\cdot g(x) &= \left(\sum_{k=0}^na_kx^k\right)\left(\sum_{k=0}^lb_kx^k\right) \\
&=\sum_{k=0}^{n+l}c_kx^k
\end{align*}
Also
\[
\deg(f\cdot g)=\deg f +\deg g
\]
und $f,g\neq 0 \implies f\cdot g\neq 0$
\end{itemize}
\end{ex}

\begin{lem}
\label{10.11}
Für $P,Q\in K[x]\setminus\{0\}$ gilt
\[
\deg (P\cdot Q) = \deg P+\deg Q
\]
Inbesondere hat also $K[x]$ keine Nullteiler.
\end{lem}

\begin{df}
\label{10.12}
Seien $f,g\in K[x]$.
$f$ \emph{teilt} $g$ oder äquivalent: $f$ ist \emph{Teiler} von $g$ genau dann, wenn
\[
\exists h\in K[x]: g=f\cdot h
\]
$f$ heißt \emph{echter Teiler} von $g$, wenn zusätzlich gilt:
\[
0 < \deg f < \deg g
\]
$g$ heißt \emph{irreduzibel} (oder \emph{prim}) \emph{genau dann}, wenn
\[
\text{$g$ hat keine echten Teiler und $\deg g > 0$}
\]
Notation:
\[
f|g \text{ bedeutet } f \text{ teilt } g
\]
\end{df}

\underline{Ziel:} Teilbarkeitstheorie für Polynome, analog zur Zahlentheorie

\begin{lem}
\label{lem:10.13}
\begin{enumerate}[(a)]
\item
Lineare Polynome $a_1x+a_0$ ($a_1\neq 0$) sind irreduzibel.
\item
$fh_1=fh_2$, $f\neq 0$ impliziert $h_1=h_2$ (Kürzungsregel).
\item
$f|g$ und $g|h \implies f|h$
\item
$f|g$ und $f|h \implies f|(g+h)$
\end{enumerate}
\begin{proof}
\begin{enumerate}[(a)]
\item
\begin{align*}
a_1x+a_0=f\cdot g &\implies 1=\deg(f\cdot g) =\deg f + \deg g\\
&\implies \deg f = 0, \deg g=1 \qquad \text{(oder umgekehrt)}
\end{align*}
	Echte Teiler haben jedoch $\deg > 0$, also ist $a_1x + a_0$ irreduzibel.
\item
\begin{align*}
fh_1=fh_2 \implies f(h_1-h_2)=0
\end{align*}
Da $K[x]$ keine Nullteiler besitzt und nach Vorraussetzung $f\neq 0$ gilt
\[
h_1-h_2=0
\]
\item
Es gilt $g=fh_1$ und $h=gh_2$ für $h_1,h_2\in K[x]$.
\[
h=gh_2=(fh_1)h_2=f(h_1h_2)\implies f|h
\]
\item
Es gilt $g=fh_1$ und $h=fh_2$ für $h_1,h_2\in K[x]$.
\[
g+h=fh_1+fh_2=f(h_1+h_2)
\]
\end{enumerate}
\end{proof}
\end{lem}

\begin{thm}[Division mit Rest]
\label{thm:10.14}
Seien $f,g\in K[x]$ und $g\neq 0$.
Dann existieren $q,r\in K[x]$, mit
\[
f=qg+r
\]
mit $\deg r < \deg g$.
$q$ und $r$ sind eindeutig bestimmt.

\begin{proof}
	\begin{seg}{Existenz}
		$f=0 \implies q=r=0$.
Sei also $f\neq 0$.
Induktion nach $\deg f$.
Wenn $\deg f = 0$, dann $f=a_0$.
Für $\deg g=0$ ergibt sich
\[
a_0=\frac {a_0}{b_0}b_0 + 0\
\]
Und für $\deg g > 0$
\[
a_0=0\cdot g + a_0
\]
Sei nun $\deg f>0$
\begin{align*}
f(x)&=\sum_{k=0}^na_kx^k\\
g(x)&=\sum_{k=0}^lb_kx^k
\end{align*}
Für $\deg f < \deg g$ ist
\[
f=0\cdot g + f \implies r=f
\]

Sei jetzt $\deg f\ge \deg g$.
Sei
\[
f_1:=f-\frac{a_n}{b_l}x^{n-l}g
\]
Es ergibt sich ein Polynom von Grad $\le n-1$.
Nach Induktionsvorrausetzung existierten $q_1,r_1$, sodass
\[
f_1=q_1g+r_1 \quad \deg r_1<\deg g
\]
\begin{align*}
f&=f_1+\frac {a_n}{b_l}x^{n-l}g\\
&=q_1g+r_1+\frac {a_n}{b_l}x^{n-l}g\\
&=\underbrace{\left(q_1+\frac{a_n}{b_l}x^{n-l}\right)}_{=:q}g+\underbrace{r_1}_{=:r}
\end{align*}
Also ist
\[
f=qg+r
\]
mit $r=r_1$ und $\deg r<\deg g$
\end{seg}
\begin{seg}{Eindeutigkeit}

Seien $f=q_1g+r_1=q_2g+r_2$, dann ist
\[
0=f-f=q_1g+r_1-q_2g-r_2=(q_1-q_2)g+r_1-r_2
\]
\begin{align*}
\implies \underbrace{(q_1-q_2)g}_{\deg{\dotsc}=\deg(q_1-q_2)+\deg g} &=\underbrace{r_2-r_1}_{\deg{\dotsc}<\deg g}\\
\implies q_1-q_2&=0=r_1-r_2
\end{align*}
\end{seg}
\end{proof}
\end{thm}

\begin{ex}
$f(x)=x^6$, $g(x)=x^2-1$.
Man führe die bekannte Polynomdivision durch.
\begin{align*}
\Rightarrow x^6&=(x^2-1)(x^4+x^2+1)+1 \\
\Rightarrow x^6-1 &= (x^2-1)(x^4+x^2+1)
\end{align*}
\end{ex}

\begin{kor}
\label{kor:10.15}
Sei $f\in K[x], f\neq 0$ und $a\in K$ eine Nullstelle, d.h. $f(a)=0$.
Dann existiert ein $q\in K[x]$ mit
\[
f(x) = (x-a)q(x)
\]
Falls $\deg f= n$, so hat $f$ höchstens $n$ Nullstellen.
\begin{proof}
Nach \ref{thm:10.14} gilt: $f(x)=q(x)(x-a)+r(x)$.
mit $\deg r < \deg g = 1$.
Einsetzen von $a$ liefert:
\[
0=f(a)=q(a)(a-a)+r(a)
\]
Also ist $r(a)=0$ und da $r$ konstant, gilt: $r=0$.

Sei $\deg f=n\ge 0$ mit paarweise verschiedene Nullstellen $a_1,\dotsc,a_l\in K$.
\begin{align*}
f(x)&=(x-a_1)q_1(x)\\
n= \deg f &= \deg(x-a) +\deg q_1(x)\\
\Rightarrow \deg q_1(x) &=n-1\\
\underbrace{f (a_2)}_{=0} &=\underbrace{(a_2-a_1)}_{\neq 0}q_1(a_2)
\end{align*}
also $q_1(a_2) = 0$ und ebenso $q_1(a_3)=\dotsb=q_1(a_l)=0$.
Analog folgt: $\exists{q_2}: q_1(x)=(x-a_2) q_2(x)$.
Daher folgt induktiv
\begin{align*}
f(x)&=(x-a_1)(x-a_2)\dotsb (x-a_l)q_l(x)\\
\implies n &= 1 + 1 +\dotsb + 1 + \deg q_l=l+ \deg q_l
\end{align*}
$\implies l\le n$
\end{proof}
\end{kor}

\begin{note} Wenn $K$ unendlich viele Elemente hat, 
ist der Ringhomomorphismus $K[x]\to \Abb(K,K) : f(x)\mapsto (\lambda \mapsto f(\lambda))$ injektiv.

\begin{proof}
Insbesondere ist $K[x]\to \Abb(K,K) : f(x)\mapsto (\lambda \mapsto f(\lambda))$ auch lineare Abbildung.
Betrachte man den Kern der Abbildung, gäbe es ein Polynom $P$ mit $\deg P<\infty$, sodass die zugehörige Abbildung, die 
Nullabbildung ist und das zugehörige Polynom nach \ref{kor:10.15} unendlich viele Nullstellen besitzt, dies steht aber im 
Widerspruch dazu, dass das Polynom einen endlichen Grad besitzt.
\end{proof}
\end{note}

Die Existenz von Nullstellen hängt von $K$ ab:
\[
f(x)=x^2-2
\]
hat in $\R$ zwei Nullstellen $\pm \sqrt{2}$, aber in $\Q$ keine:
Angenommen $\sqrt 2\in \Q \implies \exists_p,q \sqrt(2)=\frac{p}{q}$ ($p, q \in \Z, q \neq 0, p$ und $q$ teilerfremd)
\begin{align*}
\left(\frac pq\right)^2=2
&\implies p^2=2q^2
\implies p \text{ gerade}\\
2 | p \implies 4 | p^2
&\implies 2 | q^2
\implies \text{q gerade}
\end{align*}

Es ergibt sich der Widerspruch zur Teilerfremdheit zwischen $p$ und $q$.

$g(x)=x^2+1$ hat in $\R$ keine Nullstellen, da
\[
\forall x\in\R:x^2\ge 0
\]
In $\C$ dagegen existieren die Nullstellen $\pm i$.
Nach dem \emph{Fundamentalsatz der Algebra} hat jedes $f(x)\in \C[x]$, mit $\deg f\ge 1$ mindestens eine Nullstelle in $\C$.

Sei $K$ ein kommutativer Ring, $M\in \Mat(n\times n, K)$.
Dann existiert die Determinante $\det M$
\[
\det M := \sum_{\sigma\in\sum_n}(\sgn\pi)m_{1\sigma(1)}\cdot\dotsb\cdots m_{n\sigma(n)}
\]
Es gilt der Entwicklungssatz und $\det M \cdot I = M\cdot \adj(M)^T$.
Aber es gilt nicht:
\[
M=\frac 1{\det M}\adj(M)^T
\]
da $\f 1{\det M}$ nicht definiert sein muss (multiplikatives Inverses existiert in einem Ring nicht).

\begin{ex}
\begin{align*}
M&=\begin{pmatrix}2&0\\0&2\end{pmatrix}\\
\det M&=4
\end{align*}
aber $M$ ist in $\Mat(2\times 2,\Z)$ nicht invertierbar, denn
\[
M^{-1} =\begin{pmatrix}\frac 12&0\\0&\frac 12\end{pmatrix} \not\in \Mat(2\times 2,\Z)
\]
Problem: $\det M$ ist in $\Z$ nicht invertierbar
\end{ex}

Sei $K$ ein Körper, $A\in \Mat(n\times n, K)$. $A-xI \in \Mat(n\times n,K[x])$, dann ist
$\det(A-xI)$ definiert.
Also $\det(A-xI)\in K[x]$ ein Polynom.

Die Nullstellen dieses Polynoms sind die Eigenwerte der Matrix $A$.

\begin{df}
\label{df:10.16}
Sei $V$ ein $K$-Vektorraum, $\dim V=n$ und $\phi:V\to V$ ein Endomorphismus und $A$ die Matrix zu $\phi$ bezüglich einer Basis von $V$.
Das Polynom
\[
\chi_{\phi}(x) = \chi_{A}(x) := \det(A-\lambda I)
\]
heißt das \emph{charakteristische Polynom} von $\phi$ und von $A$.
\end{df}

$\chi_A(x)$ wird für die Normalformen von $A$ entscheidend sein.

\[
A-xI = \begin{pmatrix}
a_{11}-x & a_{12} &\cdots & a_{1n}\\
a_{21}& a_{22}-x & \cdots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots \\
a_{n1}&\cdots &\cdots & a_{nn}-x
\end{pmatrix}
\]
Also ist $\det(A-xI)$ ein Polynom vom Grad $n$.
\[
\det(A-xI)=\sum_{k=0}^na_kx^k
\]

Was ist $a_n$? $a_{n-1}$? $a_0$?
\fixme
\[
\det(A-xE)=\sum_{\delta \in \Sigma_n} \cdots
\]

Für den Summanden $\sigma=\Eins$ gilt:
 \[
 (a_{11}-x)(a_{22}-x)...(a_{nn}-x)=(-1)^nx^n+(a_{11}+a_{22}+\cdots+a_{nn})x^{n-1}+...
 \]

Für die anderen Summanden finden wir stets $\delta(i)=j\neq i$  und verpassen damit die Einträge $a_{ii}-x$ und $a_{jj}-x$.  Daher  besitzen diese Summanden höchstens den Grad $n-2$. $\det(A-xI)$ setzt sich aus dem Summanden zu $\delta=id$ und den Summanden vom Grad $\leq n-2$ zusammen.  Daraus ergeben sich die wichtigen Koeffizienten:
\begin{align*}
a_n&=(-1)^n \\
a_{n-1}&=(-1)^{n-1} \underbrace{(a_{11}+a_{22}+\cdots+a_{nn})}_{\text{Spur von A}} \\
a_0&=\chi_\phi(0)=\det(A-0I)=\det A
\end{align*}

\begin{note}
Die Spur hängt ausschließlich von $\phi$ ab, die Basiswahl spielt keine Rolle.
Genauso $\det A$.
\end{note}
\begin{note}
Die Spur von A wird formal durch 
\[
	\tr A
\]
(engl.: trace=Spur) angegeben. 
Unter anderem ergibt sich aus der Unabhängigkeit von $\phi$ der Spur, dass ähnliche Matrizen, dieselbe Spur besitzen:  
\[
	\tr (S^{-1}AS)= \tr (A)
\]
Konkret lassen sich die Nullstellen des charakteristischen Polynoms häufig nicht berechnen.  
Dies hängt nicht damit zusammen, dass Mathematiker zu faul waren, herauszufinden wie man dies ausrechnet. 
Es wurde bewiesen, dass es für Polynome von Grad $\ge 5$ keine allgemeine algebraische Lösungsformel gibt.
\end{note}


\begin{thm}[Cayley und Hamilton]
\label{thm:10.17}
Sei $\dim V=n<\infty$, $\phi:V\to V$ linear, mit Matrix $A$ und $\chi_\phi(x)=\sum_{k=0}^na_kx^k$.
Dann gilt
\begin{align*}
a_nA^n+a_{n-1}A^{n-1}+\dotsb+a_1A+a_0I&=0\\
a_n\phi^n+a_{n-1}\phi^{n-1}+\dotsb+a_1\phi+a_0\Id&=0
\end{align*}

\begin{proof}
$M:=A-xI$, $\tilde M = (\adj M)^T$ die komplementäre Matrix
\[
M\tilde M = \det M \cdot I
\]
Einträge von $\tilde M$ sind $(n-1)\times(n-1)$ Unterdeterminanten und damit Polynome vom Grad $\le n-1$.
Also:
\begin{align*}
\tilde M = \sum_{k=0}^{n-1}C_{k}x^{k} \qquad C_i\in \Mat(n\times n, K)
\end{align*}
\begin{align*}
(A-xI)\left(\sum_{k=0}^{n-1}C_k x^k\right) &= \chi_A(x)\cdot I\\
	\intertext{Durch Koeffizientenvergleich ergeben sich die Zusammenhänge:}
								A C_0 &= a_0 I\\
(AC_1-C_0)x &= a_1xI\\
(AC_2-C_1)x^2 &= a_2x^2I\\
\vdots &= \vdots\\
(AC_{n-1}-C_{n-2})x^{n-1} &= a_{n-1}x^{n-1}I\\
-C_{n-1}x^n &= a_nx^nI
\end{align*}
Man dividiert durch die $x_i$ und multipliziert $A^i$.
Die linke Seite addiert sich als Teleskopsumme zur Nullmatrix und es ergibt sich:
\[
0 = a_0I + a_1A+\dotsb a_nA^n
\]
\end{proof}
\end{thm}


\chapter{Diagonalisierbarkeit und Trigonalisierbarkeit}



$\phi: V\to V$ linear, $\dim V < \infty$.
Wähle Basis $v_1,\dotsc, v_n$ von $V$, sodass die Matrix $A$ zu $\phi$ möglichst einfach wird.
Oder: Für gegebenes $B\in \Mat(n\times n, K)$, suche $A=SBS^{-1}$ mit $S\in GL(n,K)$ und $A$ möglichst einfach.

\section{Diagonalisierbarkeit}

Falls $\phi$ Eigenwerte $\lambda_1,\dotsc, \lambda_n$ (paarweise verschieden) hat und $n=\dim V$,
wähle Eigenvektoren $v_i$ zu $\lambda_i$.
Dann bilden $v_1,\dotsc, v_n$ eine Basis von $V$.
Bezüglich dieser Basis hat $A$ Diagonalform:
\[
A=\begin{pmatrix}\lambda_1&0&\cdots&0\\ 0&\lambda_2&\cdots&0\\ \vdots & \vdots & \ddots & \vdots \\0&0&\cdots& \lambda_n\end{pmatrix}
\]
Dieses Kriterium kann aber nicht die notwendige Bedingung sein, denn $\phi=\Id$ hat nur einen Eigenwert $\lambda=1$ und ist trotzdem diagonalisierbar mit $A=I$.

Sei $\phi$ diagonalisierbar mit $A$ Diagonalmatrix.
$\lambda_1,\dotsc,\lambda_n$ Eigenwerte (nicht notwendigerweise verschieden) und es existiere eine Basis von $V$ aus Eigenvektoren $v_1,\dotsc,v_n$.

\begin{align*}
\chi_\phi=\chi_A&=\det(A-\lambda I)\\
&=\prod_{k=1}^n(\lambda_k-x)
\end{align*}

\begin{ex}
\begin{align*}
A&= \begin{pmatrix}1&1\\0&1\end{pmatrix}\\
\chi_A&=\det(A-xI)=(1-x)^2
\end{align*}
Obwohl A in Linearfaktoren zerfällt, ist $A$ nicht diagonalisierbar! Damit kann auch dies keine hinreichende Bedingung sein.
\end{ex}


\begin{df}
\label{df:11.1}
Sei $\dim V<\infty$, $\phi:V\to V$ linear und $\lambda$ Eigenwert von $\phi$.
Wenn $\chi_\phi$ geschrieben wird als
\[
\chi_\phi=(\lambda - x)^m f(x)
\]
mit $f(\lambda)\neq 0$.
Dann heißt $m$ \emph{Vielfachheit} der Nullstelle $\lambda$ von $\chi_\phi$, oder \emph{Vielfachheit des Eigenwertes $\lambda$}, oder \emph{algebraische Vielfachheit} des Eigenwerts $\lambda$, oder \emph{Multiplizität}.
Man schreibt
\[
m=\my(\chi_\phi,\lambda)
\]

Der Eigenraum $\{v,\in V:\phi(v)=\lambda v\}$ zum Eigenwert $\lambda$ wird mit
$\Eig(\phi,\lambda)$ bezeichnet.
Seine Dimension heißt \emph{geometrische Vielfachheit} des Eigenwertes $\lambda$.
\end{df}

\begin{ex}
\[
A=\begin{pmatrix}1&1\\0&1\end{pmatrix}
\]
$\lambda_1$ hat algebraische Vielfachheit $2$ (wegen $\chi_\phi=(1-x)^2$) und geometrische Vielfachheit $1$ ($\dim \Eig(\phi,\lambda)=1$).
\end{ex}

\begin{lem}
\label{11.2}
Sei $\dim V<\infty$, $\phi: V\to V$ linear, $\lambda\in K$.
Dann gilt:
\[
\dim \Eig(\phi,\lambda) \le \my(\chi_\phi,\lambda)
\]
Die geometrische Vielfachheit von $\lambda$ ist also höchstens so groß wie die algebraische Vielfachheit.
\begin{proof}
Sei $\lambda$ kein Eigenwert, dann ist $\my(\chi_\phi, \lambda)=0=\dim \Eig(\phi,\lambda)$.

Sei $\lambda$ also Eigenwert, dann ist $1\le \dim \Eig(\phi,\lambda) =: r$.
Wähle Basis $v_1,\dotsc, v_r$ von $\Eig(\phi,\lambda)$.
Ergänze diese zu einer Basis von $V$: $v_1\dotsc, v_r,\dotsc, v_n$.
Für $i\le r$ ist $v_i$ Eigenvektor von $\phi$.
$A$ hat also die Form:
% (FIXME):  Provisorischer FIX!  Abzuklären mit Stephan
\[
A=\begin{pmatrix}
\diag\{\lambda, ..., \lambda\} & \fbox{$*$} \\
0 & B
\end{pmatrix}
\]
Wir nennen den rechten unteren Block $B\in \Mat(n-r\times n-r, K)$.
\[
\chi_\phi = \det(A-xI) = (\lambda-x)^r\cdot \det(B-xI)
\]
Also muss
\[
\my(\chi_\phi,\lambda) = \underbrace{\my((\lambda-x)^r),\lambda)}_{=r} + \my(\det(B-xI)) \ge r
\]
\end{proof}
\end{lem}


\begin{ex}
\[
	A=\diag\{\lambda,\dotsc,\lambda,\my_1,\dotsc, \my_s\}
\]
Dann ist
\[
\chi_A = \det(A-xI) = (\lambda -x)^r(\my_1-x)\dotsb(\my_s-x)
\]
Die algebraische Vielfachheit ist $\my(\chi_A, \lambda)=r$.
$\Eig(A, \lambda)$ hat Basis $v_1, \dotsc, v_n$.
Also:
\[
\dim \Eig(A,\lambda) = r = \my(\chi_A,\lambda)
\]
\end{ex}

\begin{thm}
\label{11.3}
Sei $\dim V <\infty$ und $\phi:V \to V$ linear.
Dann sind die folgenden Bedingungen äquivalent:
\begin{enumerate}
\item $\phi$ ist diagonalisierbar
\item Das charakteristische Polynom $\chi_\phi$ ist ein Produkt aus Linearfaktoren und für jeden Eigenwert $\lambda$ gilt:
\[
\dim \Eig (\phi, \lambda) = \my(\chi_\phi,\lambda)
\]
\item $\{\lambda_1,\dotsc, \lambda_l\}$ sei die Menge aller Eigenwerte von $\phi$ ist und es gilt:
\[
V=\bigoplus_{i=1}^l \Eig(\phi, \lambda_i)
\]
\end{enumerate}

\begin{proof}
	\begin{seg}{(1)$\implies$ (3)}
Seien $\lambda_1,\dotsc, \lambda_n$ die Eigenwerte von $\phi$ (nicht notwendigerweise $\neq 0$).
Da die Eigenvektoren eine Basis von $V$ bilden ist
\[
	V = \sum_{i=1}^n \Eig(\phi,\lambda_i)
\]
Nach \ref{prop:10.3} sind Eigenvektoren zu verschiedenen Eigenwerten linear unabhängig.
Wählen wir also nur die Eigenräume zu verschiedenen Eigenwerten $\lambda_1,\dotsc,\lambda_l$, ergibt sich eine direkte Summe:
\[
V = \bigoplus_{i=1}^l \Eig(\phi,\lambda_i)
\]
\end{seg}
\begin{seg}{(3)$\implies$ (2)}
Wegen der direkten Summe gilt für die Dimensionen
\[
	\dim V = \sum_{i=1}^l\limits \underbrace{\dim ( \Eig(\phi, \lambda_l) )}_\text{geometrische Vielfachheiten} \le \sum_{i=1}^l \underbrace{\my(\chi_\phi, \lambda_l)}_{\text{algebraische Vielfachheiten}} = \deg \chi_\phi
\]
Aber $\dim V = \deg \chi_\phi$. also muss
\[
\dim  \Eig(\phi, \lambda_i) =\my(\chi_\phi, \lambda_i) \qquad \forall i=1,\dotsc,l
\]
gelten.
\end{seg}
\begin{seg}{(2) $\implies$ (3)}
Seien $\lambda_1,\dotsc,\lambda_k$ paarweise verschieden Eigenwerte von $\phi$, dann gilt zunächst
\[
	\dim V = \deg \chi_\phi = \sum_{k=1}^l \my(\chi_\phi,\lambda_k) = \sum_{k=1}^l \dim ( \Eig(\phi,\lambda_k) )
\]
also ist
\[
\dim V =\sum_{k=1}^l \dim (\Eig(\phi,\lambda_i)) = \dim\left (\bigoplus_{k=1}^l \Eig(\phi, \lambda_k)\right )
\]
und damit, weil die Eigenräume Unterräume von $V$ sind:
\[
V=\bigoplus_{k=1}^l \Eig(\phi,\lambda_k)
\]
\end{seg}
\begin{seg}{(3) $\implies$ (1)}
Wähle für die Eigenräume $\Eig(\phi,\lambda_i)$ jeweils Basen.

Da $V = \bigoplus_{i=1}^l \Eig(\phi,\lambda_i)$, ergeben die Basen zusammengesetzt eine Basis von $V$ aus Eigenvektoren.
Bezüglich dieser Basis besitzt $A$ Diagonalform.
\end{seg}
\end{proof}
\end{thm}


\section{Trigonalisierbarkeit}


\begin{ex}
Sei $A$ obere Dreiecksmatrix.
Dann ist
\[
\chi_A = \det(A-xI) = \prod_{k=1}^n(a_{kk}-x)
\]
ein Produkt von Linearfaktoren.
Die Eigenwerte sind also genau die Diagonaleinträge $a_{ii}$.
\end{ex}

Wir wollen also Matrizen betrachten, die man auf obere Dreiecksform bringen kann.

\begin{df}
\label{df:11.4}
Eine Matrix $A$ mit zugehöriger linearen Abbildung $\phi$ heißt \emph{triagonalisierbar} genau dann, wenn
\[
\exists S\in \GL(n, K): SAS^{-1} \text{ obere Dreiecksmatrix}
\]
\end{df}

Für die Basis $v_1,\dotsc,v_n$ bedeutet das, dass
\begin{align*}
\phi(v_1)&\in[v_1]=:V_1\\
\phi(v_2)&\in[v_1,v_2] =:V_2\\
\vdots &\in \vdots\\
\phi(v_n)&\in[v_1,\dotsc,v_n]=:V_n
\end{align*}
was man sich leicht an der Dreiecksform veranschaulichen kann (Spalten sind die Bilder der Basisvektoren).

Damit ergibt sich eine äquivalente Bedingung zur Trigonalisierbarkeit: 
Es existiert eine Kette von Unterräumen
$ V_1 \subset V_2 \subset ... \subset V_n=V $
mit $\dim V_i=i$ und $\phi(V_i)\subset V_i$.

\begin{thm}
Sei $\dim V <\infty$, $\phi: V\to V$ linear.
$\phi$ ist genau dann trigonalisierbar, wenn das charakteristische Polynom $\chi_\phi$ ein Produkt von Linearfaktoren ist.
\begin{note}
Für $K=\C$ besagt \emph{der Fundamentalsatz der Algebra} also, dass über $\C$ jede Matrix trigonalisierbar ist.
\end{note}

\begin{proof}
	\begin{seg}{$\Longrightarrow$}
Sei $\phi$ trigonalisierbar und $A$ die zugehörige Matrix in Dreiecksform.
Dann lässt sich das charakteristische Polynom als Produkt von Linearfaktoren schreiben
(Die Determinante einer Dreiecksmatrix ergibt sich als Produkt der Diagonaleinträge).
\end{seg}
\begin{seg}{$\Longleftarrow$}
Sei jetzt umgekehrt $\chi_A$ Produkt von Linearfaktoren.

Also existieren Eigenwerte $\lambda_1, \dotsc, \lambda_n$ (nicht notwendig verschieden) von $A$.
Es existiert also mindestens Eigenwert $\lambda_1$ mit Eigenvektor $v_1$.

Führe Induktion nach $n=\dim V$ durch.

Ergänze $v_1$ zu einer Basis $v_1,w_2,\dotsc,w_n$ von $V$.
Bezüglich der neuen Basis hat $\phi$ die Matrix:
\[
B=\begin{pmatrix}\lambda_1 & * & \cdots & * \\
0 & * & \cdots & * \\
\vdots & \vdots & \ddots & \vdots \\
0 & * & \cdots & *
\end{pmatrix}
\]
Entfernt man die erste Zeile und erste Spalte von $B$, ergibt sich eine $n-1\times n-1$-Matrix $C$.
Da
\[
V=\<v_1\>\oplus \underbrace{\<w_2,\dotsc,w_n\>}_{W:=}
\]
Für $w\in W$ gilt $\phi(w)=\my_1v_1+\sum_{k=2}^n\my_kw_k$.
Betrachte also
\[
\psi := \pi_W \circ \phi:\qquad w\mapsto \phi(w)\mapsto \sum_{k=2}^n\my_kw_k
\]
Zu $\psi$ gehört genau die Matrix $C$.
Und da
\[
\chi_A= \chi_B = (\lambda_1-x)\chi_C \implies \chi_C = \prod_{k=2}^n(\lambda_k-x)
\]
ist C per Induktion trigonalisierbar.

Also ist auch A trigonalisierbar.
\end{seg}
\end{proof}
\end{thm}

\begin{note}
Wie wir später feststellen, ist die Trigonalisierbarkeit zur Existenz der Jordannormalform äquivalent, so lässt sich in jedem Fall eine "`schöne"' Dreiecksform durch die Konstruktion einer Basis, die zur Jordannormalform gehört, erhalten.
Das ist oftmals leichter, als nach diesem Verfahren vorzugehen.

Sonst kann man am Beweis die Konstruktion der Dreiecksmatrix erkennen. Nachdem man das charakteristische Polynom bestimmt hat, konstruiert man sich in jedem Schritt $\phi$-invariante Räume durch die Betrachtung der jeweiligen Eigenvektoren.  Die Basiswechselmatrix sind dabei in einer Form, die eine schnelle Matrixmultiplikation erleichtert.
\end{note}



\chapter{Rationale Normalform und Jordansche Normalform}

Die Rationale Normalform funktioniert für alle Matrizen und für jeden zugrundeliegenden Körper.
Die Jordansche Normalform ist dagegen eine Erweiterung der rationalen Normalform und liefert unter der Vorraussetzung, dass über dem zugrundeliegenden Körper alle Polynome in Linearfaktoren zerfallen eine bessere, schönere Normalform.

\begin{df}
\label{df:12.1}
Seien $A,B\in \Mat(n\times n,K)$.
$A$ und $B$ heißen \emph{ähnlich} genau dann, wenn
\[
\exists S\in \GL(n,K):\quad B=SAS^{-1}
\]
\begin{note}
	Die Ähnlichkeit von Matrizen ist eine Äquivalenzrelation
	\begin{proof}
		Reflexivität ist gegeben für $S:=I$ und für die Symmetrie wählt man für $S$ die Invesere.
		Transitivität:
		\[
			C = PBP^{-1}, \quad B= QAQ^{-1} \implies C = PQAQ^{-1}P^{-1}
		\]
		mit der Wahl $S:= PQ$.
	\end{proof}
\end{note}
\end{df}

\begin{ex}
Sei $\phi:V\to V$, $\dim V<\infty$.
Sei $v_0\neq 0$.
Setze $v_{i+1}=\phi(v_i)$.

Die $v_i$ können nicht alle linear unabhängig sein.
Es existiert ein minimales $n$, sodass $v_0,\dotsc,v_n$ linear unabhängig, aber $v_0,\dotsc,v_{n+1}$ linear abhängig ist.
Wir können also schreiben:
\[
v_{n+1} = \sum_{i=0}^n a_iv_i
\]

Sei $v_0,\dotsc,v_n$ bereits Basis von $V$ (\fixme[Verwirrend, denn $\dim V=n+1$]), dann hat die Matrix von $\phi$ bezüglich dieser Basis \emph{Begleitmatrixform}:
\[
A= \begin{pmatrix}
0 & 0 & \cdots & 0 & a_0\\
1 & 0 & \ddots & 0 & a_1\\
0 & 1 & \ddots & 0 & a_2\\
\vdots & \ddots & \ddots & 0 & \vdots\\
0 & \cdots & 0 & 1 & a_n
\end{pmatrix}
\]

Zerlege jetzt die allgemeine Situation in (endlich) viele solcher Situationen, sodass man eine Zusammenstellung als Blockmatrix von Matrizen obiger Form erhält. Also in der Form:
\[
\begin{pmatrix}
  \fbox{$\begin{smallmatrix} 0 & \cdots  & 0 & a_0 \\ 1 &  & 0 & a_1 \\ & \ddots &  & \vdots \\ 0 &  & 1 & a_n  \end {smallmatrix}$}  &  & 0 \\
 & \ddots &  \\
0 &  & \fbox{$\begin{smallmatrix} 0 & \cdots  & 0 & a_0 \\ 1 &  & 0 & a_1 \\ & \ddots &  & \vdots \\ 0 &  & 1 & a_n  \end {smallmatrix}$}
\end{pmatrix}
\]
Zeige außerdem: $a_0,\dotsc,a_n$ hängen nicht von der Wahl von $v_0$ ab (um die Eindeutigkeit zu gewährleisten).


\begin{align*}
\chi_A &= \det \begin{pmatrix} -x &   & 0 & a_0 \\ 1 & \ddots  &  & a_1 \\ & \ddots & -x  & \vdots \\ 0 &  & 1 & a_n-x  \end {pmatrix} \\  &=  \sum_{k=0}^{n} (-1)^{n+k} (-1)^ka_kx^k - (-1)^nx^{n+1}= (-1)^n \left (\sum_{k=0}^{n} a_kx^k - x^{n+1} \right )
\end{align*}
Die $a_i$ sind bestimmt als Koeffizienten von $\chi_A$, hängen also nicht von der Wahl von $v_0$ ab.
(Größe von $A$ ist $n+1$).
Methode:  betrachte charakteristisches Polynom.
\end{ex}



\section{Teilbarkeitstheorie für Polynome, analog zu $\N$}

\begin{df}
\label{df:12.2}
Seien $f,g,d \in K[x]$.
$d$ heißt \emph{gemeinsamer Teiler} von $f$ und $g$ genau dann, wenn
\[
d|f \land d|g
\]
Falls zusätzlich $\deg d > 0$, heißt $d$ \emph{echter gemeinsamer Teiler}.

$f$ und $g$ heißen \emph{teilerfremd} oder \emph{relativ prim} genau dann, wenn
\[
f, g \text{ besitzen keinen echten gemeinsamen Teiler}
\]

$d$ heißt \emph{größter gemeinsamer Teiler} von $f$ und $g$ \emph{genau dann}, wenn
\[
d|f \land d|g \land (\forall h\in K[x]:h|f\land h|g \implies h|d)
\]
Notation: $d= \ggT(f,g)$.
\end{df}

Zeige: Existenz und Eindeutigkeit (bis auf skalare Vielfache) des $\ggT(f,g)$.

\begin{prop}
\label{prop:12.3}
Seien $f,g\in K[x]\setminus\{0\}$.
Dann existiert $d=\ggT(f,g)$.
Wenn auch $d'=\ggT(f,g)$, dann gilt
\[
\exists \lambda \in K\setminus\{0\}:d'=\lambda d
\]
Der $\ggT(f,g)$ ist also bis auf skalare Vielfache eindeutig bestimmt.

\begin{proof}[abstrakt]
Betrachte
\[
I=\{q_1f+q_2g:q_1,q_2\in K[x]\}
\]
Da $f,g\in I$ ist $I$ nicht leer.
Wähle $d\in I$ mit $d\neq 0$ und $\deg d$ minimal in $\{\deg h:h\in I\}$.

Zeige: $I\ni d=\ggT(f,g) \implies \exists h_1,h_2 : d=h_1f+h_2g$

z.z.: $d=\ggT(f,g)$.
Teile daher „f durch d“:
\begin{align*}
f&=dq + r \qquad \deg r < \deg d\\
&=(h_1f + h_2g)q + r\\
\implies r&= f-(h_1f+h_2g)q\\
&= f(1-h_1q) - g(h_2q) \in I
\end{align*}
Also $r\in I$ und $\deg r < \deg d$, aber $\deg d$ war minimal, also
\[
\deg r = -\infty \implies r=0
\]
Analog ergibt sich auch $d|g$ und es verbleibt zu zeigen, dass d gerade der größte gemeinsame Teiler ist.

% Ist in Ordnung denk ich, aber warum zweimal fast den gleichen Beweis?
\end{proof}

\begin{proof}[formal]
	\begin{seg}{Existenz}
$f,g\in K[x] \setminus \{0\}$. Zeige $\exists \ggT(f,g)$.
\[
I := \{q_1f+q_2g:q_1,q_2\in K[x]\}
\]
In $I\setminus \{0\}$ wähle $d$ von minimalem Grad.
\[
d=h_1f +h_2g
\]
Zeige $d=\ggT(f,g)$.
Zeige zunächst $d|f$ und $d|g$.
Division mit Rest:
\[
f=qd+r
\]
mit $\deg r < \deg d$, zeige $r=0$.
\begin{align*}
d&=h_1f+h_2g \\
\implies r&=f-qd \\
			 &= f-q(h_1f+h_2g)\\
	   &= \underbrace{(1-qh_1)}_{=q_1}f + (\underbrace{-qh_2}_{=q_2}) g \qquad \in I
\end{align*}
Da $\deg r < \deg d$ und $\deg d$ minimal in $I\setminus \{0\}$ gewählt war, muss $r=0$.

Damit ist $f=qd$, d.h. $d|f$.
Analog zeigt man $d|g$.

Sei $e\in K[x]$ mit $e|f$ und $e|g$.
Wegen
\[
d=h_1f+h_2g
\]
gilt also nach \ref{lem:10.13} auch $e|d$.
Also ist $d=\ggT(f,g)$.
\end{seg}
\begin{note}
Dieser Existenzbeweis war nicht konstruktiv!
\end{note}

\begin{seg}{Eindeutigkeit:}
Seien $d$ und $d'$ zwei $\ggT(f,g)$.
$d|f$ und $d|g$, $d'=\ggT(f,g) \implies d|d'$, ebenso $d'|d$.
Also $\exists j,h: d'=hd$ und $d=jd' \implies d=(jh)d$.

Also ist
\[
\deg jh = 0 = \deg j = \deg h \implies j,h\in K\setminus \{0\}=K^*
\]
\end{seg}

\end{proof}


\end{prop}

\begin{alg*}[Euklidischer Algorithmus zur Bestimmung des $\ggT$] 
Seien $f,g\in K[x]$ gegeben.
Wir suchen:
\[
d=\ggT(f,g)
\]
sowie $h_1,h_2\in K[x]$ sodass
\[
	d=h_1f+h_2g
\]
Sei o.B.d.A $\deg f \ge \deg g$.

\begin{algorithmic}
	\State $r_0 \gets f$
	\State $r_1 \gets g$

	\For {$i=1,2,\dotsc$}
		\State Bestimme $q_{i+1}$ und $r_{i+1}$ durch Division mit Rest: 
			\[
				r_{i-1} = q_{i+1}r_i + r_{i+1} \qquad \deg r_{i+1} < \deg r_{i}
			\]
		\If{$r_{i+1} = 0$}
			\State $\ggT(f,g) \gets r_i$
			\State Algorithmus abgeschlossen
		\EndIf
	\EndFor
\end{algorithmic}

Wegen $\deg r_{i+1} < \deg r_{i}$ ist irgendwann $r_{n+1} = 0$ und der Algorithmus bricht ab.
Dann ist
\[
	r_{n} = \ggT(r_{n-1},r_{n}) = \ggT(r_{n-2},r_{n-1}) = \dotsb = \ggT(f,g)
\]
also funktioniert der Algorithmus.

Zum finden der $h_1,h_2$ mit $r_n = h_1f + h_2g$ geht man folgendermaßen vor.
Es ist aus dem Algorithmus bekannt
\[
	r_{i+1} = r_{i-1} - q_{i+1}r_i 
\]
Führe also sukzessive zurück auf
\begin{align*}
	\ggT(f,g) = r_n &= r_{n-2} - q_nr_{n-1} \\
					&= r_{n-2} - q_n(r_{n-3} -q_{n-1}r_{n-2})\\
		&= \dotsc\\
	 &= (\dotso)r_0 + (\dotso)r_1 = h_1f + h_2g
\end{align*}

Es gilt 
\[
	\deg h_1,\deg h_2 \le \deg g \le \deg f
\]

\end{alg*}


\begin{ex}
Für $f=17, g=5$, sei $z\in \Z:\deg z=|z|$.
\begin{align*}
f= q_1g+r_1 &: \qquad 17 = 3\cdot 5 + 2\\
g=q_2r_1 +r_1&: \qquad 5 = 2\cdot 2 + 1\\
r_1=q_3r_2+r_3&: \qquad  2 = 2\cdot 1 + 0
\end{align*}
Also $\ggT(17,5)=1$
Gesucht $h_1,h_2\in \Z$
\begin{align*}
1 &=h_1f+h_2g\\
1 &=5-2\cdot 2=5-2(17-3\cdot 5)=- 2 \cdot 17+7 \cdot 5
\end{align*}


\end{ex}

\begin{ex}
$f=5725, g=135$
\begin{align*}
5725 &= 42\cdot 135 + 55\\
135 &=  2\cdot 55 + 25\\
55 &= 2\cdot 25 + 5\\
25 &= 5\cdot 5 + 0
\end{align*}
Also ist $\ggT(5725,135)=5$.
\begin{align*}
5&=1\cdot 55 - 2\cdot 25\\
&=1\cdot 55 -2(1\cdot 135 - 2\cdot 55)\\
&=-2\cdot 135 + 5\cdot 55\\
&= -2\cdot 135 + 5(1\cdot 5725 - 42\cdot 135)\\
&= 5\cdot 5725 - 212\cdot 135
\end{align*}
\end{ex}

\begin{ex}
$K[x]: f=x^6, g=x^2-1$
\begin{align*}
x^6 &=(x^4+x^2+1)(x^2-1) + 1\\
x^2-1 &= (x^2-1)\cdot 1 + 0\\
\end{align*}
Also ist $\ggT(f,g)=1$.
\begin{align*}
1 = 1\cdot x^6 - (x^4+x^2+1)(x^2-1)
\end{align*}
\end{ex}

\begin{ex}
$f(x)) = x^{12}-1$ und $g(x)=x^8-1$
\begin{align*}
x^{12}-1 &= x^4\qquad (x^8-1)+(x^4-1)\\
x^8 -1 &= (x^4-1)(x^4+1) +0
\end{align*}
$\implies$
\begin{align*}
x^4-1=(x^{12}-1)\cdot 1 -x^4(x^8-1)
\end{align*}
\end{ex}

Weiter Analogie zwischen $\Z$ und $K[x]$:
Irreduzible Polynome verhalten sich wie Primzahlen.

\begin{lem}
\label{lem:12.4}
Seien $f,g,h,p\in K[x]$.
\begin{enumerate}[(a)]
\item Sei $\ggT(f,g)=1$ und $f|gh \implies f|h$
\item Sei $p$ irreduzibel.
Dann ist $\ggT(p,f)\in \{1,p\}$.
\item Sei $p$ irreduzibel, $p|fg \implies p|f \lor p|g$.
\end{enumerate}

\begin{proof}
\begin{enumerate}[(a)]
\item Wähle $q$, so dass $gh = qf$.
	Wegen $\ggT(f,g)=1$
\[
\exists h_1,h_2 :\quad 1 = h_1f + h_2g \implies h=hh_1f +hh_2g = hh_1f + h_2qf
\]
Also gilt $f|h$.
\item 
	Es gilt $\ggT(p,f)|p$.
	Da $p$ jedoch irreduzibel (hat also keine echten Teiler $\neq 1, p$), kann nur
	\[
		\ggT(p,f)\in \{1,p\}
	\]
\item 
	Wähle $q$ so, dass $fg=qp$.
	Falls $p|f$, sind wir fertig.
Wenn nicht, dann ist $\ggT(p,f)=1$.
\begin{align*}
	\exists h_1,h_2:\quad 1 =h_1f+h_2p \implies g =h_1fg+h_2pg =h_1qp + h_2g p
\end{align*}
Also gilt $p|g$
\end{enumerate}
\end{proof}
\end{lem}

\begin{thm}[Eindeutige Primfaktorzerlegung]
\label{thm:12.5}
Sei $f\in K[x]$, $\deg f\ge 1$.
Dann existieren (nicht notwendigerweise verschiedene) irreduzible Polynome $p_1,\cdots,p_n$ mit
$f=p_1\cdots p_2$.
Die Faktoren $p_i$ sind eindeutig bis auf Reihenfolge und bis auf Multiplikation mit Skalaren in $K\setminus \{0\}$.

\begin{proof}
	\begin{seg}{Existenz:}
Ist $f$ irreduzibel, dann sind wir fertig.
Wenn nicht: $f=f_1\cdot f_2$ mit $\deg f_1,\deg f_2 \ge 1$.\\
Ist $f_1, f_2$ irreduzibel, dann sind wir fertig, wenn nicht:\\
\[
f=f_1\cdot f_2\cdots f_m \implies \deg f=\deg f_1 + \dotsb + \deg f_m \implies m\le \deg f
\]
Also bricht das Verfahren spätestens bei $m=\deg f$ ab.
Man erhält
\[
	f=p_1\dotsb p_l \qquad \forall i=1,\dotsc,l :\; p_i \text{ irreduzibel}
\]
\end{seg}

\begin{seg}{Eindeutigkeit:}
Sei $f=p_1\dotsb p_l=q_1\dotsb q_m$ mit $p_i$, $q_j$ irreduzibel.
\\Zeige $\exists j:p_1=q_j$ (bis auf Faktoren in $K \setminus \{0\}$).
\[
p_1|f = q_1\cdot (q_2\dotsb q_m) \implies p_1|q_1 \lor p_1|(q_2\dotsb q_m)
\]
Also $\exists j:p_1=q_j$.
Wir nehmen o.B.d.A an, dass $j=1$ ist.
\[
f=p_1(p_2\dotsb p_l) = \underbrace{q_1}_{=p_1}(q_2\dotsb q_m)
\]
Nach der Kürzungsregel \ref{lem:10.13} folgt:
\[
p_2\dotsb p_l = q_2 \dotsb q_m
\]
Durch Induktion folgt die Behauptung.
\end{seg}
\end{proof}
\end{thm}

Seien $f,g\in K[x]\setminus \{0\}$, $a_i,b_i\ge 0$ und $p_i$ irreduzibel und
\[
	f=p_1^{a_1} \dotsb p_e^{a_e} \qquad\qquad g=\alpha p_1^{b_1}\dotsb p_e^{b_e}
\]
mit $\alpha \in K\setminus \{0\}$.

Seien die $p_i,p_j$ teilerfremd für $i\neq j$.
Dann ist
\[
\ggT(f,g) = p_1^{m_1}\dotsb p_e^{m_e} \qquad {m_i=\min(a_i,b_i)}
\]
Sei
\[
M_i =\max(a_i,b_i)
\]
$p_1^{M_1}\dotsb p_e^{M_e}$ ist das kleinste gemeinsame Vielfache von $f,g$: $\kgV(f,g)$.


\section{Die rationale Normalform}


Ziel: Eine Normalform von Matrizen bezüglich der Ähnlichkeit von Matrizen:
\[
\begin{pmatrix}\fbox{$*$} & 0 & \cdots & 0 \\ 0 & \ddots & \ddots & \vdots \\ \vdots & \ddots & \ddots & 0 \\ 0 & \cdots & 0 & \fbox{$*$}  \end{pmatrix}
\]
Wir brauchen also eine Zerlegung von V für jedes Kästchen.  Eine spezielle Form der Basis ist gesucht.

\begin{df}
\label{12.6}
Sei $V$ ein Vektorraum, $\phi:V\to V$ ein Endomorphismus.
Ein Unterraum $U\le V$ heißt $\phi$-invariant \emph{genau dann}, wenn
\[
\phi(U) \le U
\]
Wenn $V$ sich schreiben lässt als direkte Summe $V=U_1\oplus U_2$ mit $U_1,U_2$ $\phi$-invariant und $U_1,U_2\neq \{0\}$,
dann sagt man: $V$ \emph{zerfällt bezüglich $\phi$} in die direkte Summe $V=U_1\oplus U_2$.
\end{df}

\begin{lem}
\label{lem:12.7}
Sei $\dim V<\infty$, $\phi:V\to V$ ein Endomorphismus.
Dann gibt es eine Zerlegung von $V$ bezüglich $\phi$:
\[
V=\bigoplus_{i=1}^l U_i\qquad \text{($\phi$-invariante $U_i$)}
\]
sodass die $U_i$ nicht bezüglich $\phi$ zerlegbar sind.
Bezüglich einer Basis von $V$, die die Vereinigung von Basen der $U_i$ ist,
hat $\phi$ eine Matrix in Blockdiagonalform.

\begin{proof}
Wenn $V$ unzerlegbar ist bezüglich $\phi$, dann schreiben wir $U_1=V$ und wir sind fertig.

Wenn $V$ zerlegbar bezüglich $\phi$, dann schreiben wir
\[
V=U_1\oplus U_2
\]
Falls $U_1,U_2$ nicht zerlegbar, dann sind wir fertig. Sonst zerlegen wir auch $U_1$ und $U_2$.

Da $\dim V<\infty$, bricht das Verfahren ab.

Wir wählen also die Basis bezüglich $U_1, ..., U_l$ und vereinigen diese zur Basis von $V$. Bezüglich dieser Basen hat $A$ Blockdiagonalform:
\[
\begin{tabular}{*{3}{>{$}c<{$}}}
  Basis & \begin{matrix} U_1 & U_2 & \cdots & U_l \end{matrix} \\
  \begin{matrix} U_1 \\ U_2 \\ \vdots \\ U_l \end{matrix} &  \begin{pmatrix}\fbox{$*$} & 0 & \cdots & 0 \\ 0 & \fbox{$*$} & \ddots & \vdots \\ \vdots & \ddots & \ddots & 0 \\ 0 & \cdots & 0 & \fbox{$*$}  \end{pmatrix}
\end{tabular}
\]
\end{proof}
\end{lem}


\begin{lem}
	\label{lem:12.8}
	Sei $\phi:V\to V$ linear.
	Dann ist die Abbildung
	\begin{align*}
		\alpha:K[x] &\to \Hom(V,V)\\
			  f(x) &\mapsto f(\phi)
	\end{align*}
	die $\phi$ in das Polynom einsetzt ein Ringhomomorphismus.
	
	\begin{proof}
		Die 1 bleibt erhalten, denn $ 1 \mapsto \Id$.  \\
		Nun bleibt zu zeigen, dass Multiplikation und Addition homogen sind. Sei also analog: $g(x)=b_0+b_1x+...+b_lx^l$
		\begin{align*}
		\alpha(f+g)&=\alpha((a_0+b_0)+(a_1+b_1)x+...)=\alpha(a_0+a_1x+...+b_0+b_1x+...)\\ &=\alpha(f)+\alpha(g) \\
		\alpha(f\cdot g)&=\alpha((a_0+a_1x+...)(b_0+b_1x+...))=\alpha(f)\alpha(g)
		\end{align*}
		
		
	\end{proof}
\end{lem}


\begin{lem}
\label{lem:12.9}
Sei $\phi: V\to V$ linear und $f=f_1f_2\in K[x]$ mit $\ggT(f_1,f_2)=1$.
Sei $f(\phi)$ die Nullabbildung.
Sei $W_1:=\ker(f_1(\phi))$ und $W_2:=\ker(f_2(\phi))$.
Dann ist
\[
V=W_1\oplus W_2
\]
eine Zerlegung in $\phi$-invariante Teilräume.

\begin{proof}
	\begin{seg}{$V=W_1+W_2$}
Es ist $f=f_1f_2$, $\ggT(f_1,f_2)= 1$.
Also existieren $h_1,h_2\in K[x]$, sodass
\[
1=h_1f_1+h_2f_2
\]
Wir wenden \ref{lem:12.8} an und setzen $\phi$ in das Polynom ein.
\[
	\Id=\big(h_1f_1+h_2f_2\big)(\phi)
\]
Sei $u\in V$ beliebig, dann ist
\[
	u= \Id(u) = \Big(h_1(\phi)f_1(\phi) + h_2(\phi)f_2(\phi)\Big)(u) = \underbrace{\big(h_1(\phi)f_1(\phi)\big)(u)}_{u_1} + \underbrace{\big(h_2(\phi)f_2(\phi)\big)(u)}_{u_2} 
\]
$W_1=\ker(f_1(\phi))$. Behauptung: $u_2\in W_1$ denn
\begin{align*}
(f_1(\phi))(u_2) &= \big(f_1(\phi)h_2(\phi)f_2(\phi)\big)(u) \\
&= \big(h_2(\phi)f_1(\phi)f_2(\phi)\big)(u)\\
&= h_2(\phi)\big(\underbrace{f(\phi)(u)}_{=0}\big)\\
&= h_2(\phi)(0) = 0
\end{align*}
Also ist
\[
u_2\in \ker(f_1(\phi)) = W_1
\]
Analog zeigt man $u_1\in W_2$.
Damit ist $V=W_1+W_2$
\end{seg}
\begin{seg}{$W_1\cap W_2 =\{0\}$}
Sei $u\in W_1\cap W_2$, schreibe $u=u_1+u_2$ wie oben.
Dann ist $u\in \ker(f_1(\phi))$ und $u\in \ker(f_2(\phi))$.
Also
\[
	u= \Id(u) = \Big(h_1(\phi)f_1(\phi) + h_2(\phi)f_2(\phi)\Big)(u) = \underbrace{\big(h_1(\phi)f_1(\phi)\big)(u)}_{=0} + \underbrace{\big(h_2(\phi)f_2(\phi)\big)(u)}_{=0}  =0
\]
Damit ist $W_1\cap W_2 = \{0\}$.
\end{seg}
Also haben wir gezeigt, dass $V=W_1\oplus W_2$ gilt.
Es bleibt zu zeigen, dass $W_1,W_2$ auch $\phi$-invariant sind.

\begin{seg}{$W_1$ und $W_2$ sind $\phi$-invariant}
	Für $u\in V$ ist entweder $u\in W_1$ oder $u\in W_2$.

Für $u\in W_1=\ker(f_1(\phi))$ gilt
\[
f_1(\phi)(\phi(u)) = \phi(f_1(\phi)(u)) = 0
\]
Für $u\in W_2$ verläuft der Beweis analog.
\end{seg}
\end{proof}

\end{lem}

Das Lemma ist anwendbar.
Sei $f=\chi_\phi$ das charakteristische Polynom,
dann gilt $f(\phi)=0$ nach dem Satz von Cayley-Hamilton \ref{thm:10.17}

Eine Zerlegung von $\chi_\phi$ liefert also eine Kästchenform.
$W_1,W_2$ können durch den Gauß-Algorithmus berechnet werden (der Gauß-Algorithmus berechnet schließlich Kerne).

Frage: Ist $\chi_\phi$ das „beste“ $f$ zur Anwendung von $\ref{lem:12.9}$?

Gibt es ein eindeutiges (bis auf skalare Vielfache), bestes Polynom mit $f(\phi)=0$?
\begin{ex}
$\phi = \Id$, $A=I$, $\chi_A = (1-x)^n$.
Dann ist $f=x-1$ besser als $\chi_A$, denn es gilt immernoch
\[
	f(A)=A-I=0
\]
\end{ex}

Wir suchen das beste Polynom in zwei Schritten:
zuerst für einzelne Vektoren, dann für ganz $V$ ($\dim V<\infty$).

\begin{lem}[Minimalpolynom von Vektoren]
\label{lem:12.10}
Sei $\phi:V\to V$ mit Matrix $A$, $0\neq u\in V$.
Dann existiert ein Polynom $f(x)\in K[x]$, sodass gilt:
\begin{enumerate}[(a)]
	\item $\deg f\ge 1$ und $f$ ist normiert
	\item $f(A)(u)=0$
\item Für $g\in K[x]\setminus \{0\}$: $g(A)(u)=0\implies f|g$ (also $f$ ist von minimalem Grad mit den genannten Eigenschaften)
\end{enumerate}

\begin{proof}
	\begin{enumerate}[(a)]
\item
	Nicht weiter schwierig, Multiplikation mit einem skalaren Vielfachen ändert an den Eigenschaften nichts.
\item Wir betrachten $u, Au, A^2u,\dotsc,A^mu$.
Sei $m$ minimal, so dass $u,Au,\dotsc,A^{m-1}u$ linear unabhängig sind, aber $u,Au,\dotsc, A^{m-1}u,A^mu$ linear abhängig
($m$ existiert, weil $\dim V <\infty$).
Also $A^mu\in \langle u,Au,\dotsc,A^{m-1}u\rangle$.

Es existiert also eine Linearkombination
\begin{align*}
A^m u &= -a_0u-a_1Au-\dotsc -a_{m-1}A^{m-1}u\\
\implies 0&= A^mu +a_{m-1}A^{m-1}u + \dotsb + a_1Au + a_0I u
\end{align*}
Sei $f(x)=x^m+a_{m-1}x^{m-1}+\dotsb +a_1x+a_0 \in K[x]$.
Also
\[
f(A)(u)=0
\]
und $f(x)$ erfüllt (a).
\item
Sei $g(x)\in K[x]\setminus\{0\}$ mit $g(A)(u) = 0$.
Zeige $f|g$.
Führe Division mit Rest aus:
\[
g(x)=q(x)f(x)+r(x)
\]
Zeige: $r(x)=0$.
Angenommen $r(x) \neq 0$, dann ist wegen $\deg r<\deg f$
\[
r(x) = b_0 +b_1x+\dotsb +b_lx^l \qquad\qquad b_l\neq 0,\quad 1\le l=\deg r <\deg f
\]
Also
\[
\underbrace{g(A)(u)}_{=0} = \underbrace{q(A)f(A)(u)}_{=0} + r(A)(u) \implies r(A)(u) = 0
\]
Damit sind die
\[
u,Au, \dotsb, A^lu
\]
linear abhängig, was ein Widerspruch zur Minimalität von $f$ erzeugt (siehe Konstruktion in (a)).
Also ist $r(x)=0$.
\end{enumerate}
\end{proof}
\end{lem}


Aus der Minimalität in \ref{lem:12.10} (c) folgt, dass $f$ eindeutig ist.
(Normierung in (a) eindeutig, nicht nur bis auf skalare Vielfache)

\begin{lem}[Minimalpolynom]
	\label{lem:12.11}
	Sei $n=\dim V <\infty$ und $v_1,\dotsc,v_n$ eine Basis von $V$.
	Sei $\phi:V\to V$ linear mit darstellender Matrix $A$ bezüglich dieser Basis.
	Dann existiert ein eindeutiges Polynom $m\in K[x]$, sodass gilt:
	\begin{enumerate}[(a)]
		\item
			$\deg m\ge 1$ und $m$ ist normiert
		\item
			$\displaystyle {m(A)(u)=0} \forall u\in V$
		\item
			für $g\in K[x]$ mit $g(A)(u)=0, \forall u\in V$, dann gilt: $m|g$
	\end{enumerate}
	
	Seien $f_1,\dotsc, f_n$ die Polynome aus \ref{lem:12.10} für $u=v_1,\dotsc, v_n$.
	Dann gilt:
	\[
		m=\kgV(f_1,\dotsc, f_n)
	\]
	\begin{proof}
		Sei $v_1,\dotsc,v_n$ eine Basis von $V$.
		Zu $u:=v_i$ existiert nach \ref{lem:12.10} ein $f_i$.
		Sezte
		\[
			m:= \kgV(f_1,\dotsc,f_n)
		\]
		Wir zeigen: $m$ erfüllt (a), (b), (c).
		\begin{enumerate}[(a)]
			\item
				Ist erfüllt
			\item
				$m(A)$ ist linear und $\forall u\in V: m(A)(u)=0$ genau dann wenn
				\[
					m(A)(v_i) = 0 \qquad i=1,\dotsc,n
				\]
				Für ein $g_i\in K[x]$ ist:
				\[
					m(A) = \kgV(f_1,\dotsc,f_n) = f_i \cdot g_i
				\]
				Dann ist:
				\[
					m(A)(v_i) = g_i(A)\underbrace{f_i(A)(v_i)}_{=0} = 0
				\]
				Also folgt (b)
			\item
				Sei $g\in K[x]$ mit $g(A)(u)=0$ ($\forall u\in V$).
				Zeige $m|g$.
				\[
					g(A)(v_i) = 0  \stackrel{\ref{lem:12.10}}\implies f_i|g \quad\forall i=1,\dotsc,n
				\]
				Also
				\[
					m=\kgV(f_1,\dotsc,f_n) |g
				\]
				und daraus folgt (c)
		\end{enumerate}
	\end{proof}
\end{lem}

\begin{df}
	\label{df:12.12}
	Das Polynom aus \ref{lem:12.11} heißt \emph{Minimalpolynom} von $\phi$ oder $A$ und wird mit
	$
		m_A
	$
	bezeichnet
\end{df}

\begin{note}
	Zwei Methoden zur Bestimmung des Minimalpolynoms:
	\begin{itemize}
		\item
			Wähle Basis von $V$: $v_1,\dotsc,v_n$.

			Betrachte für jedes $v_i$:
			\[
				\overbrace{\underbrace{v_i, Av_i, \dotsc, A^{l-1}v_i}_{\text{ linear unabhängig}}, A^lv_i}^{\text{linear abhängig}}
			\]
			Also:
			\[
				A^lv_i = -a_0v_i - a_1Av_i - \dotsb - a_{l-1}A^{l-1}v_i
			\]
			Setze
			\begin{align*}
				f_i &:= a_0 + a_1x+\dotsb +a_{i}A^i+ \dotsb + a_{l-1}x^{l-1}+1\cdot x^l\\
			\end{align*}
			Dann ist offensichtlich
			\[ 
				f_i(A)(v_i) = 0
			\]

			Setze jetzt $m:= \kgV(f_1,\dotsc, f_n)$.
			\begin{note}
				Das kleinste gemeinsame Vielfache lässt sich folgendermaßen bestimmen:
				\[
					\kgV(f_1,f_2) = \frac {f_1f_2}{\ggT(f_1,f_2)}
				\]
			\end{note}
		\item
			Berechne das charakteristische Polynom $\chi_A$.
			Nach Cayley-Hamilton ist $\chi_A(A)=0$, also folgt man aus \ref{lem:12.11} (c), dass
			\[
				m_A | \chi_A
			\]
			Zerlege nun $\chi_A$ in Primfaktoren.
			
			In dieser Zerlegung lässt man dann nacheinander Faktoren weg, ohne die Bedinung, dass $A$ Nullstelle ist, zu verlezten.
			Wenn keine Faktoren mehr weggelassen werden können, ohne die Bedingung zu verletzen, hat man $m_A$ gefunden.
	\end{itemize}
\end{note}

Sei $\phi: V\to V$, $W<V$ und $W$ $\phi$-invariant.
Sei $m$ bekannt, dann ist
\[
	\phi|_W : W\to W
\]
Das Minimalpolynom von $\phi|_W$ ist ein Teiler von $m$.
$\phi$ induziert außerdem
\begin{align*}
	\_\phi: V/W &\to V/W\\
	u+W &\mapsto \phi(u)+W
\end{align*}
$\_\phi$ ist wohldefiniert, weil $W$ $\phi$-invariant.
Das Minimalpolynom $\_m$ teilt $m$.

\begin{ex}
	Gegeben $\phi: V\to V$ mit darstellender Matrix $A$, $\dim V<\infty$.
	Gesucht ist die Normalform.
	\begin{enumerate}[1{. Schritt}]
		\item
			Berechne das Minimalpolynom $m=m_\phi$.
			Zerlege $m$ in Primfaktoren:
			\[
				m = p_1^{a_1}\cdot p_2^{a_2} \cdot \dotsb \cdot p_l^{a_l}
			\]
			mit $a_i\ge 1$ und $p_1,\dotsc, p_l$ irreduzibel.

			Wende \ref{lem:12.9} iterativ an, um eine Kästchenform zu erzeugen.
			\begin{align*}
				m&=p_1^{a_1}\cdot (p_2^{a_2}\dotsb p_l^{a_l})\\
				f &= f_1\cdot f_2
			\end{align*}
			Definiere
			\begin{align*}
				W_1 &:= \ker(p_1^{a_1}(\phi))\\
				W_2 &:= \ker(p_2^{a_2}\dotsb p_l^{a_l}(\phi))
			\end{align*}
			Berechne $W_1, W_2$ mit dem Gauß-Verfahren.

			Wir erhalten damit eine erste Kästchenzerlegung:
			\[
			\begin{tabular}{*{3}{>{$}c<{$}}}
			 & \begin{matrix}
			   W_1 & W_2
			 \end{matrix} \\
			 \begin{matrix}
			   W_1 \\ W_2
			 \end{matrix} &
				\begin{pmatrix}
					\fbox{$*$} & 0 \\ 0 & \fbox{$*$}
				\end{pmatrix}
			\end{tabular}
			\]
			bezüglich der Basen von $W_1$ und $W_2$

			$p_1^{a_1}$ kann nicht mehr teilerfremd zerlegt werden.
			Aber den Rest können wir schreiben als
			\begin{align*}
				p_2^{a_2}\dotsb p_l^{a_l} &= p_2^{a_2}\cdot (p_3^{a_3}\dotsb p_l^{a_l})\\
				f &= f_1\cdot f_2
			\end{align*}
			Wende wiederum \ref{lem:12.9} an.  Nach $l$  Schritten ergibt sich insgesamt eine Matrix aus $\mathit{l}$ Kästchen.  Jedes Kästchen entspricht einem $p_i^{a_i}$.

		\item
			Fülle die Kästchen für jedes Kästchen separat:
			Wir haben also  ein
			\[
				\psi=\phi|_{W_i} : W_i \to W_i
			\]
			mit Minimalpolynom $p_i^{a_i}$.
			Gesucht ist eine Normalform.
			Dazu müssen wir eine passende Basis konstruieren.

			Für den Spezialfall $m=p=p_i^1$ ist
			\[
				p(x) = m(x) = a_0+a_1x+\dotsb + a_{l-1}x^{l-1} +x^l \qquad \text{für ein $l=\deg m\ge 1$}
			\]
			Wähle $v\in W_i$ mit $v\neq 0$, betrachte
			\[
				\overbrace{\underbrace{v,Av,A^2v,\dotsc,A^{s-1}v}_{\text{ linear unabhängig}},A^sv}^{\text{linear abhängig}}
			\]
			und wir können $A^sv$ als Linearkombination der vorigen schreiben.
			Führe jetzt die Konstruktion aus $\ref{lem:12.10}$ aus.
			Wir erhalten ein Polynom $f$ mit
			\[
				f(A)(v)=0
			\]
			$f$ ist minimal und hat Grad $s$.
			Da $m(A)(v)=0$ muss $f|m$ gelten.
			Weil $m=p$ irreduzibel war, ist also $f\in \{1,m\}$.
			Aber $v\neq 0$ und damit
			\[
				f=m=p \implies s=l
			\]
			Die Vektoren $v,Av,\dotsc,A^{l-1}v$ sind linear unabhängig, wir definieren $v_i$ als Basis mit diesen Vektoren.
			Da damit
			\begin{align*}
				\phi:v_0 &\mapsto v_1\\
				\phi:v_1 &\mapsto v_2\\
				\vdots \quad & \quad \vdots\\
				\phi:v_{l-2} &\mapsto v_{l-1}\\
				\phi:v_{l-1} &\mapsto \sum_{i=0}^{l-1} a_iv_i
			\end{align*}
			erhalten wir folgende Form:
			\[
				\begin{tabular}{*{3}{>{$}c<{$}}}
					& \begin{matrix} v_0 & \!\cdots\! & v_{l-2}\!  & \!v_{l-1}  \end{matrix} \\
				 \begin{matrix} v_0 \\ v_1 \\ \vdots \\ v_{l-1} \end{matrix} &
				 \begin{pmatrix} 0 & \cdots  & 0 & a_0 \\ 1 &  & 0 & a_1 \\ & \ddots &  & \vdots \\ 0 &  & 1 & a_{l-1} \end {pmatrix}  
				\end{tabular}
			\]
			Falls $\langle v_0,\dotsc, v_{l-1}\rangle = W$, sind wir fertig.
			Falls nicht, dann ist $\langle v_0,\dotsc, v_{l-1}\rangle$ ist $\phi$-invariant, also betrachte
			\[
				W|_{\langle v_0,\dotsc,v_{l-1}\rangle}
			\]
			$\_m=m=p$ ist irreduzibel, wende also die selbe Methode beim Quotienten $W/W|_{\langle v_0,\dotsc,v_{l-1}\rangle}$ an. Wir betrachten also, dass Minimalpolynom
			der Projektion. % wir werden hysterischer :D

			Oder wähle $w\in W, w\not\in \langle v_0,\dotsc, v_{l-1} \rangle$.
			Betrachte $w,Aw,\dotsc, A^{l-1}w$ und schreibe $A^lw$ als Linearkombination der vorigen.
			Allerdings wissen wir nicht, ob
			\[
				\<w,Aw,\dotsc,A^{l-1}w\> \cap \<v,Av,\dotsc,A^{l-1}v\> \stackrel ?= 0
			\]
			Es könnte schließlich
			\[
				\sum b_i A^i v = \sum c_iA^i w \implies \sum (b_iA^iv - c_i A^i w) = 0
			\]
			sein.

			Wie wählt man $v,w$ geschickt, sodass wir disjunkte Kästchen erhalten?
			
			Wir haben nur dem Spezialfall $m=p^1$ betrachtet, wie wählen wir $v,w$ im Fall $m=p^d$?
	\end{enumerate}
	
	Gegeben $V$, $\dim V<\infty$, $\phi: V\to V$.
	Das Minimalpolynom von $\phi$ ist
	\[
		m_\phi(x) = p(x)^d \qquad d\ge 1
	\]
	dabei ist $p(x)$ irreduzibel mit $\deg p = l$ und
	\[
		p(x) = \sum_{k=0}^l a_k x^k \qquad l\ge 1
	\]
	(das ist der Spezialfall $V=V_i$ in der $\phi$-invarianten Zerlegung $V_1\oplus\dotsb\oplus V_s$)

	Sei $0\neq v_0\in V$.
	$v_0$ hat „Minimalpolynom“ $f(x)$ mit $f(A)(v_0) = 0$.

	$\deg f$ ist minimal, also $f|m_\phi = p^d$.
	Damit existiert ein $a\le d$, so dass $f=p^a$.
	
	$f(x)$ hat die Form
	\begin{align*}
		f(x)&=\sum_{k=0}^s b_kx^k
	\end{align*}
	Weil $f(x)$ minimal ist mit $f(A)(v_0)$, folgt
	\begin{align*}
		v_0, Av_0, A^2v_0, \dotsc, A^{s-1}v_0 \text{ linear unabhängig}
	\end{align*}
	und es gibt kein kleineres $f$.

	Also ist $v_0$ mit $f=m=p^d$ die beste Wahl.
	Dafür ist $s=dl$, also gibt es linear unabhängige Vektoren
	\[
		v_0, Av_0, \dotsc, A^{dl-1}v_0
	\]
	solche $v_0$ existieren, da $m$ minimal.
	Wir sortieren die Vektoren in $V$ nach „Minimalpolynom“:
	\begin{alignat*}{4}
		&		&V_d&= \ker(p(A)^d)& &= V \\
  &\supset &V_{d-1} &= \ker(p(A)^{d-1})& \\
	 &\supset &\vdots\;&=\qquad \vdots &&\\
		&\supset &V_0 &= \ker(p(A)^0) &&= \{0\}
	 &\end{alignat*}
	Die Vektoren in $V_i$ haben Minimalpolynom $p^j$ mit $j\le i$.

\end{ex}

\begin{lem}
	\label{lem:12.13}
	Sei $V$ endlichdimensional und $\phi:V\to V$ ein Endomorphismus mit Minimalpolynom $m_\phi = p^d$ ($d\ge 1$), wobei $p$ irreduzibel von Grad $\deg p=l\ge 1$ ist.
	Definiere $V_i := \ker(p(A)^i)$.
	Dann gilt:
	\begin{enumerate}[(a)]
		\item
			$V=V_d \supsetneq V_{d-1} \supsetneq \dotsb \supsetneq V_1 \supsetneq V_0=\{0\}$
		\item
			Seien $v_1,\dotsc, v_r \in V_i$ ($i\ge 2$) linear unabhängig in $V_i/ V_{i-1}$.
			Dann sind 
			\[
				p(A)v_1,\dotsc, p(A)v_r
			\]
			linear unabhängig in $V_{i-1}/ V_{i-2}$.
			
			Für $0\le j<i$ sind 
			\[
				p(A)^jv_1, \dotsc, p(A)^jv_r
			\]
			linear unabhängig in $V_{i-j}/V_{i-j-1}$.

			Außerdem sind die Vektoren
			\begin{align*}
				v_1,&\dotsc, v_r \\
				p(A)v_1,&\dotsc, p(A)v_r \\
				 & \dotsc, \\
				p(A)^{i-1}v_1,&\dotsc, p(A)^{i-1}v_r \\
			\end{align*}
			linear unabhängig in $V$ für $i\le d$.
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[(a)]
			\item
				Zeige $V_i \subsetneq V_{i+1}$ ($\forall i$).
				Es gilt
				\[
					V_i=\ker(p(A)^i) \subset \ker(p(A)^{i+1})=V_{i+1}
				\]
				Wir müssen also noch zeigen, dass die Inklusionen echt sind.
				Falls $V_i=V_d$, dann wäre $p^i$ das Minimalpolynom für $i<d$, Widerspruch.
				Also muss $V_i\neq V_d$ sein.

				Damit existiert $u\in V$ sodass $u\not\in V_i$.
				Es existiert in diser Kette also ein minimales $j$, sodass $u\in V_j$, aber $u\not \in V_{j-1}$, $j>i$.
				Damit ist
				\begin{align*}
					p(A)^j u &= 0 \\
					p(A)^{j-1}u &\neq 0
				\end{align*}
				Um das auf beliebige $V_i$ zu übertragen, wähle wir einen Vektor $y:= p(A)^{j-i-1}u$.
				Dann ist
				\begin{alignat*}{4}
					p(A)^{i+1}y &= p(A)^{i+1+j-i-1}u &&= &p(A)^ju &= 0 \\
					   p(A)^i y &= p(A)^{i+j-i-1}u&&= &p(A)^{j-1} u &\neq 0
				\end{alignat*}
				Also $y\in V_{i+1} \land y\not\in V_i$, womit die Inklusion zwischen $V_{i} \subset V_{i+1}$ echt ist (für beliebiges $i=0,\dotsc,d-1$).				
			\item
				Seien $v_1,\dotsc, v_r \in V_i$ linear unabhängig in $V_i/ V_{i-1}$, d.h für ein $w\in V_{i-1}$ gilt
				\[
					\sum_{k=1}^{r}\lambda_rv_r = w \implies \lambda_1 = \dotsb = \lambda_r = 0
				\]
				(die $v_1,\dotsc,v_r$ sollen keine Komponenten in $V_{i-1}$ haben)

				Wegen der Definition der $V_i$ ist offensichtlich für $v_j\in V_i$
				\[
					0 = p(A)^i v_j = p(A)^{i-1}(p(A)v_j)
				\]
				Also ist $p(A)v_j \in V_{i-1}$.

				Wir zeigen jetzt, dass die so gewonnenen $p(A)v_j$ wiederum linear unabhängig in $V_{i-1}/V_{i-2}$ sind.

				Falls $\sum_{k=1}^r\lambda_kp(A)v_k = w'$ für ein $w'\in V_{i-2}$, dann ist $p(A)^{i-2}w' = 0$.
				Wir wenden darauf $p(A)^{i-2}$ an:
				\begin{align*}
					0= p(A)^{i-2}w' &=p(A)^{i-2}\left(\sum_{k=1}^r\lambda_k p(A)v_k\right) \\
															   &= p(A)^{i-1}\bigg(\underbrace{\sum_{k=1}^r\lambda_kv_k}_{\in V_{i-1}}\bigg)\\
				\end{align*}
				Aber für alle $w= \in V_{i-1}$ waren die $v_1,\dotsc,v_r$ bereits linear unabhängig.
				Also ist
				\[
					\lambda_1 = \lambda_2 = \dotsb = \lambda_r = 0
				\]
				Wir fahren mit Induktion (für $0\le j<i$) fort.
				Damit haben wir die Behauptung für beliebige $0<i\le d$ und beliebige $0\le j<i$ gezeigt.

				Es bleibt zu zeigen, dass die Vektoren auch zwischen den $V_i$ linear unabhängig sind, also
				\[
					v_1, \dotsc, v_r, \dotsc, p(A)^{i-1}v_1, \dotsc, p(A)^{i-1}v_r \quad \text{sind linear unabhängig}
				\]
				Es ist
				\[
					V_i \quad\isomorph\quad V_i/ V_{i-1} \;\oplus\; V_{i-1} \quad\isomorph\quad V_i/V_{i-1} \;\oplus\; V_{i-1}/V_{i-2} \;\oplus \dotsb \oplus\; V_1 / V_0
				\]
				Damit wäre für
				\[
					\sum_{k=1}^r \lambda_kv_k + \sum_{k=1}^r \lambda_k'p(A)v_k + \dotsb + \sum_{k=1}^r \lambda_k^{(i-1)}p(A)^{i-1}v_k = 0
				\]
				induktiv zunächst $\lambda_1=\dotsb=\lambda_r = 0$, dann $\lambda_1'=\dotsb=\lambda_r'=0$ und so fort.
		\end{enumerate}
	\end{proof}
\end{lem}
		

Für $v_j$ ist $\langle v_j, Av_j, A^2v_j, \dotsc\rangle$  $\phi$-invariant.
Es ist
\[
	\langle v_j, p(A)v_j, p(A)^2v_j, \dotsc \rangle \subset \langle v_j, Av_j, A^2v_j, \dotsc\rangle
\]
Aber $Av_j$ könnte ein $v_h$ sein mit $h\neq j$.
Also müssen wir $v_1, \dotsc, v_r$ noch geschickter wählen:
\[
	v_1, Av_1, \dotsc, v_2, Av_2, \dotsc
\]
Problem: Welche $A^iv_2$ sind linear unabhängig?


\begin{lem}
	\label{lem:12.14}
	Seien $v_1,\dotsc, v_r \in V_d$ Vektoren, sodass
	\[
		v_1,Av_1, \dotsc, A^{l-1}v_1, v_2, Av_2, \dotsc, A^{l-1}v_2, \dotsc, v_r,Av_r, \dotsc, A^{l-1}v_r
	\]
	linear unabhängig sind in $V_d/V_{d-1}$ und es keinen Vektor $v_s$ gibt, sodass obige Vektoren zusammen mit $v_s,Av_s,\dotsc, A^{l-1}v_s$ wieder linear unabhängig.
	Dann ist
	\[
		V_d =  V_{d-1} + \langle v_1,Av_1, \dotsc, A^{l-1}v_1, v_2, Av_2, \dotsc, A^{l-1}v_2, \dotsc, v_r,Av_r, \dotsc, A^{l-1}v_r\rangle
	\]
	Die Vektoren
	\begin{align*}
		v_1, Av_1, &\dotsc, A^{l-1}v_1, A^lv_1, \dotsc, A^{dl-1}v_1\\
		v_2, Av_2, &\dotsc, A^{dl-1}v_2\\
		&\vdots\\
		v_r, Av_r, &\dotsc, A^{dl-1}v_r
	\end{align*}
	sind linear unabhängig in $V$.
	\begin{note}
		$\langle v_i, Av_i, \dotsc, A^{dl-1}v_i\rangle$ ist $\phi$-invariant.
	\end{note}
	\begin{proof}
		\fixme[Konnte ich nicht komplett nachvollziehen, unübersichtlich]
		Definiere
		\[
			W:= V_{d-1} + \langle v_1, Av_1, \dotsc, v_r, \dotsc, A^{l-1}v_r\rangle \subset V_d
		\]
		es gilt für jedes $i$
		\begin{align*}
			V_{d-1} \ni p(A)v_i = a_0v_i + a_1Av_i + \dotsb + a_{l-1}A^{l-1}v_i + A^lv_i
		\end{align*}
		Also ist für alle $i=1,\dotsc,r$
		\[
			A^lv_i \in V_{d-1} + \langle v_1,\dotsc, A^{l-1}v_r\rangle
		\]
		Damit ist $V_{d-1}$ $A$-invariant.
		Zeige: $W = V_{d-1}$.
		Angenommen $W\subset V_d$ und $W\neq V_d$.
		Also existiert $0\neq u\in V_d \setminus W$, sodass
		\[
			u, Au, \dotsc, A^{l-1}u
		\]
		nicht alle modulo $V_{d-1}$ linear unabhängig sein können.
		Wähle $\_u\in V_d/W$ und eine Abbildung $\_\phi: V_d/W \to V_d/W$.
		\[
			m_{\_\phi}| m_{\phi} = p^d \implies m_{\_\phi} = p^l \qquad \text{für ein $l\le d$}
		\]
		Die Restklassen $\_u, \_A\_u, \dotsc, \_A^{l-1}\_u$ sind nicht linear unabhängig.
		Das Minimalpolynom $q$ von $\_u$ hat Grad $\le l-1$.
		Aber $q|p^e$, also $p|q$.
		Das ist ein Widerspruch zu $\deg p=l, \deg q \le l-1$.

		Beweise den zweiten Teil mit \ref{lem:12.13}.
		Demnach sind die Vektoren
		\[
			u_1,\dotsc, u_r,p(A)u_1, \dotsc, p(A)u_r, p(A)^2u_1, \dotsc, p(A)^2u_r, \dotsc
		\]
		linear unabhängig.
		Wähle für $u_1,\dotsc, u_s$ die Vektoren $v_1,Av_1, \dotsc, A^{l-1}v_1,v_2,\dotsc, A^{l-1}v_r$.
		Also sind
		\begin{alignat*}{3}
			v_1, Av_1, \dotsc, A^{l-1}v_1 &, v_2, \dotsc, A^{l-1}v_r\\
			p(A)v_1, p(A)Av_1, \dotsc, p(A)A^{l-1}v_1 &, p(A)v_2, \dotsc, p(A)A^{l-1}v_r\\
										   &\vdots\\
			p(A)^{d-1}v_1, p(A)^{d-1}Av_1, \dotsc, p(A)^{d-1}A^{l-1}v_1 &, p(A)^{d-1}v_2, \dots, p(A)^{d-1}A^{l-1}v_r
		\end{alignat*}
		linear unabhängig.
		Wir zeigen, dass wir überall $p(A)$ durch $A^l$ ersetzen können und wieder linear unabhängige Vektoren erhalten:
		Nach \ref{lem:12.13} ist
		\[
			\dim \langle v_1, Av_1, \dotsc, A^{l-1}v_1, p(A)v_1\rangle = l+1
		\]
		und wir können $p(A)$ schreiben als
		\[
			p(A)v_1 = a_0v_1 + a_1Av_1 + \dotsb + a_{l-1}A^{l-1}v_1 + A^lv_1
		\]
		Also müssen auch $v_1, Av_1, \dotsc, A^{l}v_1$ linear unabhängig sein.
		Wir können also die $p(A)$ durch $A^l$ ersetzen.
		Es ergibt sich:
		\[	
			v_1, Av_1, \dotsc, A^{dl-1}v_1, v_2, \dotsc, A^{dl-1}v_r
		\]
	\end{proof}
\end{lem}

Wenn wir die Vektoren $v_1,\dotsc, A^{dl-1}v_r$ zu einer Basis ergänzen, erhalten wir eine Matrix
\[
	\begin{pmatrix}
		0 & 0 & \cdots & 0 & -b_0 \\
		1& 0 &\cdots & 0 & -b_1 \\
		0& 1 & \cdots & 0 & -b_2 \\
		\vdots & \vdots & \ddots & \vdots & \vdots \\
		0 & 0& \cdots & 1 &  -b_{dl-1}
	\end{pmatrix}
\]
Die Koeffizienten $b_i$ ergeben sich als Koeffizienten des Minimalpolynoms
\[
	p(x)^d = \sum_{k=0}^{dl-1}b_kx^k
\]


\begin{thm}
	\label{thm:12.15}
	Sei $\dim V<\infty$ und $\phi: V\to V$ ein Endomorphismus mit Matrix $A$.
	Sei $p(x)$ irreduzibel vom Grad $l$ mit $m_\phi = p^d$.
	Dann gibt es Vektoren $v_{ij}$ ($1\le i \le d \;\land 1\le j \le r_i$) mit $r_d >0$, $r_i\ge 0, \forall i$ so dass gilt:
	\begin{enumerate}[(1)]
		\item
			Die Vektoren
			\[
				A^h v_{ij} \qquad\qquad  (1\le i \le d \;\land\; 1\le j \le r_i \;\land\;  0\le h<i\cdot l)
			\]
			bilden eine Basis von $V$.
		\item
			Für $1\le s\le d$ hat $V_s := \ker p(A)^s$ die Basis 
			\[
				A^hv_{ij} \qquad\qquad (1\le i\le d \;\land\; 1\le j \le r_i \;\land\; (i-s)\cdot l \le h < i\cdot l)
			\]
		\item
			Bezüglich dieser Basis aus den $A^hv_{ij}$ hat $\phi$ die Matrix
			\[
				\begin{pmatrix}
					B_{11} & \cdots &0 & 0 & \cdots & 0\\
					\vdots & \ddots &\vdots &  \vdots & \ddots & \vdots\\
					0 & \cdots & B_{1,r_1} & 0 & \cdots & 0\\
					0 & 0 & 0 & B_{2,1} &\cdots & 0\\
					\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
					0 & \cdots & 0 & 0 & \cdots & B_{d,r_d}
				\end{pmatrix}
			\]
			wobei der Block $B_{ij}$ folgende Form hat:
			\[
				\begin{pmatrix}
					0 & 0 & \cdots & 0 & -b_0 \\
					1& 0 &\cdots & 0 & -b_1 \\
					0& 1 & \cdots & 0 & -b_2 \\
					\vdots & \vdots & \ddots & \vdots & \vdots \\
					0 & 0& \cdots & 1 &  -b_{il-1}
				\end{pmatrix}
				\qquad
				m_\phi(v_{ij})(x) = x^{il} + b_{il-1}x^{il-1} + \dotsb + b_1x + b_0
			\]
			Der Block $B_{d1}$ existiert immer und die Basisvektoren zu $B_{ij}$ erzeugen einen nicht weiter zerlegbaren $\phi$-invarianten Teilraum.
	\end{enumerate}
	\begin{proof}
		Siehe Herleitung oben
	\end{proof}
\end{thm}

\begin{thm}
	\label{thm:12.16}
	Sei $\dim V<\infty$, $\phi:V\to V$ linear mit Matrix $A$ und $m_\phi = p_1^{d_1}\dotsb p_s^{d_s}$ so dass $p_1,\dotsc, p_s$ irreduzibel und paarweise verschieden sind.
	Dann ist $V = \bigoplus_{k=1}^s V_k$ (direkte Summe $\phi$-invarianter Teilrämue, so dass $\phi\big|_{V_i} : V_i \to V_i$ Minimalpolynom $p_i^{d_i}$ hat und damit Kästchenform wie in \ref{thm:12.15}.
\end{thm}

\begin{df}
	Die Normalform in \ref{thm:12.15} und \ref{thm:12.16} heißt \emph{rationale Normalform} von $\phi$ bzw. von $A$.
\end{df}

Es bleibt zu zeigen, dass diese Normalform auch eindeutig und leicht zu berechnen ist.

%vorlesung vom 11.6.12
\begin{ex}[Beispiel zur rationalen Normalform]
\fixme{erfordert Überarbeitung}\\
\[A= \begin{pmatrix} 1 & 1 & 0 & 0\\ -1 & 3& 0 & 0\\0&0&2&0\\0&0&0&2 \end{pmatrix} , B= \begin{pmatrix} 9&-7&0&2\\7&-5&0&2\\4&-4&2&1\\0&0&0&2\end{pmatrix} \]
1.Schritt: Bestimme das charakteristische Polynom:\\
\[det(A-xE)=\begin{vmatrix} 1-x&1&0&0\\-1&3-x&0&0\\0&0&2-x&0\\0&0&0&2-x\end{vmatrix}=\pm(2-x)(2-x)((1-x)(3-x)+1)\\
=\pm(2-x)^2(x^2-x-3x+3+1)=\pm(2-x)^2(x^2-4x+4)=\pm (2-x)^2(2-x)^2=\pm(x-2)^4 \rightarrow Eigenwert:2\\
B:\pm(2-x)^2((9-x)(-5x+49) \Rightarrow \chi_B=\pm(x-2)^4 \]%nicht sicher ob ich das richtige abgeschrieben hab

A hat mindestens EV $e_3$ und $e_4$, B hat mindestesn $e_3$\\
2.Schritt: bestimmen des Minimalpolynom\\
Erste Methode:Teiler des charakteristischen Polynoms:\\
$\chi_A=(x-2)^4, p(x)=x-2$ irreduzibel\\
$A-2E=p(A)\neq0$ Versuche $p^2(x)^2=(x-2)^2\\$
\[(A-2E)^2=A^2-4A+4E=\begin{pmatrix} -11 & -1 & 0 & 0\\ -1 & -1& 0 & 0\\0&0&2&0\\0&0&0&0 \end{pmatrix}^2\rightarrow m_A=(x-2)^2\]

Zweite Methode:Basisvektor,zb $e_1$, betrachte $e_1,Ae_1,A^2_e1$\\
\[e_1=\begin{pmatrix}1\\0\\0\\0\end{pmatrix},Ae_1=\begin{pmatrix}1\\-1\\0\\0\end{pmatrix}, A^2e_1=\begin{pmatrix}0\\-4\\0\\0\end{pmatrix}\]
$e_1,Ae_1,A^2_e1$ sind linear abhängig:$A^2e_1=4Ae_1-4e_1\Leftrightarrow A^2e_1-4Ae_1+4e_1=0\\
\Rightarrow e_1$ hat Minimalpolynom $\underbrace{x^2-4x+4}_{(x-2)^2}=0$\\
$e_2$ hat das selbe Minimalpolynom, $e_3$ und $e_4$ sind EV ($e_3, Ae_3=2e_3 : x-2)\Rightarrow$ Minimalpolynom $(x-2)$\\
$m_A=kgV=(x-2)^2$
\\
Bei B:$e_3$ hat Minimalpolynom x-2$(e_3 ist EV). e_1,e_2,e_4$ haben Minimalpolynom $(x-2)^2$ anders als bei A.\\
Insgesammt:$m_A=m_B=(x-2)^2,d=2$\\
3.Schritt:Bestimmung der Basis zur Normalform:\\
$\{ 0\}=V_0\subsetneqq V_1\subsetneqq V_2=V_d=V$\\ $V_2=\ker(p(A)^2)=V$ $V_1=\ker(p(A))=$Eigenraum$=\{u:Au=2u\}$
\[\begin{pmatrix}-1&1&0&0\\-1&1&0&0\\0&0&0&0\\0&0&0&0\end{pmatrix} \begin{pmatrix}x_1\\x_2\\x_3\\x_4\end{pmatrix}=\begin{pmatrix}0\\0\\0\\0\end{pmatrix}
\]
$-x_1+x_2=0$\\
Basis des Lösungsraums:\[ \begin{pmatrix} 1\\1\\0\\0\end{pmatrix},e_3=\begin{pmatrix}0\\0\\1\\0\end{pmatrix},e_4=\begin{pmatrix}0\\0\\0\\1\end{pmatrix}\]
$\dim V_1=3$(3-dim Eigenraum)$\Leftrightarrow$ A nicht diagonalisierbar.
$\dim V_2/V_1=1(=4-3)\Rightarrow$Wähle einen Vektor in $V_2-V_1$(Basis für $V_2/V_1$)
Wähle $v_{2,1}=e_1=\begin{pmatrix}1\\0\\0\\0\end{pmatrix},Ae_1=\begin{pmatrix}1\\-1\\0\\0\end{pmatrix}$
($e_1,Ae_1,A^2e_1$ sind linear abg. $\Rightarrow A^2_e1$ gehört nicht mehr dazu)
$\Rightarrow $ Beitrag zur Basis ist $e_1,Ae_1:[e_1,Ae_1]$ ist A-invariant.\\
Als Nächstes: betrachte $V_1$ $\underbrace{p(A)_{v_2,1}}_{=Av_{2,1}-2v_{2,1}=\begin{psmallmatrix}-1\\-1\\0\\0\end{psmallmatrix}}\in V_1$\\
Bleibt noch: $\dim V_1-1$\\

Wähle $e_3,e_4$ als Zusätzliche Vektoren.\\
Jeder gewählte Vektor liefert ein Kästchen:
  \[
\begin{tabular}{lcccr}

$V_2$ & $v_{2,1}=e_1$ & &&$Ae_3\in[e_3]$ A- invariant \\
$V_1$ & $Ae_1$ & $v_{1,1}=e_3$ & $v_{1,2}=e_4$&$Ae_4\in[e_4]$ A-invariant\\
$V_0$&
 \end{tabular}\]\[
 V=\underbrace{[e_1,Ae_1]}_{\text{Kästchen 2x2}}\oplus\underbrace{[e_3]}_{\text{1x1}}\oplus\underbrace{[e_4]}_{\text{1x1}}
 =NF= \begin{tabular}{*{3}{>{$}c<{$}}}
				 & \begin{matrix} \begin{smallmatrix}e_1 &Ae_1\end{smallmatrix} &\begin{smallmatrix} e_3 & e_4\end{smallmatrix}  \end{matrix} \\
				 &
				 \begin{pmatrix}
				 \begin{smallmatrix} 0&-4\\1&4 \end {smallmatrix} & 0 \\ 0& \begin{smallmatrix}2&\\&2 \end{smallmatrix}
				 \end{pmatrix}&\begin{smallmatrix}m\\(x-2)^2\\x-2\\x-2\end{smallmatrix}
				\end{tabular}
 \]
Für B:\\
\[B-2E=\begin{pmatrix}
7&-7&0&2\\7&-7&0&2\\4&-4&0&1\\0&0&0&0
\end{pmatrix}
\]
\[V_1=\ker(B-2E)\]
\[
 7x_1-7x_2+2x_4=0\]
\[
 4x_1-4x_2+x_4=0\]
\[
 8x_1-8x_2+2x_4=0\]
\[
 x_1-x_2=0\]
\[
 \Rightarrow \dim V_1=2 \text{(wähle}x_3,x_1=x_2)
\]
Betrachte $V_2/V_1:$2-dimensional. Wähle 2 Vektoren $v_{2,1}$ und $v_{2,2}$, deren Restklassen eine Basis von $V_2/V_1$ bilden. z:B $v_{2,1}=e_1$ und $v_{2,2}=e_4$
$\Rightarrow $ Basis $e_1,Be_1,e_4,Be_4 \Rightarrow$ zwei 2x2 Kästchen.\\
B-invariante Zerlegung von $V:V=[e_1,Be_1]\oplus [e_4,Be_4]$ \\
$v_{2,1}=e_1, Bv_{2,1}=\begin{pmatrix}9\\7\\4\\0\end{pmatrix}$ (weiter:$B^2v_{2,2}=\begin{pmatrix}8\\8\\4\\4\end{pmatrix}=4Bv_{2,1}-4v_{2,1})$
\[
 \begin{tabular}{c}$
 \begin{smallmatrix}v_{2,1}&Bv_{2,1}&v_{2,2}&Bv_{2,2}\end{smallmatrix}$\\
 $\begin{pmatrix}\begin{smallmatrix}0&-4\\1&4\end{smallmatrix}&0\\ 
 0&\begin{smallmatrix}0&-4\\1&4\end{smallmatrix}
 \end{pmatrix}$ 
 \end{tabular} \text{rat. NF von B} \Rightarrow \text{A und B sind nicht ähnlich}
 \]

\end{ex}
\begin{prop}
	\label{prop:12.18}
	  Sei $\phi:V\longrightarrow V$ linear und $B_1$ und $B_2$ Basen von V, bezüglich derer zu $\phi$ eine Matrix in rationaler NF gehört mit Kästchen $B^1_{ij}$ bzw $B^2_{lm}$. 
	  Dann stimmen  die Kästchen $B^1_{ij}$ bis auf die Anordnung mit den Kästchen $B^2_{lm}$ überein. Die rationale NF ist also eindeutig bis auf Anordnung
	  \begin{proof}
	   $\phi$ bestimmt das charakteristische Polynom $\chi_{\phi}$. Primfaktorzerlegung $\chi{\phi}=p_1^{d_1}\cdots p_l^{d_l}$ ist eindeutig.
	   $p_1,\cdots ,d_1,\cdots$ (bis auf Reihenfolge).\\
	   Betrachte $p=p_i$ separat.Dazu gehören Kästchen, zu Minimalpolynom $p,p^2,p^3,\cdots$\\
	   $U_j=\ker(p(\phi)^j):U_0=\{0\}\subset U_1\subsetneq \cdots \subsetneq \underbrace{U_d=V(p)}_{=V_i \text{für}p=p_i} \Rightarrow \text{Minimalpolynom}(\phi |_{v_i} %nicht sicher ob phi| oder phi/
	   )=p_i^d$ durch $\phi$ bestimmt.\\
	   $\dim U_d/U_{d-1}$ bestimmt, vie viele Vektoren $v_{d,1},\cdots ,v_{d,d}$ wir wählen dürfen.\\
	   Lemma \ref{lem:12.14}: $v_1$, $\phi(v_1),\cdots,\phi^{l-1}(v_1),\cdots,v_r,\cdots,\phi^{l-1}(v_r)$ Basis von $U_d/U_{d-1}$\\
	   $l=\deg(p)$ liegt fest $\Rightarrow \dim U_d/U_{d-1=rl} \Rightarrow r$ ist bestimmt $r=\#$ Kästchen zum Minimalpolynom $p^d$. Weiter mit Induktion.
	  \end{proof}
\end{prop}


\section{Die Jordansche Normalform}


Wir betrachten nun den Spezialfall $K=\C$.

Die irreduzible Polynome sind genau die linearen Polynome $(x-\lambda)$, $\lambda \in \C$.
Es gibt also immer Eigenwerte (damit ist auch jede lineare Abbildung $\phi$ trigonalisierbar).

Wir wollen nun die rationale Normalform unter der Vorraussetzung $K=\C$ abändern, sodass eine einfachere Normalform entsteht (z.b. Dreiecksmatrizen).

Gegeben $\phi : V\to V$ linear, $V$ ein $\C$-Vektorraum.
Das Minimalpolynom ist dann gegeben durch:
\[
	m_\phi = \prod (x-\lambda_i)^{d_i}
\]
und wir haben in der rationalen Normalform zu jedem $(x-\lambda_i)^{d_i}$.
Wir betrachten jetzt ein einzelnes Kästchen mit Minimalpolynom
\[
	m_\phi(x) = (x-\lambda)^d
\]
dann hat $p(x) = x-\lambda$ Grad $l=1$.

Die Vorschrift für die rationalen Normalform besagt:
„Wähle $y_0 (= v_{d,1})$ mit Minimalpolynom $(x-\lambda)^d$ und dazu $y_i = A^iy_0$ bis $i=dl-1=d-1$“

Wir wählen eine neue Basis für $\<y_0,\dotsc,y_{dl-1}\>$.
Wähle 
\begin{align*}
	x_0 &:= y_0\\
	x_1 &:= (A - \lambda I)x_0\\
	x_2 &:= (A - \lambda I)^2x_0\\
	\vdots \quad & \qquad \vdots\\
	x_{d-1} &:= (A-\lambda I)^{d-1}x_0
\end{align*}
Damit erhalten wir wieder $d$ Vektoren.
Aus den $x_i$ können wir die $y_i$ als Linearkombinationen schreiben:
\begin{align*}
	y_0 &= x_0\\
	y_1 &= Ax_0 = x_1 + \lambda x_0\\
	\vdots \quad & \qquad \vdots\\
\end{align*}
Also bilden die $x_i$ eine Basis von $\<y_0,\dotsc, y_{d-1}\>$.

Es gilt
\begin{align*}
	(x-\lambda)^{h+1} &=  (x-\lambda)(x-\lambda)^h = x(x-\lambda)^h -\lambda(x-\lambda)^h\\
	\implies x(x-\lambda)^h &= (x-\lambda)^{h+1} + \lambda(x-\lambda)^h
\end{align*}
Damit
\begin{align*}
	x_{h+1} &= (A-\lambda I)^{h+1} x_0 = (A-\lambda I)(A-\lambda I)^h x_0\\
	\implies A(A-\lambda I)^hx_0 &= (A-\lambda I)^{h+1}x_0 + \lambda (A-\lambda I)^h x_0
\end{align*}
d.h. es gilt:
\begin{align*}
	A x_h = x_{h+1} + \lambda x_h
\end{align*}
ausgenommen $h=d-1$.
In diesem Fall gilt
\begin{align*}
	(A-\lambda I)x_{d-1} &= (A-\lambda I)(A-\lambda I)^{d-1} x_0 = \underbrace{(A-\lambda I)^d}_{=0} x_0\\
	\implies Ax_{d-1} &= \lambda x_{d-1}
\end{align*}
Also ist $x_{d-1}$ Eigenvektor.

Die neue Basis liefert also für ein einzelnes Kästchen die Form:
\[
	\begin{pmatrix}
		\lambda & 0 & 0 & \cdots & 0 &0\\
		1 & \lambda & 0 & \cdots & 0 & 0\\
		0 & 1 & \lambda & \cdots & 0 &0\\
		\vdots & \vdots & \vdots & \ddots & \vdots &\vdots \\
		0 & 0 & 0 & \cdots & 1 & \lambda
	\end{pmatrix}
\]
Wir haben also eine untere Dreiecksmatrix, bei der alle Diagonaleinträge $\lambda$ sind ($m_\phi = (x-\lambda)^d$) und die Einsen in der ersten unteren Nebendiagonalen besitzt.

\begin{df}
	\label{df:12.19}
	Solche Kästchen heißen \emph{Jordan-Blöcke}.
	Die Normalform, bestehend aus Kästchen mit Jordan-Blöcken heißt Jordan-Normalform
\end{df}

Die Jordan-Normalform existiert für alle Matrizen in $\Mat(n\times n, K)$, wenn der Grundkörper die Eigenschaft hat, dass jedes Polynom positiven Grades ein Produkt von Linearfaktoren ist.
So ein $K$ heißt dann \emph{algebraisch abgeschlossen}.
Eine äquivalente Bedingung ist:
\[
	\forall f\in K[x], \deg f \ge 1  \exists \lambda \in K :f(\lambda) = 0
\]

\begin{ex}[Fortsetzung des letzten Beispiels]
	\[
	A = \begin{pmatrix}1&1&0&0\\-1&3&0&0\\0&0&2&0\\0&0&0&2\end{pmatrix}, \chi_A = (x-2)^4, m_A = (x-2)^2
	\]
	\[
v_{21} = \begin{pmatrix}1\\0\\0\\0\end{pmatrix}, Av_{21} = \begin{pmatrix}1\\-1\\0\\0\end{pmatrix}
	\]
	\[
	v_{11} = \begin{pmatrix}0\\0\\1\\0\end{pmatrix}, v_{12} = \begin{pmatrix}0\\0\\0\\1\end{pmatrix}
	\]
	und die rationale Normalform sah folgendermaßen aus:
	\[
	\begin{pmatrix} 0&-4&0&0\\1&4&0&0\\0&0&2&0\\0&0&0&2\end{pmatrix}
	\]
	Für die Jordannormalform setzen wir $v_{21}=y_0=x_0$ und $Av_{21}$ wird ersztet durch
	\[
	(A-\lambda I)x_0 = \begin{pmatrix}-1&1&0&0\\-1&1&0&0\\0&0&0&0\\0&0&0&0\end{pmatrix}\begin{pmatrix}1\\0\\0\\0\end{pmatrix} = \begin{pmatrix}-1\\-1\\0\\0\end{pmatrix} = x_1
	\]
	Bezüglich der neuen Basis $x_0, x_1, v_{11}, v_{12}$ bildet sich die Jordannormalform
	\[
	\begin{pmatrix}2&0&0&0\\1&2&0&0\\0&0&2&0\\0&0&0&2\end{pmatrix}
	\]
	Es ergibt sich
	\[
		Ax_1 = A(A - 2I)x_0 = A^2 x_0 -2Ax_0 = A y_1 - 2y_1
	\]
	oder
	\[
		\underbrace{(A-2I)^2}_{=0} x_0 = 0 = A(A-2I) - 2(A-2I)x_0 = Ax_1 - 2x_1 \implies Ax_1 = 2x_1
	\]		
\end{ex}

\begin{ex}
	\[
		\begin{pmatrix}
			9 & -7 & 0 & 2\\
			4 & -5 & 0 & 2\\
			4 & -4 & 2 & 1\\
			0 & 0 & 0 & 2
		\end{pmatrix}, \chi_B = (x-2)^4, m_B = (x-2)^2
	\]
	\[
	v_{21}=\begin{pmatrix}1\\0\\0\\0\end{pmatrix}, Bv{21} = \begin{pmatrix}9\\7\\4\\0\end{pmatrix}, v_{21}=\begin{pmatrix}0\\0\\0\\1\end{pmatrix}, Bv{22} = \begin{pmatrix}2\\2\\1\\2\end{pmatrix}
	\]
	die rationale Normalform hatte die Form
	\[
		\begin{pmatrix}
			0 & -4 & 0 & 0\\
			1 & 4 & 0 & 0\\
			0 & 0 & 0 & -4\\
			0 & 0 & 1 & 4\\
		\end{pmatrix}
	\]
	Wir wählen für die Jordan-Normalform
	\begin{align*}
		x_0 &:= v_{21}\\
	x_1 &:= (B-2I)x_0 = \begin{pmatrix}7\\7\\4\\0\end{pmatrix}\\
	x_0' &:= v_{22} = \begin{pmatrix}0\\0\\0\\1\end{pmatrix}\\
	x_1' &:= (B-2I)v_{22} \begin{pmatrix}2\\2\\1\\0\end{pmatrix}
	\end{align*}
	die Jordan-Normalform sieht dann folgendermaßen aus:
	\begin{align*}
		\begin{pmatrix}2&0&0&0\\1&2&0&0\\0&0&2&0\\0&0&1&2\end{pmatrix}
	\end{align*}
\end{ex}

\begin{note}
	Die Existenz der Jordan-Normalform braucht $K=\C$, aber die Rechnung kann auch über $\Q$ und $\R$ funktionieren (falls $\chi_A$ in Linearfaktoren über $\Q$ oder $\R$ zerfällt).
\end{note}

\begin{df*}
	Man nennt
	\[
		\ker(A -\lambda I)^{d_i}
	\]
	\emph{Hauptraum} zum Eigenwert $\lambda$ ($d_i$ ist die algebraische Vielfachheit der Nullstelle im charakteristischen Polynom).
	Die Zerlegung von $V$ in Haupträume, nennt man \emph{Hauptraumzerlegung}.
	Die Vektoren der Haupträume nennt man \emph{verallgemeinerte Eigenvektoren}.
\end{df*}

Wenn die Normalform existiert, kann man sie auch direkt ausrechnen, ohne zuerst die rationale Normalform zu bestimmen.

\begin{alg*}[Bestimmung der Jordannormalform]
	Gegeben Matrix $B$.
	\begin{enumerate}[{Schritt} 1]
		\item
			Bestimmung des charakteristischen Polynoms $\chi_B$ und Faktorisierung in Linearfaktoren.
			(Existiert keine Faktorisierung in Linearfaktoren, existiert die Jordan-Normalform nicht).

			Die Eigenwerte $\lambda_i$ sind die Nullstellen von $\chi_B$.
			Falls es mehrere Eigenwerte gibt ($\chi_B = \prod_i (B-\lambda_i I)^{d_i}$), dann zerlege mit \ref{lem:12.9}
			\[
				V = \bigoplus_i \ker(B-\lambda_i I)^{d_i}
			\]
			in $\phi$-invariante Teilräume und betrachte im nächsten Schritt jeden Teilraum separat ($(x-\lambda)^d$).
		\item
			Das charakteristische Polynom lautet in dem aktuellen Teilraum $i$
			\[
				(B-\lambda_i I)^{d_i}
			\]
			Berechne (z.B. durch Gauß-Verfahren) die verallgemeinerten Eigenräume
			\[
				\ker(B-\lambda I) \subset \ker(B-\lambda I)^2 \subset \dotsb \subset \underbrace{\ker(B-\lambda I)^f}_{=V_i}
			\]
			Das Minimalpolynom (des aktuellen Blockes) hat also das Aussehen $(x-\lambda)^f$.
			Bezeichne die $\ker(B-\lambda I)^j$ mit $V_j$.
			Wähle $r = \dim V_d - \dim V_{d-1}$ Vektoren, die in $V_d/V_{d-1}$ linear unabhängig sind.
			
			Zu jedem dieser Vektoren gibt es ein Kästchen in der Normalform.
			Die Basis zum $i$-ten Kästchen ist
			\[
				v_i, (B-\lambda I)v_i, (B-\lambda I)^2v_i, \dotsc, (B-\lambda I)^{d-1}v_i
			\]
		\item
			Fahre iterativ fort für die kleineren Unterräume $V_{d-k}$ und erzeuge Jordanblöcke der Größe $(d-k)\times (d-k)$.
	\end{enumerate}
\end{alg*}


\section{Die reelle Jordan-Normalform}


Betrachte $K=\R$.
Dort gibt es nicht-lineare irreduzible Polynome, z.B. $x^2+1$, aber man kann zeigen (Vorlesung Algebra), dass irreduzible Polynome über $\R$ immer Grad $1$ oder $2$ haben.

Falls das Minimalpolynom unserer Matrix $A \in \Mat(n\times n, \R)$  die Form
\[
	f(x)^d = (x^2+px+q)^d
\]
hat, mit $x^2+px+q$ irreduzibel, dann existiert keine Jordannormalform.
Die rationale Normalform lässt sich aber noch verbessern.

\begin{align*}
	x^2+px+q &= (x+\f 12 p)^2 + (q-\f 14 p^2)\\
	 x_{1,2} &= -\f 12 p \pm \sqrt{q-\f 14 p^2}
\end{align*}
In der rationalen Normalform wählt man Vektoren $v_1, Av_1, \dotsc$.
Im Beweis gab es auch die Möglichkeit:
\[
	v_1, Av_1, \dotsc, A^{d-1}v_1, p(A)v_1, Ap(A)v_1, \dotsc
\]
wir wählen für $p(x) := f(x)$.
\[
	f(A)v = A^2v +pAv + qv \implies A^2v = f(A)v - pAv -qv
\]
Wir wählen als neue Basis jetzt, indem wir abwechselnd A und f(A) auf den Basisvektor anwenden, gerade daraus ergibt sich die charakteristische Form.
\[
	v, Av, f(A)v, Af(A)v, f(A)^2v, Af(A)^2v,\dotsc
\]
(insgesamt $(dl-1)+1 = 2d$ Vektoren)\\
Bezüglich dieser Basis hat die Matrix die Form
\[
	\begin{pmatrix}
		0 & -q & 0 & 0  & 0 & \cdots \\
		1 & -p & 0 & 0  & 0 & \cdots\\
		0 & 1  & 0 & -q & 0 & \cdots\\
		0 & 0  & 1 & -p & 0 & \cdots\\
		0 & 0  & 0 & 1  & 0 & \cdots\\
		0 & 0  & 0 & 0  & 1 & \cdots\\
		\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \\
	\end{pmatrix}
\]
Man diese Form auch \emph{reelle Jordan-Normalform}.




\chapter{Euklidische und unitäre Vektorräume}


Wir wollen jetzt für Vektorräume geometrische Größen definieren, wie z.B. Längen von Vektoren und Winkeln zwischen Vektoren.
Dabei beschäftigen wir uns nur mit den Körpern $\R$ und $\C$.
Weiterhin beschäftigen wir uns mit Abbildungen, welche obige Größen erhalten (z.B. Drehungen, Streckungen, \dots).


\begin{ex}
	Sei $V=\R^2$ und $v=\begin{psmallmatrix}a\\b\end{psmallmatrix}$.
	
	Nach dem Satz von Pythagoras finden wir die Länge von $v$ durch Betrachtung von Senkrecht- und Waagrechtenkomponenten: %faulheit 
	\[
		\|v\| = \sqrt{a^2+b^2}
	\]
	Damit haben wir die Länge von $v$ aus seinen Koordinaten bestimmt.
\end{ex}


\begin{ex}
	Seien $u,v\in \R^2$. Die beiden Vektoren schließen einen Winkel $\alpha$ ein.  Zusammen mit dem Verbindungsvektor $v-u$ bilden die Vektoren ein Dreieck. % war zu faul, es zu zeichnen
	Sei $a:=\|a\|$, $b:=\|v\|$ und $c:=\|v-u\|$, dann besagt der Cosinussatz
	\[
		c^2 = a^2 + b^2 -2ab \cos \alpha
	\]
	Also ist $\cos \alpha$ durch die Koordinaten von $u$ und $v$ bestimmt.
\end{ex}


Wir definieren Zusatzstrukturen auf Vektorräumen über $\R$ und $\C$.
Mit diesen Zusatzstrukturen definieren wir dann Länge, Winkel, Orthogonalität, \dots

\begin{df}
	\label{df:13.1}
	Seien $V$ und $W$ $\R$-Vektorräume.
	Eine Abbildung $s: V\times W\to \R$ heißt \emph{Billinearform}, wenn
	\begin{enumerate}[{(S}1)]
		\item $
				\forall x,y \in V, z\in W: s(x+y,z) = s(x,z) + s(y,z)\\
				\forall x \in V, y,z\in W: s(x,y+z) = s(x,y) + s(x,z)
			$
		\item $
				\forall x\in V,y\in W,\lambda \in K: s(\lambda x, y) = \lambda s(x,y) = s(x,\lambda y)
			$
	\interitemtext{
		$s$ heißt \emph{symmetrisch}, wenn zusätzlich gilt: $V=W$ und
	}
		\item $
				\forall x,y\in V : s(x,y) = s(y,x)
			$
	\interitemtext{
		Eine symmetrische Billinearform $V\times V\to \R$ heißt \emph{Skalarprodukt} (oder \emph{inneres Produkt}), wenn $s$ \emph{positiv definit} ist:
	}
		\item $
				\forall x\in V, x\neq 0: s(x,x) > 0
			$
	\end{enumerate}
	Ein reeller Vektorraum mit einem Skalarprodukt heißt \emph{Euklidischer Vektorraum}.
\end{df}

\begin{ex}
	\begin{itemize}
		\item
			Sei $V=W=\R^n$, $e_1,\dotsc,e_n$ die Standardbasis und $x,y$ zwei Vektoren.
			Definiere
			\[
			s(x,y) := x^Ty = \begin{pmatrix}x_1&\cdots &x_n\end{pmatrix} \begin{pmatrix}y_1\\\vdots\\y_n\end{pmatrix}=\sum_{i=1}^n x_iy_i
			\]
			Dann ist
			\begin{alignat*}{3}
				s(u+v,y) &= (u+v)^T y = u^Ty +v^Ty &&= s(u,y) +s(v,y)\\
		  s(\lambda x,y) &= (\lambda x)^Ty = \lambda(x^T y) &&= \lambda s(x,y)\\
						s(x,y) &= x^Ty =\sum_{i=1}^nx_iy_i = \sum_{i=1}^ny_ix_i = y^Tx &&= s(y,x)\\
				s(x,x) &= x^Tx = \sum_{i=1}^nx_ix_i > 0 \iff x\neq 0
			\end{alignat*}
		\item
			Sei jetzt
			\[
				V=\{f:[0,1]\to \R: f \text{ stetig}\}
			\]
			und definiere
			\[
				s(f,g) := \int_0^1 f(x)g(x) dx
			\]
			die Axiome (S1), (S2), (S3) ergeben sich aus der Linearität des Integrals und der Kommutativität der Multiplikation.
			Für (S4) gilt
			\[
				s(f,f) = \int_0^1f(x)^2 dx > 0 \iff f\neq 0 \qquad (\text{wegen $f$ stetig})
			\]
			Die reine Integrierbarkeit der Funktionen reicht also nicht aus
		\item
			Sei der Vektorraum $V$ jetzt durch die Menge der quadratisch summierbaren Folgen (Hilbertscher Folgenraum $\scr l^2$)
			\[
				V=\{(a_i)_{i\in \N}\in \R^\N : \sum_{i=1}^\infty a_i^2 <\infty\}
			\]
			und definiere
			\[
				s(x,y) := \sum_{i=1}^\infty x_iy_i
			\]
			$s$ konvergiert stets, denn
			\[
				(|x_i| - |y_i|)^2 = x_i^2 +y_i^2 - 2|x_i||y_i| \le x_i^2 +y_i^2
			\]
			(rechte Seite definiert eine Majorante).
			Außerdem
			\[
				s(x,x) = \sum_{i=1}^\infty x_i^2 > 0 \iff x\neq 0
			\]
		\item
			Sei $V=\R^2$ und $v=\begin{psmallmatrix}a\\b\end{psmallmatrix}$ und $s(v,v) = v^Tv = a^2 +b^2$.
			Dann ergibt sich für die Länge von $v$
			\[
				\sqrt{s(v,v)}
			\]
			In $V=\R^1$ ist die Länge gegeben durch $\sqrt{x^2} =|x|$.
			In $V=\C^1$ ergibt sich
			\[
				\sqrt{zz} \not\in \R
			\]
			aber
			\[
				\sqrt{z\_z} \in \R
			\]
			Für $\C$ brauchen wir also ein anderes „Skalarprodukt“.

			Für $\C^n$ soll beispielsweise gelten: 
			\[
				s(x,y) = \sum_{k=1}^n \_x_k y_k
			\]			
	\end{itemize}
\end{ex}


\begin{df}
	\label{df:13.2}
	Seien $V$ und $W$ $\C$-Vektorräume.
	Eine Abbbildung $s: V\times W \to \C$ heißt \emph{Sesquilinnearform}, wenn
	\begin{enumerate}[({S}1)]
		\item
			$\forall x,y\in V, z\in W: s(x+y,z) = s(x,z) + s(y,z)$\\
			$\forall x\in V, y.z\in W: s(x,y+z) = s(x,y) + s(x,z)$
		\item
			$\forall x\in V, y\in W, \lambda \in \C : s(\lambda x,y) = \_\lambda s(x,y) = s(x,\_\lambda y)$
	\interitemtext{	
		$s$ heißt \emph{hermitesch}, wenn zusätzlich gilt: $V=W$ und
	}
		\item $\forall x,y\in V : s(x,y) = \_{s(y,x)}$
	\interitemtext{
		Eine hermitesche Sesquilinearform $V\times V\to \R$ heißt \emph{Skalarprodukt} (oder \emph{inneres Produkt}), wenn $s$ \emph{positiv definit} ist:
	}
		\item $\forall x\in V, x\neq 0: \R \ni s(x,x) > 0$
	\end{enumerate}
	Ein komplexer Vektorraum mit einem Skalarprodukt heißt \emph{Unitärer Vektorraum}.
\end{df}

\begin{ex}
	\begin{enumerate}
		\item
			Sei $V = \C^n$ und eine feste Basis gewählt.
			Definiere
			\[
				s(x,y) := \sum_{k=1}\_x_k y_k = \_x^Ty
			\]
			Dann gilt
			\begin{align*}
				s(\lambda x,y) &= \sum_{k=1}^n \_{\lambda x_k} y_k = \_\lambda s(x,y)\\
				s(y,x) &= \sum_{k=1}^n \_{y_k} x_k = \_{s(y,x)}\\
				s(x,x) &= \sum_{k=1}^n \_{x_k}x_k = \sum_{k=1}^n |x_k|^2 > 0 \qquad \forall x\neq 0
			\end{align*}
		\item
			Sei jetzt
			\[
				V=\{f=g+hi:[a,b]\to \C: f \text{ stetig}\}
			\]
			und definiere
			\[
				s(f_1,f_2) := \int_a^b \_{f_1(x)}f_2(x) dx
			\]
			Die Axiome (S1), (S2), (S3) ergeben sich aus der Linearität des Integrals und der Kommutativität der Multiplikation.
			Für (S4) gilt
			\[
				s(f,f) = \int_a^b |f(x)|^2 dx > 0 \iff f\neq 0 \qquad (\text{wegen $f$ stetig})
			\]
			Die reine Integrierbarkeit der Funktionen reicht also nicht aus
		\item
			Sei der Vektorraum $V$ jetzt durch die Menge der quadratisch summierbaren Folgen (Hilbertscher Folgenraum $\scr l^2$)
			\[
				V=\{(a_i)_{i\in \N}\in \C^\N : \sum_{i=1}^\infty |a_i|^2 <\infty\}
			\]
			und definiere
			\[
				s(x,y) := \sum_{i=1}^\infty \_{x_i}y_i
			\]
			$s$ konvergiert stets, denn
			\[
				|\_{x_i}y_i|=|x_i| |y_i|\le |x_i|^2+|y_i|^2 
			\]
			(rechte Seite definiert eine Majorante).
			Außerdem
			\[
				s(x,x) = \sum_{i=1}^\infty |x_i|^2 > 0 \iff x\neq 0
			\]
	\end{enumerate}
\end{ex}

\begin{note}
	$s(x,y)$ ist genau dann Sesquilinearform, wenn $s(x,\_y)$ oder $s(\_x,y)$ Billinearformen sind.

	Auch: $s(x,y)$ hermitesch, wenn $s(x,\_y)$ symmetrisch, etc.
\end{note}


\section{Beschreibung durch Matrizen}


Sei $V$ ein Vektorraum über $\R$, $\dim V=n$ und eine Basis $v_1,\dotsc, v_n$.
Sei $s(x,y)$ eine symmetrische Billinearform.
\begin{align*}
	s(x,y) &= s\left(\sum_{k=1}^nx_iv_i, \sum_{j=1}^ny_jv_j\right) \\
					   &= \sum_{i=1}^n\sum_{j=1}^n x_i s(v_i,v_j) y_j
\end{align*}
Wir definieren $M(s) := (s(v_i,v_j))_{i,j}$ und damit ist das Skalarprodukt gegeben durch
\[
	s(x,y) = x^T M(s) y
\]
Im Beispiel $s(x,y) = x^Ty$ ist $M(s) = (s(e_i,e_j)_{i,j}) = I$.

$M(s)$ hängt von der Wahl der Basis $v_1,\dotsc, v_n$ ab.

Da $s$ symmetrisch, ist $s(v_i,v_j) = s(v_j,v_i)$ und $M(s)$ ist damit auch symmetrisch.


Sei $V$ jetzt ein Vektorraum über $\C$, $\dim V=n$ und eine Basis $v_1,\dotsc, v_n$.
Sei $s(x,y)$ eine hermitesche Sesquilinearform.
\begin{align*}
	s(x,y) &= s(\sum_{k=1}^nx_iv_i, \sum_{j=1}^ny_jv_j) \\
					   &= \sum_{i=1}^n\sum_{j=1}^n \_{x_i} s(v_i,v_j) y_j
\end{align*}
Wir definieren $M(s) := (s(v_i,v_j))_{i,j}$ und damit ist das Skalarprodukt gegeben durch
\[
	s(x,y) = \_x^T M(s) y
\]
Im Beispiel $s(x,y) = x^Ty$ ist $M(s) = (s(e_i,e_j)_{i,j}) = I$.

$M(s)$ hängt von der Wahl der Basis $v_1,\dotsc, v_n$ ab.

Da $s$ hermitesch, ist $s(v_i,v_j) = \_{s(v_j,v_i)}$ und $M(s) = \_{M(s)}^T$ ist damit \emph{hermitesch}.


\begin{df}
	\label{df:13.3}
	Eine Matrix $A\in \Mat(n\times n,\R)$ heißt \emph{symmetrisch} genau dann, wenn
	\[
		A = A^T
	\]
	Eine Matrix $A\in \Mat(n\times n,\C)$ heißt \emph{hermitesch} genau dann, wenn
	\[
		A = \_A^T
	\]
	\begin{note}
		\begin{itemize}
			\item
				Jede symmetrische Matrix auch hermitesch.
			\item
				Für hermitesche Matrizen ist
				\[
					b_{ii} = \_{b_{ii}} \implies b_{ii} \in \R
				\]
		\end{itemize}
	\end{note}
\end{df}


\begin{prop}
	\label{prop:13.4}
	Sei $\dim V=n$ und $v_1,\dotsc, v_n$ eine Basis von $V$.
	Die Abbildung $s \mapsto M(s)$ ist eine bijektive Abbildung von den symmetrischen Bilinearformen zu den symmetrischen Matrizen.

	Die Abbildung $s \mapsto M(s)$ ist eine bijektive Abbildung von den hermiteschen Sesquilinearformen zu den hermiteschen Matrizen.

	\begin{note}
		Die Bijektion hängt von der gewählten Basis ab.
	\end{note}

	\begin{proof}
		Dass $M(s)$ symmetrisch, bzw. hermitesch ist, wurde schon gezeigt.

		Sei $M$ symmetrische, bzw. hermitesche Matrix.
		Definiere
		\[
			s(x,y) := \_x^TMy
		\]
		Dann ist $m_{ij} = s(v_i,v_j)$, bezüglich einer Basis $v_1,\dotsc, v_n$.
		$s(x,y)$ ist symmetrische Bilinearform, bzw. hermitsche Sesquilinearform (siehe Beispiele).

		Es ist $s_1=s_2 \implies M_1 = M_2$ (durch Auswerten an $v_i,v_j$).

		Damit ist die Abbildung bijektiv.
	\end{proof}
\end{prop}


\begin{df}[Länge und Abstand von Vektoren]
	\label{df:13.5}
	Sei $V$ ein Vektorraum über $\C$ oder $\R$.
	Eine \emph{Norm} $\|\cdot\|$ ist eine Abbildung $V\to \R, x\mapsto \|x\|$ für die gilt
	\begin{enumerate}[({N}1)]
		\item
			$\forall x\in V, \lambda \in \{\R,\C\} : \| \lambda x\| = |\lambda | \|x\|\quad$ (Homogenität)
		\item
			$\forall x,y\in V: \|x+y\| \le \|x\| + \|y\|\quad $(Dreiecksungleichung)
		\item
			$\forall x\in V \|x\| \ge 0$ und $\|x\| = 0 \iff x=0\quad $(Positive Definitheit).
	\end{enumerate}
	$(V, \|\cdot\|)$ heißt \emph{normierter} Vektorraum.

	\begin{note}
		Die Norm soll die „Länge“ eines Vektors definieren.
		$\sqrt{s(x,x)}$ soll eine Norm definieren, wenn $s$ ein Skalarprodukt ist.
	\end{note}
\end{df}


\begin{df*}[Metrischer Raum]
	Sei $V$ ein Vektorraum und $d: V\times V \to \R$ eine Abstandsfunktion, die folgenden Axiome erfüllt:
	\begin{enumerate}[({M}1)]
		\item $\forall x,y\in V: d(x,y) = d(y,x)\quad $ (Symmetrie)
		\item $\forall x,y,z\in V: d(x,z) \le d(x,y) + d(y,z)\quad $(Dreiecksungleichung)
		\item $\forall x,y \in V: d(x,y) = 0 \iff x=y \land d(x,y) \ge 0$
	\end{enumerate}
	dann heißt $d$ \emph{Metrik} und $(V,d)$ \emph{metrischer Vektorraum}.
\end{df*}


\begin{thm}
	\label{thm:13.6}
	Sei $V$ ein euklidischer oder unitärere Vektorraum mit Skalarprodukt $\<x,y\> := s(x,y)$.
	Dann definiert
	\[
		\|x\| := \sqrt{\<x,x\>}
	\]
	\begin{enumerate}[(a)]
		\item
			$\forall x,y \in V: |\<x,y\>| \le \|x\|\cdot \|y\|\quad$ (Cauchy-Schwarzsche Ungleichung)\\
			$\<x,y\> = \|x\|\cdot \|y\| \iff x $ und $ y$ sind linear abhängig. 
		\item
			$(V,\|\cdot\|)$ ist ein normierter Vektorraum
		\item
			Für $x,y\in V$ sei $d(x,y) := \|x-y\|$ der \emph{Abstand} (Distanz).
			Dann ist $(V,d)$ ein metrischer Raum mit Metrik $d$.
		\item
			$\forall x,y \in V: \|x+y\|^2 = \|x\|^2 +\|y\|^2 + \<x,y\> + \<y,x\>\quad $ (Satz des Pythagoras, Kosinussatz)
		\item
			$\forall x,y \in V: \|x+y\|^2 + \|x-y\|^2 = 2(\|x\|^2 + \|y\|^2)\quad $ (Parallelogrammgleichung)
	\end{enumerate}

	\begin{proof}
		\begin{enumerate}[(a)]
			\item
				Sei $x,y\in V$.
				Für $x=0 \lor y=0$ trivial, sei also $y\neq 0$, dann ist $\<y,y\> > 0 \implies \|y\|>0$.
				Für $\lambda\in \R$ oder $\lambda\in \C$:
				\begin{align*}
					0&\le \<x-\lambda y,x-\lambda y\> \\
					&= \<x,x\> - \lambda \<x,y\> - \_\lambda \<y,x\> + \lambda\_\lambda\<y,y\>
				\end{align*}
				Wähle $\lambda := \f {\_{\<x,y\>}}{\_{\<y,y\>}}$, dann ist
				\begin{align*}
					0&\le \<x,x\> - \f {\_{\<x,y\>}}{\_{\<y,y\>}} \<x,y\> - \f {\<x,y\>}{\_{\<y,y\>}}\<y,x\> + \f{\_{\<x,y\>}\<x,y\>}{\<y,y\>\<y,y\>} \<y,y\> \\
					0&\le \<x,x\>\<y,y\> - \_{\<x,y\>}\<x,y\> - \<x,y\>\_{\<x,y\>} + \_{\<x,y\>}\<x,y\> \\
					\implies 0 &\le \<x,x\>\<y,y\> - |\<x,y\>|^2\\
					\implies \|x\|^2\cdot \|y\|^2 &\ge |\<x,y\>|^2\\
					\implies \|x\|\cdot \|y\| &\ge |\<x,y\>|
				\end{align*}
				Gleichheit herscht genau dann, wenn
				\[
					\exists \lambda\in K : 0= \<x-\lambda y, x-\lambda y\> \exists \lambda\in K :\iff x-\lambda y = 0
				\]
				und damit müssen $x$ und $y$ linear abhängig sein.
			\item
				Es gelten die Axiome:
				\begin{enumerate}[({N}1)]
					\item $\<\lambda  x, \lambda x\> = \_{\lambda}\lambda \<x,x\> = |\lambda|^2\<x,x\>$
					\item
						Wir zeigen $\|x+y\|^2 \le (\|x\| + \|y\|)^2$:
						\begin{align*}
							\<x+y,x+y\> &= \<x,x\> + \<y,y\> + \<x,y> + \_{\<x,y\>}\\
										&= \|x\|^2 + \|y\|^2 + 2\Re \<x,y\>\\
							   &\le \|x\|^2 + \|y\|^2 + 2|\<x,y\>|\\
							   &\stackrel{\text{(a)}}\le  \<x,y\> + \<y,y\> + 2\|x\|\cdot \|y\| \\
							   &= (\|x\| + \|y\|)^2
						\end{align*}
					\item
						\[
							\|x\| := \sqrt{\<x,x\>} \ge 0 \land (\|x\|=0 \iff x=0)
						\]
				\end{enumerate}
			\item
				Wir zeigen allgemeiner: für normierte Vektorräume definiert
				\[
					d(x,y) := \|x-y\|
				\]
				immer eine Metrik auf $V$.

				\begin{enumerate}[({M}1)]
					\item
						$
							d(x,z) = \|x-z\| = \|(x-y)+(y-z)\| \stackrel{\text{(N2)}}\le \|x-y\| + \|y-z\| = d(x,y) + d(y,z)
						$
					\item
						$
							d(x,y) = \|x-y\| = \|(-1)(y-x)\| = |-1|\|y-x\| = d(y,x)
						$
					\item
						Es gilt $d(x,y) \ge 0$ für alle $x,y$ und
						\[
							d(x,y) = 0 \iff \|x-y\| = 0 \iff x-y = 0 \iff x=y
						\]
				\end{enumerate}
			\item
				$
					\|x+y\|^2 = \<x+y,x+y\> = \<x,x\> + \<y,y\> + \<x,y\> + \<y,x\> = \|x\|^2 + \|y\|^2 + \<x,y\> + \<y,x\>
				$				
			\item
				\begin{align*}
					\<x+y,x+y\> + \<x-y,x-y\> &= \<x,x\> + \<y,y\> + \<x,y\> + \<y,x\> + \<x,x\> + \<y,y\> - \<x,y\> -\<y,x\>\\
											  &= 2\<x,x\> + 2\<y,y\>
				\end{align*}
		\end{enumerate}
	\end{proof}
\end{thm}

\begin{df}[Orthogonalität]
	\label{df:13.7}
	Sei $V$ ein euklidischer oder unitärer Vektorraum.
	Zwei Vektoren $x,y\in V$ heißen zueinander \emph{orthogonal} genau dann, wenn
	\[
		\<x,y\> = 0
	\]
	Wir schreiben dann $x \orth y$.

	Zwei Unterräume $U,W$ von $V$ heißen zueinander \emph{orthogonal} genau dann, wenn
	\[
		\forall x\in U, y\in W : x \orth y
	\]
	Wir schreiben dann $U \orth W$.

	Sei $U<V$ ein Unterraum.
	Dann ist das \emph{orthogonale Komplement} $U^\orth$ von $U$ definiert als
	\[
		U^\orth = \{ x\in V \big| \forall y\in U: x\orth y\}
	\]

	Seien $x_i$ Vektoren in $V$ mit $i\in I$.
	Die $x_i$ heißen \emph{orthogonal} genau dann, wenn
	\[
		\forall i,j\in I, i\neq j : x_i \orth x_j
	\]
	Die $x_i$ heißen \emph{orthonormal} genau dann, wenn zusätzlich gilt
	\[
		\forall i\in I: \|x_i\| = 1
	\]

	Eine \emph{Orthonormalbasis} von $V$ ist eine Basis, die aus orthonormalen Vektoren besteht.
\end{df}

\begin{note}
	Wie hängt $U^\orth$ zusammen mit $U^0$?
	Keine Gleichheit, aber es herscht ein Isomorphismus zwischen beiden Räumen.
\end{note}

\begin{st*}
	$U^\orth$ ist ein Unterraum von $V$
	\begin{proof}
		Für alle $x,y\in U^\orth, \lambda \in K$ gilt
		\begin{align*}
			\<x+y, u\> &= \<x,u\> + \<y,u\> = 0\\
			\<\lambda x, u\> = \_\lambda \<x,u\> = 0
		\end{align*}
	\end{proof}	
\end{st*}

\begin{ex}
	Sei $V=\R^n$ oder $V=\C^n$ mit Skalarprodukt
	\[
		\<x,y\> = x^Ty \text{ bzw. }\<x,y\>=\_{x}^Ty
	\]
	Dann ist die Standardbasis eine Orthonormalbasis, da $\<e_i, e_j\> = 0$ für alle $i\neq j$.
\end{ex}

\begin{lem}
	\label{lem:13.8}
	Seien $v_i \neq 0$ ($i\in I$) orthogonal und sei 
	\[
		c_i := \f 1{\|v_i\|}
	\]
	Dann gilt
	\begin{enumerate}[(a)]
		\item
			Die Vektoren $c_i v_i$ sind orthonormal
		\item
			Die Vektoren $v_i$ sind linear unabhängig.
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[(a)]
			\item
				Orthogonalität:
				\begin{align*}
					i\neq j: &\qquad \<c_i v_i, c_j v_j\> = c_ic_j\< v_i,v_j\> = 0\\
					i = j: &\qquad \<c_i v_i, c_j v_j\> = c_ic_j\< v_i,v_j\> = \f 1{\|v_i\|}\cdot \f 1{\|v_i\|} \<v_i,v_i\> = 1
				\end{align*}
			\item
				Sei $0= \sum_{k=1}\lambda_k v_{i_k}$ für paarweise verschiedene $i_k\in I$.
				Es gilt für beliebiges $d \in \{1,\dotsc, n\}$.
				\[
					0 = \<v_{i_d}, 0\> = \sum_{k=1}^n\<v_{i_k},\lambda_k v_{i_k}\> = \lambda_d \|v_{i_d}\|^2 \implies \lambda_d = 0
				\]
				\begin{note}
					Das Skalarprodukt mit $v_{i_j}$ liefert den Koeffizienten $\lambda_j$.
					Die Koeffizienten haben eine geometrischen Bedeutung.
				\end{note}
		\end{enumerate}
	\end{proof}
\end{lem}

\begin{ex}
	Sei $V=\R^2$, $u,u',v\in V$ mit $\|u\|=1=\|u'\|$ und $u\orth u'$.
	Dann kann $v$ geschrieben werden als
	\[
		v = au + bu' \qquad a,b\in K
	\]
	der Koeffizient $a$ ergibt sich durch
	\[
		a = \<u,v\>
	\]
	und
	\[
		\<u',v\> = \<u',au+bu'\> = \<u',au\> + \<u',bu'\> = b
	\]
\end{ex}

\begin{ex}
	Sei $V$ euklidischer oder unitärer Vektorraum und $u,v\in V$ mit $\|u\| = 1$.

	Gesucht ist $u'$ mit $u\orth u'$ und $v = au + bu'$.
	Setze
	\[
		v = \<u,v\>u + (v-\<u,v\> u) := \<u,v\> + u'
	\]
	Wir zeigen $u'\orth u$.
	\begin{align*}
		\<u, u'\> &= \<u,v\> - \<u,\<u,v\>u\>\\
				  &= \<u,v\> -\<u,v\>\<u,u\>\\
			&= 0
	\end{align*}
	Also ist $u\orth u'$.

	Diese Zerlegung ist für festes $u$ eindeutig ($u'$ ist also bis auf seine Länge eindeutig bestimmt).
	\begin{proof}
		Sei $v= u_1 + u_2, u_1 = \lambda u, \<u,u_2\> = 0$, zeige $\lambda = \<u,v\>$:
		\begin{align*}
			\<u,v\> &= \<u,\lambda u\> + \<u,u_2\>\\
					&= \lambda \<u,u\> \\
			  &= \lambda
		\end{align*}
		Also muss
		\[
			u_2 = v - u_1 = v - \<u,v\> u
		\]
	\end{proof}
\end{ex}

\begin{prop}
	\label{prop:13.9}
	Seien $V$ ein euklidischer oder unitärer Vektorraum, $u_1,\dotsc, u_l$ in $V$ orthonormal und $v\in V$.
	Dann existiert eine eindeutige Zerlegung von $v$
	\[
		v = \sum_{k=1}^l \lambda_ku_k + v^\orth
	\]
	mit $v^\orth \orth u_i$ für $i=1,\dotsc, l$.
	Außerdem gilt
	\[
		\lambda_i = \<u_i, v\>
	\]
	\begin{proof}
		\begin{seg}{Eindeutigkeit}
			Sei $v=\sum_{i=1}^n\lambda_iu_i + v^\orth$ mit $\<u_i,v^\orth\>=0$.
			Sei $i$ fest.
			\begin{align*}
				\<u_i,v\> &= \<u_i,\sum_{k=1}^n\lambda_k u_k + v^\orth\>\\
						  &=\sum_{k=1}\lambda_k\<u_i,u_k\> +\<u_i,v^\orth\>\\
						  &=\lambda_i\<u_i,u_i\> = \lambda_i
			\end{align*}
			Außerdem steht $v^\orth$ auch fest:
			\[
				v^\orth = v - \sum_{k=1}^n\lambda_ku_k
			\]
		\end{seg}
		\begin{seg}{Existenz}
			Wir müssen (wie eben gezeigt) $\lambda_i=\<u_i,v\>$ und $v^\orth = v-\sum_{k=1}^n\<u_k,v\>u_k$ wählen.
			Zeige: $v^\orth$ ist orthogonal zu allen $u_i$.
			\begin{align*}
				\<u_i,v^\orth\> &= \<u_i,v\> -\sum_{k=1}^n \<u_i,\<u_k,v\>u_i\>\\
								&=\<u_i,v\> -\sum_{k=1}^n \<u_i,v\>\cdot\<u_i,u_k\>\\
					&=\<u_i,v\> - \<u_i,v\>\\
				 &=0
			\end{align*}
		\end{seg}
	\end{proof}
\end{prop}

\begin{kor}[Besselsche Ungleichung]
	\label{kor:13.10}
	Seien $u_1,\dotsc,u_l$ orthonormal und $v\in V$ beliebig.
	Dann gilt
	\[
		\sum_{i=1}^l |\<u_i,v\>|^2 \le \|v\|^2
	\]
	Die Gleichheit gilt genau dann, wenn $v = \sum_{i=1}^l\<u_i,v\>u_i \in \<u_1,\dotsc,u_l\>$ (d.h. $v^\orth=0$).
	\begin{proof}
		\begin{align*}
			\|v\|^2 &= \<v,v\>\\
					&=\left\<\sum_{i=1}^l\<u_i,v\>u_i + v^\orth, \sum_{i=1}^l\<u_i,v\>u_i + v^\orth\right\>\\
			&=\sum_{i=1}^l\sum_{j=1}^l\_{\<u_i,v\>}\<u_j,v\>\<u_i,u_j\>   +   \left\<\sum_{i=1}^l\<u_i,v\>u_i,v^\orth\right\> + \left\<v^\orth,\sum_{i=1}^l\<u_i,v\>u_i\right\> + \<v^\orth,v^\orth\>\\
			&=\sum_{i=1}^l\_{\<u_i,v\>}\<u_i,v\> + \<v^\orth,v^\orth\>\\
			&\ge \sum_{i=1}^l |\<u_i,v\>|^2
		\end{align*}
		Die Gleichheit ist dabei erfüllt, wenn $\<v^\orth,v^\orth\>=\|v^\orth\|^2=0 \iff v^\orth=0$.
		Dann ist nach \ref{prop:13.9} aber auch
		\[
			v=\sum_{i=1}^l \<u_i,v\> u_i \iff v\in\<u_1,\dotsc,u_l\>
		\]
	\end{proof}
	\begin{note}
		Damit können wir testen, ob $v\in\<u_1,\dotsc,u_l\>$ oder wie gut $\sum_{i=1}^l\<u_i,v\>u_i$ den Vektor $v$ approximiert.
		Das wird für viele Approximationen benutzt.
	\end{note}
\end{kor}

\begin{ex}
	Sei $V$ der Vektorraum der stetigen Funktionien auf $[0,2\pi]$ nach $\R$ mit Skalarprodukt
	\[
		\<f,g\> := \int_0^{2\pi}f(x)g(x)dx
	\]
	Wir definieren uns orthonormale Vektoren: \fixme \texttt{Warum sind sie orthonormal?}
	\[
		\int_0^{2\pi}\cos (kx) \sin (lx)dx = 0 \qquad \forall k,l\in \N
	\]
	Damit ist $f(x):=\cos kx \orth g(x):=\sin lx$.
	Außerdem:
	\begin{align*}
		\int_0^{2\pi}\cos(kx)\cos(lx) dx = 0 \qquad \forall k\neq l\\
		\int_0^{2\pi}\sin(kx)\sin(lx) dx = 0 \qquad \forall k\neq l
	\end{align*}
	Die Funktionen (=Vektoren) $\cos kx$ und $\sin lx$ sind paarweise orthogonal.
	Für die Länge der Vektoren gilt:
	\[
		\int_0^{2\pi}(\cos kx)^2 dx = \pi = \int_0^{2\pi}(\sin lx)^2dx 
	\]
	Also sind $\f 1{\sqrt\pi} \cos kx$ und $\f 1{\sqrt\pi}\sin lx$ orthonormale Vektoren.

	Die \emph{Fourier-Analysis} versucht $f:[0,2\pi] \to \R$ zu approximieren durch
	\[
		\sum_{k=0}^\infty a_k\cdot \frac 1{\sqrt\pi}\cos kx + \sum_{l=1}^\infty \frac 1{\sqrt\pi}\sin lx
	\]
	\ref{prop:13.9} besagt, dass wir $a_k$ und $b_k$ wie folgt wählen müssen:
	\begin{align*}
		a_k = \left\<\f1{\sqrt\pi}\cos kx,f\right\> = \int_0^{2\pi}\f 1{\sqrt\pi}\cos (kx)f(x)dx
		b_k = \left\<\f1{\sqrt\pi}\sin kx,f\right\> = \int_0^{2\pi}\f 1{\sqrt\pi}\sin (kx)f(x)dx
	\end{align*}
	Nach \ref{kor:13.10} ist die Approximation optimal genau dann, wenn
	\[
		\sum_{k=0}^\infty a_k^2 + \sum_{l=1}^\infty b_l^2 = \|f\|^2 = \int_0^{2\pi}f(x)^2dx
	\]
	Dann ist
	\[
		\sum_{k=0}^\infty a_k \cos (kx) + \sum_{l=1}^\infty b_l \sin (lx)
	\]
	die „Fourierreihe” für $f$.
\end{ex}


\section{Konstruktion von orthonormalen Vektoren}


\begin{alg*}[Gram-Schmidtsches Orthogonalisierungsverfahren]
	\begin{algorithmic}
		\Input $V$ euklidischer oder unitärer Vektorraum
		\Input Linear unabhängige Vektoren $v_1,\dotsc, v_l \in V$
		\Output Orthonormale Vektoren $w_1,\dotsc, w_l \in V$ mit $\<v_1,\dotsc,v_l\> = \<w_1,\dotsc,w_l\>$.
		\Statex
		\State $\displaystyle w_1 := \frac {v_1}{\|v_1\|}$
		\For {$k=2,\dotsc,n$}
		\State $\displaystyle w_k' := v_k - \sum_{i=1}^{l-1} \<w_i,v_k\>w_i$
			\State $\displaystyle w_k := \frac{w_k'}{\|w_k'\|}$
		\EndFor
	\end{algorithmic}
\end{alg*}

\begin{thm}
	\label{thm:13.11}
	Das Gram-Schmidtsche Orthogonalisierungsverfahren stellt aus linear unabhängigen Vektoren $v_1,\dotsc,v_l$ orthonormale Vektoren $u_1,\dotsc,u_l$ her mit
	\[
		\<v_1,\dotsc,v_l\> = \<u_1,\dotsc,u_l\>
	\]
	\begin{note}
		Insbesondere hat jeder euklidische oder unitäre Vektorraum $V$ mit $\dim V<\infty$ eine Orthonormalbasis.
		Und jede Orthonormalbasis eines Unterraums $U$ kann zu einer Orthonormalbasis von $V$ ergänzt werden. 
		Das Orthonormalisierungsverfahren lässt sich induktiv sogar auf alle abzählbare Basen anwenden, jedoch lassen
		sich die zugehörigen Orthonormalbasen im Allgemeinen nicht konstruieren.
	\end{note}
	\begin{proof}
		Wir zeigen per Induktion nach $n$:
		\begin{seg}{Induktionsanfang ($n=1$)}
			$\<v_1\> = \<u_1\>$ ist erfüllt
		\end{seg}
		\begin{seg}{Induktionsschritt}
			\[
				u_{n+1}' = v_{n+1}-\sum_{k=1}^n\<u_k,v_{n+1}\>u_k
			\]
			Also
			\[
				v_{n+1} = \sum_{k=1}^n \<u_k,v_{n+1}\>u_1 + u_{n+1}'
			\]
			Nach \ref{prop:13.9} ist $u_{n+1}'$ senkrecht zu $u_1,\dotsc,u_n$.

			Wenn $u_{n+1}' = 0$, dann wäre
			\[
				v_{n+1} \in \<u_1,\dotsc,u_n\> = \<v_1,\dotsc,v_n\>
			\]
			was ein Widerspruch zu $v_1,\dotsc,v_{n+1}$ linear unabhängig darstellt.
			Also muss $u_{n+1}' \neq 0$ und $u_{n+1}$ ist wohldefiniert.

			Nach Induktionsannahme ist
			\[
				\<v_1,\dotsc,v_n\> = \<u_1,\dotsc,u_n\>
			\]
			Außerdem ist
			\[
				u_{n+1}\in \<u_1,\dotsc,u_n,v_{n+1}\>  =\<v_1,\dotsc,v_n,v_{n+1}\>
			\]
			Also gilt auch
			\[
				\<u_1,\dotsc,u_{n+1}\> = \<v_1,\dotsc,v_{n+1}\>
			\]
		\end{seg}
	\end{proof}
\end{thm}

\begin{ex}
	Sei $V=\R^3$ mit kanonischen Skalarprodukt und
	\[
	v_1=\begin{pmatrix}1\\0\\0\end{pmatrix},v_2=\begin{pmatrix}1\\1\\0\end{pmatrix}, v_3=\begin{pmatrix}1\\1\\1\end{pmatrix}
	\]
	Dann liefert das Gram-Schmidt-Verfahren:
	\begin{align*}
	u_1 = \f{v_1}{\|v_1\|} &= \begin{pmatrix}1\\0\\0\end{pmatrix}\\
		u_2' := \begin{pmatrix}1\\1\\0\end{pmatrix} - \left\<\begin{pmatrix}1\\0\\0\end{pmatrix},\begin{pmatrix}1\\1\\0\end{pmatrix}\right\> \begin{pmatrix}1\\0\\0\end{pmatrix} - 1\cdot \begin{pmatrix}1\\0\\0\end{pmatrix} &= \begin{pmatrix}0\\1\\0\end{pmatrix}\\
	u_2= \f{u_2'}{\|u_2'\|} &= \begin{pmatrix}0\\1\\0\end{pmatrix}\\
	u_3' =\begin{pmatrix}1\\1\\1\end{pmatrix}-\left\<\begin{pmatrix}1\\0\\0\end{pmatrix},\begin{pmatrix}1\\1\\0\end{pmatrix}\right\>\begin{pmatrix}1\\0\\0\end{pmatrix} - \left\<\begin{pmatrix}0\\1\\0\end{pmatrix},\begin{pmatrix}1\\1\\1\end{pmatrix}\right\>\begin{pmatrix}0\\1\\0\end{pmatrix} &= \begin{pmatrix}0\\0\\1\end{pmatrix} =: u_3
	\end{align*}
\end{ex}

\begin{note}
	Die Reihenfolge der Eingangsvektoren beeinflusst selbstverständlich die Ausgabe des Gram-Schmidtverfahrens.
\end{note}

\begin{df}
	\label{df:13.12}
	Sei $M\subset V$ eine Teilmenge.
	Dann definieren wir
	\[
		M^\orth = \{v\in V \big| \forall m\in M \<v,m\>=0\}
	\]
	$M^\orth$ heißt der \emph{Orthogonalraum} der Menge $M$.
\end{df}

\begin{st*}
	$M^\orth$ ist ein Unterraum.

	\begin{proof}
		$0\in M^\orth$, denn $\<0,m\>=0$ für alle $m$.
		Außerdem
		\begin{align*}
			\<m,v_1,+v_2\> = \<m,v_1\> + \<m,v_2\> = 0 &\implies v_1+v_2 \in M^\orth\\
			\<m,\lambda  v_1\> = \lambda \<m,v_1\> = 0 &\implies \lambda _1 \in M^\orth
		\end{align*}
	\end{proof}
\end{st*}

\begin{prop}
	\label{prop:13.13}
	Sei $\dim V < \infty$ und $U < V$ ein Unterraum von $V$.
	Dann ist 
	\[
		V = U \oplus U^\orth
	\]
	$U^\orth$ ist das orthogonale Komplement von $U$.
	Jeder Vektor $v\in V$ hat also eine eindeutige Zerlegung
	\[
		v = v_1 + v_2 \qquad v_1\in U, v_2\in U^\orth
	\]
	Außerdem gilt
	\[
		\dim V = \dim U + \dim U^\orth
	\]
	und
	\[
		(U^\orth)^\orth = U
	\]
	\begin{proof}
		Sei $\dim V =: n$ und $\dim U =: l \le n$.
		$U$ hat eine Orthonormalbasis $u_1,\dotsc, u_l$.
		Diese ergänzen wir zu einer Orthonormalbasis $u_1,\dotsc,u_l,u_{l+1},\dotsc,u_n$ von $V$.
		Dann sind $u_{l+1},\dotsc,u_n\in U^\orth$.

		Da $u_1,\dotsc,u_l,u_{l+1},\dotsc,u_n$ eine Basis von $V$ ist, gilt offensichtlich $V=U+U^\orth$.
		Sei $v\in U\cap U^\orth$, dann ist $\<v,v\> = 0$ und damit $v=0$.
		Damit ist $V=U\oplus U^\orth$.
		Daraus folgt auch direkt die Dimensionformel.

		Es gilt $U\subset (U^\orth)^\orth$, denn $\<u,v\>=0$ für alle $u\in U, v\in U^\orth$.
		Mit der Dimensionsformel gilt $U=(U^\orth)^\orth$.
	\end{proof}
\end{prop}
\begin{note}
 Insbesondere lässt sich eine Basis von $U^\orth$ konstruieren durch eine Basisergänzung von U mit darauffolgender 
Anwendung des Orthonormalisierungsverfahren. Hierbei sind zunächst die Basisvektoren von U zu berücksichtigen.
\end{note}


\section{Vergleich von $U^\orth$ und $U^0$}

Es gilt
\begin{align*}
	U^\orth &= \{v\in V \big| \forall u\in U : \<v,u\> = 0\}\\
	U^0 &= \{\phi \in V^* = \Hom(V,K) \big|\forall u\in U :\phi(u) = 0\}
\end{align*}

Sei $\<-,-\>$ ein Skalarprodukt auf $V$.
Das Skalarprodukt ist wegen seiner Bilinearität insbesondere linear in der zweiten Variablen.
Sei $v\in V$, dann definiert
\[
	\<v,-\> : V\to K
\]
ein lineares Funktional, also $\<v,-\>\in V^*$.

\begin{thm}[Spezialfall des Darstellungssatzes von Riesz]
	\label{thm:13.14}
	Sei $\dim V<\infty$.
	Zu jedem $\phi\in V^*$ existiert genau ein $v\in V$, sodass
	\[
		\phi(u) = \<v,u\> \qquad \forall u\in V
	\]
	Diese Abbildung $V \ni v\mapsto \<v,-\> \in V^*$ ist ein Isomorphismus $V\isomorph V^*$. 
	Für $W< V$ wird dabei $W^\orth$ auf $W^0$ abgebildet.
	\begin{note}
		Der Isomorphismus ist kanonisch, d.h. hängt \emph{nicht} von der Wahl einer Basis ab
	\end{note}
	\begin{proof}
		Für $v\in V$ ist $\<v,-\>\in V^*$ wohldefiniert.
		\begin{seg}{Injektivität}
			Seien $v_1,v_2\in V$
			\[
				\<v_1,-\> = \<v_2,-\> \implies \<v_1-v_2,u\> = 0  \quad \forall u\in V
			\]
			Dann ist aber auch
			\[
				\<v_1-v_2,v_1-v_2\> = 0 \implies v_1 = v_2
			\]
		\end{seg}
		\begin{seg}{Linearität}
			\begin{align*}
				\<v+v',-\> &= \<v,-\> + \<v',-\>\
			\end{align*}
		\end{seg}
		\begin{seg}{Surjektivität}
			Sei $\phi\in V^*$, zeige $\exists v\in V, \forall u\in V: \phi(u)=\<v,u\>$.
			Für $\phi=0$ wähle $v=0$.
			Sei also $\phi\neq 0$, dann ist $\phi: V\to K$ surjektiv.
			Dann ist $\dim(\ker(\phi)) = n-1$ und $\ker(\phi)^\orth$ hat Dimension 1.
			Also existiert eine Basis $v'\in V$ von $\ker(\phi)^\orth$ (also mit $\phi(v')\neq 0$).
			Setze
			\[
				v := \_{\f{\phi(v')}{\<v',v'\>}}
			\]
			Sei $u\in V$, zerlege $u=u_1+u_2$ mit $u_1\in \ker(\phi), u_2\in \ker(\phi)^\orth$.
			Falls $\phi(u) = 0$, dann ist $u=u_1$ und $\<v,u\>=0$.
			Sei also $\phi(u)\neq 0$ und schreibe: $\phi(u) = \my \phi(v)$ mit $\my\neq 0$.
			Dann ist
			\[
				\phi(u-\my v) = \phi(u) - \my\phi(v) = 0
			\]
			Also ist $u-\my v\in \ker\phi$ und $\my = \f{\phi(u)}{\phi(v)}$.
			Dann ist die Zerlegung gegeben durch
			\[
				u = (u-\my v) + (\my v) =: u_1 + u_2
			\]
			Es gilt
			\begin{align*}
				\<v,u\> = \<v,u_1\> + \<v,u_2\> = \<v,u_2\> = \left\<v,\f{\phi(u)}{\phi(i)}v\right\> = \f{\phi(u)}{\phi(v)}\<v,v\> = \phi(u)
			\end{align*}
		\end{seg}
		\begin{seg}{$W^\orth \mapsto W^0$}
			Sei $u\in W^\orth$ dann ist für alle $w\in W$
			\[
				\<u,w\> = 0 \implies \<u,-\> \in W^0
			\]
			Sei $\psi\in W^0$, dann existiert $u\in V$ mit $\psi=\<u,-\>$.
			Es gilt für alle $w\in W$
			\[
				0 = \psi(w) = \<u,w\> \implies u\in W^\orth
			\]			
		\end{seg}
	\end{proof}
\end{thm}

\begin{note}
	Über $\R$ ist dieser Isomorphismus in Ordnung.
	Aber die Linearität bezüglich der Skalarmultiplikation in $\C$ ist nicht sofort gegeben:
	\[
		\lambda v \mapsto \<\lambda v, -\> = \_{\lambda}\phi
	\]
	Im Bild muss man also komplex konjugieren.
\end{note}


\section{Längen- und Winkelerhaltende Abbildungen}


\begin{df}
	\label{df:13.15}
	Sei $V$ euklidischer (oder unitärer) Vektorraum und $\phi$ ein Endomorphismus über $V$.
	$\phi$ heißt \emph{orthogonal} (bzw. \emph{unitär}), wenn gilt
	\[
		\forall x,y\in V : \<\phi(x), \phi(y)\> = \<x,y\>
	\]
\end{df}

\begin{lem}
	\label{lem:13.16}
	Sei $\phi$ orthogonal (oder unitär).
	Dann gilt
	\begin{enumerate}[(a)]
		\item
			$\forall x\in V: \|x\| = \|\phi(x)\|$, d.h. $\phi$ erhält die Länge von Vektoren.
		\item
			$\forall \lambda \text{ Eigenwert von }\phi : |\lambda| = 1$
		\item
			$\forall x,y\in V:x\orth y \iff \phi(x) \orth \phi(y)$
		\item
			$\phi$ ist injektiv.
			Falls $\dim V<\infty$ ist $\phi$ sogar Isomorphismus und $\phi^{-1}$ ist auch orthogonal (bzw. unitär).
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[(a)]
			\item
				$\|x\|^2 = \<x,x\> = \<\phi(x),\phi(x)\> = \|\phi(x)\|^2$
			\item
				Sei $x$ Eigenvektor zum Eigenwert $\lambda$, dann gilt nach (a):
				\[
					\|x\| = \|\phi(x)\| = \|\lambda x\| = |\lambda|\cdot \|x\|
				\]
				Also $|\lambda| = 1$.
			\item
				$x\orth y \iff 0=\<x,y\> \iff 0=\<\phi(x),\phi(y)\> \iff \phi(x) \orth \phi(y)$
			\item
				Bei lineare Abbildungen genügt es zu zeigen, dass $\ker\phi = 0$.
				\[
					x\neq 0 \implies \|x\| > 0 \implies \|\phi(x)\| = \|x\| > 0 \implies \phi(x) \neq 0
				\]
				Wenn $\dim V$ endlich ist, dann folgt aus der Dimensionsformel $\dim \im(\phi) = n$ und damit $\phi$ surjektiv.
				Also existiert in dem Fall $\phi^{-1}$.
				$\phi^{-1}$ ist auch orthogonal (bzw. unitär):
				\begin{align*}
					\<y_1,y_2\> = \<\phi(x_1),\phi(x_2)\> = \<x_1,x_2\> = \<\phi^{-1}(y_1),\phi^{-1}(y_2)\>
				\end{align*}
		\end{enumerate}
	\end{proof}
\end{lem}


\subsection{Skalarprodukterhaltende Matrizen}


Wegen \ref{lem:13.16} müssen solche Matrizen bezüglich Orthonormalbasen operieren (denn die Orthogonalität muss erhalten bleiben und die Länge der Basisvektoren ebenfalls).
Wir wählen uns also eine Orthonormalbasis von $V$ und beschreiben $\phi$ durch eine Matrix $A$ bezüglich der Orthonormalbasis $v_1,\dotsc,v_n$.

Sei $x=\sum_{i=1}^nx_iv_i$ und $y=\sum_{i=1}^ny_iv_i$.
Da $\<v_i,v_j\> = \delta_{ij}$, gilt
\[
	\<x,y\> = \left\<\sum_{i=1}^nx_iv_i,\sum_{j=1}^ny_jv_j\right\> = \sum_{i,j=1}^n\_{x_i}y_j\<v_i,v_j\> = \sum_{i=1}^n \_{x_i}y_i = \_x^T y
\]
Also ist das Skalarprodukt das Standardskalarprodukt, wenn wir eine orthonormale Basis vorraussetzen.
Für die Matrizen gilt dann
\[
	\_x^Ty = \<x,y\> = \<Ax,Ay\> = \_{Ax}^T Ay = \_x^T(\_A^TA)y
\]
Das ist für alle $x,y$ genau dann der Fall, wenn
\[
	\_A^TA = I
\]
Diese Bedingung, oder äquivalent: $A^{-1}=\_A^T$ charakterisiert orthogonale (bzw. unitäre) Matrizen.
\begin{df}
	\label{df:13.17}
	Eine Matrix $A\in \GL(n,\R)$ heißt \emph{orthogonal} genau dann, wenn
	\[
		A^T = A^{-1}
	\]
	Die Menge der orthogonalen $n\times n$-Matrizen wird mit $O(n)$ bezeichnet und heißt \emph{orthogonale Gruppe}.
	
	Eine orthogonale Matrix $A$ mit
	\[
		\det A = 1
	\]
	heißt \emph{speziell orthogonal} (oder \emph{eigentlich orthogonal}).

	Eine Matrix $B\in \GL(n,\C)$ heißt \emph{unitär} genau dann, wenn
	\[
		\_B^T = B^{-1}
	\]
	Die Menge der unitären $n\times n$-Matrizen wird mit $U(n)$ bezeichnet und heißt \emph{unitäre Gruppe}.
	\begin{proof}
		$O(n)$ ist eine Untergruppe von $\GL(n,\R)$, denn
		\[
			I^{-1} = I^T  \implies I \in O(n)
		\]
		\[
			(AB)^T = B^TA^T = B^{-1}A^{-1}=  (AB)^{-1}
		\]
		Der Beweis für $U(n)$ erfolgt analog.
		Für die spezielle orthogonale Gruppe gilt:
		\[
			\det(AB) = \det(A)\cdot \det(B) = 1\cdot 1 = 1
		\]
	\end{proof}
\end{df}

\begin{prop}
	\label{prop:13.18}
	Sei $V = \R^n$ (oder $\C^n$) mit dem kanonischen Skalarprodukt.
	Sei $A$ eine $n\times n$-Matrix.
	Dann sind äquivalent:
	\begin{enumerate}[(a)]
		\item
			$A$ ist orthogonal (bzw. unitär)
		\item
			Die Spalten von $A$ bilden eine Orthonormalbasis
		\item
			Die Zeilen von $A$ bilden eine Orthonormalbasis
	\end{enumerate}
	\begin{proof}~
		\begin{seg}{„$a \iff b$“}
			Die Spalten von $A$ sind die Bilder der Basisvektoren.
			Es gilt
			\[
				\<Ae_i,Ae_j\> = \_{Ae_i}^TAe_j = e_i^T\_A^TAe_j = (\_A^TA)_{ij} = \delta_{ij} \iff \_A^TA = I
			\]
		\end{seg}
		\begin{seg}{„$a \iff c$“}
			Analog wie oben, bloß mit $A^T$.
		\end{seg}
	\end{proof}
\end{prop}

\begin{prop}
	\label{prop:13.19}
	Sei $V$ endlichdimensional euklidisch oder unitär und $\phi:V\to V$ ein Endomorphismus.
	Sei $M(\phi)$ die Matrix zu $\phi$ bezüglich einer Orthonormalbasis.
	Dann gilt
	\[
		\phi \text{ orthogonal (unitär) } \iff M(\phi) \text{ orthogonal (unitär) }
	\]
	\begin{proof}
		Sei $e_1,\dotsc, e_n$ die Orthonormalbasis, dann ist $\<x,y\>=\_x^Ty$.
		Es gilt analog wie im Beweis der letzten Proposition:
		\begin{align*}
			\<x,y\> = \<\phi(x),\phi(y)\> = \<Ae_i,Ae_j\> = \_{Ae_i}^TAe_j = e_i^T\_A^TAe_j = (\_A^TA)_{ij} = \delta_{ij} \iff \_A^TA = I
		\end{align*}
	\end{proof}
\end{prop}

\subsection{Normalformen für orthogonale, bzw. unitäre Matrizen}

\begin{thm}
	\label{thm:13.20}
	Sei $V$ unitär mit $\dim V<\infty$ und $\phi:V\to V$ ein unitärer Endomorphismus.
	Dann existiert eine Orthonormalbasis von $V$, die aus Eigenvektoren von $\phi$ besteht.
	\begin{note}
		Mit anderen Worten: $\phi$ ist diagonalisierbar bezüglich einer Basis aus orthonormalen Eigenektoren.
	\end{note}
	\begin{proof}
		Sei $\chi_\phi$ das charakteristische Polynom von $\phi$.
		Da $K=\C$ zerfällt $\chi_\phi$ in Linearfaktoren.
		Wir beweisen per Induktion nach $n = \dim V$.

		Der Induktionsanfang für $n=1$ ist trivial (wähle irgendeinen Basisvektor und normiere ihn).
		
		\begin{seg}{Induktionsschritt}
			Die Behauptung gelte für $\dim V=n$, zeige jetzt für $\dim V=n+1$.

			Wähle einen normierten Eigenvektor $v'$ zum Eigenwert $\lambda'$ von $\phi$.
			Sei $W := \<v'\>^\orth$.
			Zerlege jetzt $V$ in
			\[
				V = W \oplus W^\orth = W \oplus \<v'\>
			\]
			Es ist $\dim W = n$.
			Zeige, dass $\phi\big|_W$ ein Endomorphismus auf $W$ ist, damit die Induktionsvorraussetzung auf $W$ angewandt werden kann:

			\begin{seg}{$\phi(W) \subset W$}
				Es ist $\lambda'\neq 0$, da $\phi$ unitär und damit insbesondere invertierbar.
				Also sind wegen
				\[
					\lambda' \<\phi(w),v'\> = \<\phi(w),\lambda'v'\> = \<\phi(w),\phi(v')\> = \<w,v'\> = 0
				\]
				die Vektoren $\phi(w)$ und $v'$ orthogonal zueinander und damit $\phi(w)\in W$				
			\end{seg}
			Wegen
			\[
				\<w_1,w_2\> = \<\phi(w_1),\phi(w_2)\> = \<\phi\big|_W(w_1),\phi\big|_W(w_2)\>
			\]
			ist offensichtlich auch $\phi|_W$ unitär.

			Damit können wir die Induktionsvorraussetzung auf $W$ und $\phi|_W$ anwenden und erhalten eine orthonormale Basis aus Eigenvektoren $v_1,\dotsc,v_n$ von $\phi|_W$ also auch von $\phi$ und schließlich ist
			\[
				v_1,\dotsc, v_n, v'
			\]
			eine Orthonormalbasis aus Eigenvektoren von $\phi$.
		\end{seg}
	\end{proof}
	\begin{note}
		Der Beweis funktioniert auch für orthogonale Matrizen, deren charakteristisches Polynom ein Produkt von Linearfaktoren ist.
	\end{note}
\end{thm}


\begin{kor}
	\label{kor:13.21}
	Sei $A\in U(n)$.
	Dann existiert $S\in U(n)$, sodass
	\[
	\_S^TAS = \begin{pmatrix}\lambda_1 & \cdots & 0\\\vdots& \ddots & \vdots\\0&\cdots & \lambda_n \end{pmatrix} \qquad \lambda_i \in \C, |\lambda_i| = 1
	\]
	
	Seien $\my_1,\dotsc,\my_l$ die paarweise verschiedenen Eigenwerte.
	Dann zerfällt $V$ in die direkte Summe
	\[
		V = \bigoplus_{i=1}^l \Eig(\my_i)
	\]
	von paarweise orthogonalen Eigenräumen.
	\begin{proof}
		$A$ hat bezüglich der Basis aus \ref{thm:13.20} Diagonalform.
		Die neuen Basisvektoren tauchen in der Basiswechselmatrix als Spalten auf, also ist $S$ nach \ref{prop:13.18} unitär und deshalb $S^{-1}=\_S^T$.

		Sei $x$ Eigenvektor zu $\my_i$ und $y$ Eigenvektor zu $\my_j$ mit $i\neq j$, zeige: $x\orth y$
		\[
			\<x,y\> = \<\phi(x),\phi(y)\> = \<\my_i x,\my_j y\> = \_{\my_i}\my_j\<x,y\>
		\]
		Angenommen $\<x,y\> \neq 0$, dann ist $\_{\my_i}\my_j = 1$  und dann
		\[
			\my_i = \my_j(\_{\my_i}\my_i) = \my_j|\my_i|^2 = \my_j
		\]
		Was ein Widerspruch zur annahme $\my_i\neq \my_j$ ist.
		Also $\<x,y\> = 0$.
	\end{proof}
\end{kor}

\begin{ex}
	Sei $V=\R^2$ und $\phi$ eine Drehung.
	Dann muss $\phi$ keine Eigenwerte besitzen, muss also auch nicht diagonalisierbar sein.
\end{ex}

Betrachte $V=\R^2$ und
\[
	A = \begin{pmatrix}a&c\\b&d\end{pmatrix}
\]
$A\in O(2)$ gilt genau dann, wenn $A^TA = I$.
Es ergeben sich die Gleichungen:
\begin{align*}
	a^2 + b^2 &= 1\\
	ac + bd &= 0\\
	c^2 + d^2 &= 1
\end{align*}
Wähle $a:=\cos \alpha, b:=\sin \alpha, c:= \sin \beta, d:= \cos \beta$ (die erste und letzte Gleichunge ist dann stets erfüllt).
Es ergibt sich:
\begin{align*}
	0 = \cos \alpha \cdot \sin \beta + \sin \alpha \cdot \cos \beta = \sin(\alpha + \beta)
\end{align*}
Also $\exists n\in \Z : \alpha + \beta = n\pi$.
$n$ gerade oder ungerade, also entweder
\begin{align*}
	\sin \alpha &= - \sin \beta \land \cos \alpha = \cos \beta\\
	\text{oder} \qquad \sin \alpha &= \sin \beta \land \cos \alpha = -\cos \beta
\end{align*}
Damit erhalten wir folgende zwei Sorten von orthogonalen $2\times 2$-Matrizen:
\begin{align*}
A &= \begin{pmatrix}\cos \alpha & -\sin \alpha \\ \sin \alpha & \cos \alpha \end{pmatrix} \qquad \det A = 1, A \in SO(2), \text{ entspricht einer Drehung} \\
B &= \begin{pmatrix}\cos \alpha & \sin \alpha \\ \sin \alpha & -\cos \alpha \end{pmatrix} \qquad \det A = -1,  \text{ Spiegelung an $\Eig(1)$} 
\end{align*}
Unser Ziel ist es, die Normalform jedes $\phi$ aus einem Diagonalteil (zu den Eigenwerten $-1$ und $1$) und $2\times 2$-Blöcken mit Drehungen der Ebene zusammenzusetzen.

Wir müssen dazu zeigen, dass die irreduziblen Faktoren des charakteristischen Polynoms $\chi_\phi$ nur Grad 1 oder 2 haben.

\begin{prop}
	\label{prop:13.22}
	Sei $f(x)\in \R[x]$ mit $\deg f \ge 1$.
	Dann ist die Primfaktorzerlegung von $f(x)$ von der Form
	\[
		f(x) = f_1(x) \cdot \dotsb \cdot f_l(x)
	\]
	wobei die $f_j(x)$ ($1\le i\le l$) irreduzibel und entweder linear oder quadratisch sind.
	\begin{note}
		Insbesondere bedeutet das, dass für alle irreduziblen $f \in \R[x]$ gilt:
		\[
			\deg f \le 2
		\]
	\end{note}
	\begin{proof}
		In $\C[x]$ ist $f$ ein Produkt von Linearfaktoren.
		Sei $z\in \C$ eine Nullstelle von $f(x) = x^n + a_{n-1}x^{n-1} + \dotsb + a_1x + a_0$.
		Dann gilt
		\[
			f(z) = z^n + \sum_{i=0}^{n-1}a_iz^i = 0 = \_0 = \_z^n + \sum_{i=0}^{n-1}\_{a_i}\_z^i = f(\_z)
		\]
		wegen $a_i\in \R$.
		Also ist $\_z$ auch eine Nullstelle und wir können schreiben:
		\begin{align*}
			f(x) &= \prod_{i}(x-x_i)\cdot \prod_{j}(z-z_j)(z-\_z_j) \qquad x_i\in \R, z_j\in \C
		\end{align*}
		Da
		\[
			(x-z_j)(x-\_z_j) = (x-(a+bi))(x-(a-bi)) = x^2 - 2ax + (a^2+b^2) \in \R
		\]
		besteht also $f(x)$ aus einem Produkt von Polynomen von Grad 1 oder 2.
	\end{proof}
\end{prop}

\begin{thm}
	\label{thm:13.23}
	Sei $V$ endlich dimensional euklidisch und $\phi : V\to V$ ein orthogonaler Endomorphismus.
	Dann existiert eine Orthonormalbasis von $V$, bezüglich der $\phi$ durch eine Matrix folgender Form beschrieben ist:
	\[
		\begin{pmatrix}
			\begin{matrix}1&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&1\end{matrix} & 
												   0&0\\
						0&	  
			\begin{matrix}-1&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&-1\end{matrix} &
			0\\
			0&	  
   0&					
  \begin{matrix}A_1&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&A_k\end{matrix} \\
		\end{pmatrix}
	\]
	mit
	\[
		A_j = \begin{pmatrix}\cos \alpha_j & -\sin \alpha_j \\ \sin \alpha_j & \cos \alpha_j\end{pmatrix} \in \SO(2)
	\]
	\begin{proof}
		Beweise durch Induktion nach $n=\dim V$.
		Der Induktionsanfang ist trivialerweise erfüllt.

		Wir suchen $W < V$ mit $\phi(W) = W$ und $\dim W\le 2$.  
		Sei $f(x) = \chi_\phi(x)$.
		$f(x)$ zerfällt nach \ref{prop:13.22}.

		Falls $\exists f_j: \deg f_j = 1$, dann ist $W$ gegeben durch die lineare Hülle des Eigenvektors $x$ und $\f x{\|x\|}$ wird als Basisvektor gewählt.

		Falls nicht, dann gilt für alle $j$: $\deg f_j = 2$.
		Nach Cayley-Hamilton ist $f(\phi) = 0$.
		Falls $f_1(\phi)(v) = 0$ für ein $v\neq 0$, dann wählen wir
		\[
			W := \<v,\phi(v)\> \qquad \text{$\phi$-invariant, $\dim W =2$}
		\]
		Falls nicht, dann existiert für $v\neq 0$ ein $j$ mit $f_1(\phi)\cdot \dotsb \cdot f_{j-1}(\phi) \neq 0$ und $f_1(\phi)\cdot \dotsb \cdot f_j(\phi) = 0$.
		Wähle $w:= f_1(\phi)(v) \dotsb f_{j-1}(\phi)(v) \neq 0$ und
		\[
			W := \<w,\phi(w)\> \qquad \text{$\phi$-invariant, $\dim W = 2$}
		\]
		Zerlege $V = W \oplus W^\orth$.
		Da $W$ und $W^\orth$ $\phi$-invariant sind, ergibt sich die Blockstruktur und wir können die Induktion fortsetzen.

		\fixme[Die $2\times 2$-Blöcke haben keine Orthonormalbasis, wir müssen erst wieder die Blöcke in Drehungen/Spiegelungen aufteilen wie im Beispiel für $\R^2$]
	\end{proof}
\end{thm}


\section{Normalformen für symmetrische Billinearformen und für hermitesche Sesquilinearformen}


An den Normalformen sollte ablesbar sein, ob die Form positiv definit (also ein Skalarprodukt ist).
Zu einer symmetrischen Billinearform gehört eine symmetrische Matrix und zu einer hermiteschen Sesquilinearform eine hermitesche Matrix.
Es gilt
\begin{align*}
	s(x,y) &= x^T Ay \\
	s(x,y) &= x^T By
\end{align*}
Wähle eine Orthonormalbasis für $V$, dann ergibt sich das Standardskalarprodukt $\<x,y\> =x^Ty$, bzw. $\<x,y\> = \_x^Ty$.
\begin{align*}
	s(x,y) &= \<x,Ay\> \\
	\text{bzw.}\qquad	s(x,y) &= \<x,By\>
\end{align*}
Wegen $A=A^T$ gilt $\<x, Ay\> = s(x,y) = x^TA^Ty = (Ax)^Ty = \<Ax,y\>$, bzw. $\<x,By\> = \<Bx,y\>$.
Sei $\phi:V\to V$ definiert durch $A$, bzw. $B$ (bezüglich der Orthonormalbasis).

Dann gilt für alle $x,y\in V$
\[
	\<\phi(x),y\> = \<x,\phi(y)\>
\]
Wir betrachten jetzt allgemein Abbildungen $\phi$, die diese Bedingung erfüllen.

Unser Ziel ist eine Charakterisierung von Abbildungen, für die $V$ eine Orthonormalbasis aus Eigenvektoren besitzt.

\begin{prop}
	\label{prop:13.24}
	Sei $V$ endlich-dimensional und euklidisch (oder unitär) und $\phi : V\to V$ ein Endomorphismus.
	Dann existiert genau eine lineare Abbildung $\phi^* =: \phi^{\ad} \in \End(V)$, so dass gilt
	\[
		\<\phi(x), y\> = \<x,\phi^*(y)\> \qquad x,y\in V
	\]
	$\phi^* = \phi^{\ad}$ heißt die zu $\phi$ \emph{adjungierte Abbildung}.

	Sei $\phi$ (bezüglich einer Orthonormalbasis) durch eine Matrix $A$ gegeben.
	Dann ist $\phi^*$ bezüglich der selben Orthonormalbasis durch die Matrix $\_A^T$ gegeben.
	$\_A^T$ heißt die zu $A$ \emph{adjungierte Matrix}.
	
	\begin{proof}
		Sei $e_1,\dotsc, e_n$ Orthonormalbasis und $A$ die Matrix zu $\phi$.
		Falls $\phi^*$ existiert, gehört es zu einer Matrix $B$.
		$x=e_i, y=e_j$ mit
		\[
			\_a_{ji} = \left\<\sum_{l=1}^na_{l_i}e_l,e_j\right\> = \<\phi(x),y\> = \<x,\phi^*(y)\> = \left\<e_i,\sum_{h=1}^n b_{h_j}\right\> = b_{ij}
		\]
		also $B = \_A^T$.

		Sei umgekehrt $\phi^*$ definiert durch die Matrix $B=\_A^T$.
		Analog wie oben ergibt sich $\<\phi(x),y\> = \<x,\phi^*(y)\>$.
	\end{proof}
\end{prop}

\begin{ex}
	Für $A=\_A^T$ gilt $\phi^* = \phi$.

	Sei $\phi$ orthogonal oder unitär, also $\<x,y\> = \<\phi(x),\phi(y)\>$ für alle $x,y\in V$.
	Dann gilt
	\[
		\<\phi(x),y\> = \<\phi(x),\phi(\phi^{-1}(y))\> = \<x, \phi^{-1}(y)\> \qquad \forall x,y\in V
	\]
	Also $\phi^* = \phi^{-1}$.
\end{ex}

\begin{thm}
	\label{thm:13.25}
	Sei $V$ endlich-dimensioal unitär, $\phi\in \End(V)$.
	Dann sind folgende Aussagen äquivalent:
	\begin{enumerate}[(a)]
		\item
			$V$ hat eine Orthonormalbasis aus Eigentvektoren von $\phi$
		\item
			$\phi$ kommutiert mit $\phi^{\ad}$
			\[
				\phi \circ \phi^{\ad} = \phi^{\ad} \circ \phi
			\]
	\end{enumerate}
	\begin{proof}
		\begin{seg}{„$(a)\implies (b)$“}
			Sei eine Orthonormalbasis aus Eigenvektoren gegeben.
			\[
				\_A^T, A\cdot \_A^T = \_A^T A
			\]
			Mit der Isomorphie von A und $\phi:=phi_A$ folgt: $\phi\circ \phi^{\ad} = \phi^{\ad}\circ \phi$
		\end{seg}
		\begin{seg}{„$(b)\implies (a)$“}
			Es gilt $\phi \circ \phi^{\ad} = \phi^{\ad}\circ \phi$.
			Induktion nach $\dim V$.
			Über $\C$ hat $\chi_\phi$ eine Nullstelle $\lambda_1$.
			Sei $x_1$ Eigenvektor zu $\lambda_1$, also $\phi(x_1) = \lambda_1 x_1$.
			Es existiert folgende $\phi$-invariante Zerlegung:
			\[
				V = \<x_M\>  \oplus \<x_1\>^\orth
			\]
			Es bleibt zu zeigen, $\phi^{\ad}: \<x_1\> \to \<x_1\>$ und $\phi^{\ad}: \<x_1\>^\orth \to \<x_1\>^\orth$.
			\[
				\phi(x_1) -\lambda_1 x_1 = 0 \implies \<\phi(x_1)-\lambda x_1, \phi(x_1) - \lambda x_1\> = 0
			\]
			Es gilt
			\begin{align*}
				0 &= \<\phi(x_1)-\lambda x_1, \phi(x_1) - \lambda x_1\> \\
				  &= \<\phi(x_1),\phi(x_2)\> - \_\lambda_1 \<x_1,\phi(x_1)\> - \lambda_1 \<\phi(x_1), x_1\> + \_\lambda_1\lambda_1\<x_1,x_1\>\\
				  &= \<x_1,\phi^*(\phi(x_1))\> - \_\lambda_1\_{\<\phi(x_1),x_1\>} - \lambda_1\<x_1,\phi^*(x_1)\> + \_\lambda_1\lambda_1 \<x_1,x_1\>\\
				  &= \_{\<\phi^*(\phi(x_1)),x_1\>} - \_\lambda_1 \<\phi^*(x_1),x_1\> - \lambda_1\<x_1,\phi^*(x_1)\> + \_\lambda_1\lambda_1 \<x_1,x_1\>\\
				  &= \_{\<\phi(\phi^*(x_1),x_1\>} - \_\lambda_1\<\phi^*(x_1),x_1\> - \lambda_1\<x_1,\phi^*(x_1)\> + \_\lambda_1\lambda_1\<x_1,x_1\>\\
				  &= \_{\<\phi^*(x_1),\phi^*(x_1)\>} - \_\lambda_1 \<\phi^*(x_1),x_1\> - \lambda_1 \<x_1,\phi^*(x_1)\> + \_\lambda_1\lambda_1 \<x_1,x_1\>\\
				  &= \<\phi^*(x_1)-\_\lambda_1x_1, \phi^*(x_1)-\_\lambda_1x_1\>
			\end{align*}
			Also $\phi^*(x_1) = \_\lambda_1 x_1$ und $x_1$ ist Eigenvektor von $\phi^*$ zum Eigenwert $\_\lambda_1$.

			Sei $y\in \<x_1\>^\orth$, zeige $\phi(y)\in \<x_1\>^\orth$
			\[
				\<\phi(y),x_1\> = \<y,\phi^*(x_1)\> = \<y,\_\lambda_1x_1\> = \_\lambda_1 \<y,x_1\> = \_\lambda_1 \cdot 0 = 0
			\]
			Also $\phi(y) \in \<x_1\>^\orth$.

			Es gilt
			\[
				\<x_1,\phi^*(y)\> = \<\phi(x_1),y\> = \<\lambda_1x_1,y\> = \_\lambda_1 \<x_1,y\> = 0
			\]
			Also $\phi^*(y) \in \<x_1\>^\orth$.

			Also können wir die Induktion anwenden \fixme[Blockstruktur, Zerlegung in $x_1$, $\<x_1\>^\orth$].
			Nach Induktion ist
			\[
				\phi\Big|_{\<x_1\>^\orth} \quad \text{bzw.} \quad \phi^{\ad}\Big|_{\<x_1\>^\orth}
			\]
			diagonalisierbar, also existiert ein Orthonormalbasis aus gemeinsamen Eigenvektoren.
		\end{seg}
	\end{proof}
\end{thm}

\begin{df}
	\label{df:13.26}
	Eine lineare Abbildung $\phi$ heißt \emph{selbstadjungiert} genau dann, wenn
	\[
		\phi^{\ad} = \phi
	\]
	also: $\<\phi(x),y\> = \<x,\phi(y)\>$
\end{df}

\begin{kor}[Spektralsatz]
	\label{kor:13.27}
	Sei $V$ endlich-dimensional unitär.
	Dann sind selbstadjungierte Abbildungen genau die hermiteschen Matrizen (bezüglich Orthonormalbasis).

	Selbstadjungierte Abbildungen und hermitsche Matrizen sind diagonalisierbar.

	Alle Eigenwerte sind reell.
	\begin{proof}
		Aus dem Beweis von \ref{thm:13.25} folgt, dass $\lambda_1$ Eigenwert zu $\phi$ und $\_\lambda_1$ Eigenwert zu $\phi^{\ad}$ zum selben Eigenvektor, also wegen $\phi=\phi^{\ad}$ auch $\lambda_1 = \_\lambda_1$, also reell.

		\fixme[Rest?]
	\end{proof}
\end{kor}

\begin{kor}
	\label{kor:13.28}
	Sei $V$ endlich-dimensional euklidisch, $\phi\in \End(V)$.
	Dann sind folgenden Aussagen äquivalent:
	\begin{enumerate}[(a)]
		\item
			$V$ hat eine Orthonormalbasis aus Eigenvektoren von $\phi$
		\item
			Alle (komplexen) Nullstellen von $\chi_\phi$ sind reell und es gilt:
			\[
				\phi^*\circ \phi = \phi\circ \phi^*
			\]
	\end{enumerate}
\end{kor}

\begin{kor}
	\label{kor:13.29}
	Sei $V$ endlich-dimensional euklidisch.
	Selbstadjungierte Abbildungen sind bezüglich Orthonormalbasen durch symmetrische Matrizen gegeben.

	Selbstadjungierte Abbildungen und symmetrische Matrizen sind diagonalisierbar (alle Eigenwerte sind reell).
\end{kor}

Hermitesche Sequilinearformen und symmetrische Billinearformen können (bezüglich geeigneter Orthonormalbasis) durch Diagonalmatrizen mit reellen Einträgen beschrieben werden.

$s:V\times V\to \R$ (oder $\C$):
\[
	s(x,y) = \_x^T \begin{pmatrix}\lambda_1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 &\cdots & \lambda_n\end{pmatrix}y
\]
wobei $x,y$ die Koordinaten bezüglich Orthonormalbasen.
\[
	s(x,x) = \sum_{j=1}^n \lambda_j \_x_j x_j = \sum_{j=1}^n \lambda_j |x_j|^2
\]
$s$ ist positiv definit (Skalarprodukt) genau dann, wenn $\lambda_j > 0$ für alle $j$.

\begin{kor}
	\label{kor:13.30}
	Eine symmetrische Bilinearform, oder eine hermitesche Sesquilinearform ist positiv definit geanu dann, wenn alle Eigenwerte positiv sind.
\end{kor}

\begin{alg*}[Verfahren zur Entscheidung, ob $s$ positiv definit ist]
	\begin{algorithmic}
		\State Bestimme $A$, sodass $s(x,x) = \_x^TAy$ (wähle $x,y\in \{e_1,\dotsc,e_n\}$)
		\State Bestimme die Eigenwerte von $A$.
	\end{algorithmic}
\end{alg*}


\chapter{Quadriken}


\begin{df}
	\label{df:14.1}
	Ein affiner Raum über $\R$ bezüglich $V$ heißt \emph{Euklidischer Raum}, wenn $V$ ein euklidischer Vektorraum ist.
\end{df}

Ist $E$ euklidisch, dann ist eine Abstandsfunktion durch
\[
	d(p,q) := \|\vec{pq}\|
\]
definiert.
Damit ist $E$ ein metrischer Raum

Bisher haben wir affine Unterräume als Lösungsmengen inhomogener linearer Gleichungssysteme beschrieben.
Unser Ziel ist es, die Lösungsmengen inhomogener „quadratischer“ Gleichungen zu beschreiben, z.B.:
\[
	x_1^2 + 3x_1x_2 - x_3^2 + x_1x_3 + 4x_1 + 5 = 0
\]

Wir vergleichen das mit den Bilinearformen:
\[
	x^T \begin{pmatrix}a&b\\c&d\end{pmatrix}x = ax_1^2 + (c+b)x_1x_2 + dx_2^2
\]

Die Theorie der Normalformen der Bilinearformen wollen wir auf die quadratische Gleichungen anwenden.
Geometrisch bedeutet das, dass wir ein Koordinatensystem geschickt wählen.

Welche geometrischen Objekte können als Lösungsmenge auftreten?

\begin{ex}
	Sei $E=\R^2$.
	\begin{align*}
		x_1^2 &= -1\\
		x_1^2 &= 0\\
		x_1^2 &= 1
	\end{align*}
	Die zweite Gleichung beschreibt die Gerade $x_1=0$.
	Die dritte Gleichung beschreibt die beiden Geraden $x_1=\pm 1$.
	
	Die folgende Gleichung $x_1^2 - x_2^2 = 0$ beschreibt zwei Geraden $x_1=x_2$ oder $x_1 = -x_2$

	$x_1^2+x_2^=0$ beschreibt den Punkt $(0,0)$.
	
	Die Gleichung
	$
		x_1^2 + x_2^2 = 1
	$
	beschreibt den Einheitskreis.

	Die Gleichung
	$
		x_1^2 + 2x_2^2 = 1
	$
	beschreibt eine Ellipse.
	
	$x_1^2 - x_2 = 0$ beschreibt eine Parabel.

	$x_1^2 - x_2^2 = 1$ beschreibt eine Hyperbel.
\end{ex}

Sei $E$ euklidisch.
Verlange, dass $\vec{p_0p_1}, \dotsc, \vec{p_0p_n}$ eine Orthonormalbasis ist.
Diese Koordinaten heißen \emph{cartesische} Koordinaten (Descartes).

Erlaubte Transformationen: Wechsel von einer Orthonormalbasis zu einer anderen, Verschiebung des Basispunkts.

Wir betrachten „quadratische“ Gleichungen:
\[
	\sum_{i,j=1}^n \alpha_{ij}x_ix_j + \sum_{i=1}^n \beta_ix_i + \gamma = 0 \qquad \alpha_{ij},\beta_i, \gamma \in \R
\]
$x_ix_j = x_jx_i$ haben die Koeffizienten $\alpha_{ij}$ und $\alpha_{ji}$.
Wir ändern das ab in
\[
	a_{ij} := \f{\alpha_{ij} + \alpha_{ji}}2 =: a_{ji}
\]
Damit bleibt die obige Gleichung erhalten, aber wir können die $a_{ij}$ als symmetrische Matrix $A=(a_{ij})_{i,j=1,\dotsc,n}$.
\begin{align*}
	\sum_{i,j=1}^n a_{ij}x_ix_j + 2 \sum_{i=1}^n b_i x_i + c &= 0\\
	x^TAx + 2b^T + c &= 0
\end{align*}

\begin{df}
	\label{df:14.2}
	Die Lösungsmenge $Q\subset E$ einer Gleichung der Form
	\begin{align*}
		\sum_{i,j=1}^n a_{ij}x_ix_j + 2 \sum_{i=1}^n b_i x_i + c &= 0
	\end{align*}
	heißt \emph{Quadrik}, oder \emph{Hyperfläche 2. Ordnung}.
\end{df}

Bei einer orthogonalen Basistransformation lässt sich eine Quadrik wiederrum darstellen als:
\[
	x^T(T^TAT)x + 2b^TTx + c = 0
\]
Mit anderen Worten: Wir können die Matrix $A$ stets in Diagonalform bringen (weil $A$ symmetrisch und diagonalisierbar ist).
Mit
\[
	T^TAT = \begin{pmatrix}\lambda_1 & \cdots & 0\\ \vdots & \ddots & \vdots \\ 0 & \cdots & \lambda_n\end{pmatrix}
\]
folgt für die Quadrik die Gleichung
\[
	\sum_{k=1}^n \lambda_kx_k^2 + 2\sum_{k=1}b_k'x_k + c = 0
\]
bzw, wenn man die Eigenwerte $\lambda_{r+1}=\dotsb=\lambda_n=0$ auflöst und quadratisch ergänzt:
\[
	\sum_{k=1}^r \lambda_k\underbrace{\left(x_k + \f{b_k'}{\lambda_k}\right)^2}_{=: x_k} + 2\sum_{k=r+1}^n b_k'x_{k} + c' = 0
\]
mit
\[
	c' = c - \sum_{k=1}\f{b_k'^2}{\lambda_k}
\]
Die Vektoren des cartesischen Koordinatensystems $\vec{p_0p_k}$ ($k=1,\dotsc,r)$ sind orthogonal zu $\vec{p_0p_{k}}$ ($k=r+1,\dotsc, n$) also auch zu
\[
	y := 2\sum_{k=r+1}^n b_{k}'\vec{p_0p_{k}}
\]
Wir ergänzen $\vec{p_0p_1},\dotsc, \vec{p_0p_r}, \f {y}{\|y\|}$ zu einer Orthonormalbasis.
Dann wird die Gleichung zu
\[
	\sum_{k=1}\lambda_kx_k^2 + ay + c' = 0
\]
Falls $a=0$ und $c' \neq 0$ fällt der lineare Anteil weg und es ergibt sich
\[
	\sum_{k=1}^r a_kx_k^2 
\]
Falls $a\neq 0$
\[
	\sum_{k=1}^r \lambda_kx_k^2 + a\underbrace{\left(x_{r+1} + \f {c'}{a}\right)}_{=:x_{r+1}} = 0
\]
und es ergibt sich mit neuem $x_{r+1}$:
\[
	\sum_{k=1}^r a_kx_k^2 = -x_{r+1}
\]
Falls $a=0=c$ ergibt sich
\[
	\sum_{k=1}^r a_kx_k^2 = 0
\]
Durch Umordnung der Koordinaten erreichen wird, dass $a_1,\dots,a_d > 0$ und $a_{d+1},\dotsc, a_r < 0$.
Wir definieren
\[
	c_i = \begin{cases} \sqrt{\f 1{a_i}} & i \le d\\ \sqrt{\f 1{-a_i}} & i > d\end{cases}
\]
Dann ergibt sich mit $r:= \rg(A) >0$ und $0\le d \le r$:
\[
	\sum_{k=1}^d\f{x_k^2}{c_k^2} - \sum_{k=d+1}^r \f{x_{k}^2}{c_k^2} = \begin{cases}
		1 & r \le n\\
		-x_{r+1} & r+1 \le n \;\land\; r-d \le d\\
		0 & r-d \le d \le r
	\end{cases}
\]

\begin{thm}
	\label{thm:14.3}
	Sei $E$ ein $n$-dimensionaler euklidischer Raum und $Q\subset E$ eine Quadrik.
	Dann existiert ein cartesisches Koordinatensystem ($p_0,\dotsc, p_n$) bezüglich dem $Q$ durch die folgenden Gleichungen beschrieben wird:
	\begin{align*}
	\sum_{k=1}^d\f{x_k^2}{c_k^2} - \sum_{k=d+1}^r \f{x_{k}^2}{c_k^2} = \begin{cases}
		1 & r \le n\\
		-x_{r+1} & r+1 \le n \;\land\; r-d \le d\\
		0 & r-d \le d \le r
	\end{cases}
	\end{align*}
	Die Koordinatenachsen heißen \emph{Hauptachsen} von $Q$.
	Die Transformation auf ein solches System heißt \emph{Hauptachsentransformation}.
	\begin{note}
		Die Matrix $A$ hat stets Diagonalform.
		Im ersten und letzten Fall ist $b=0$. 
		Die Hauptachsen und die Hauptachsentransformation sind nicht eindeutig.
	\end{note}
\end{thm}

\begin{seg}{Fragen}
	\begin{description}
		\item[Ist das eine Normalform der Quadriken?]
			Nein, die Eindeutigkeit ist nicht gewährleistet.
			Beispielsweise gibt es freie Koordinaten, wie die Verschiebung des Basispunkts.
			Folgende beide Gleichungen beschreiben die gleichen Quadriken
			\[
				x_1^2 - x_2^2 = 0 \qquad \iff  \qquad -x_1^2 + x_2^2 = 0
			\]
		\item[Was ist die geometrische Bedeutung]

		\item[Was bedeuten die Parameter $r,d,c_i$?]
	\end{description}
\end{seg}

Wenn man eine andere Orthonormalbasis wählt, wird $A$ durch $S^TAS = S^{-1}AS$ ($S$ orthonormal) ersetzt.
$S^TAS$ hat aber wegen der Ähnlichkeit gleiche Eigenwerte wie $A$ und damit bleiben die Diagonaleinträge gleich (die $c_k$).

\begin{ex}[Quadriken in $E=\R^2$]
	$n=2$
	\begin{enumerate}[(I)]
		\item
			\begin{itemize}
				\item $(r,d)=(1,0)$, $-\f{x_i^2}{c_1^2}=1$ beschreibt die leere Menge

				\item $(r,d)=(1,1)$, $\f{x_1^2}{c_1^2}=1$ beschreibt
					\[
						\{x_1=c_1\} \cup \{x_1=-c_1\}
					\]
				\item $(r,d)=(2,0)$, $-\f{x_1^2}{c_1^2}-\f{x_2^2}{c_2^2}=1$ beschreibt die leere Menge

				\item $(r,d)=(2,1)$, $\f{x_1^2}{c_1^2}-\f{x_2^2}{c_2^2}=1$ beschreibt eine Hyperbel
				\item $(r,d)=(2,2)$, $\f{x_1^2}{c_2^2} + \f{x_2^2}{c_2^2}=1$ Ellipse
			\end{itemize}
		\item
			\begin{itemize}
				\item $(r,d)=(1,1)$, $\f{x_1^2}{c_1^2} = -x_2$ beschreibt eine Parabel
			\end{itemize}
		\item
			\begin{itemize}
				\item $(r,d)=(2,1)$, $\f{x_1^2}{c_1^2}-\f{x_2^2}{c_2^2} = 0 \iff c_2x_1 = \pm c_1x_2$
				\item $(r,d)=(1,1)$, $\f{x_1^2}{c_1^2} = 0$ beschreibt die $x_2$-Achse
					\[
						\{x_1 = 0\}
					\]
				\item $(r,d)=(2,2)$, $\f{x_1^2}{c_1^2,}+ \f{x_2^2}{c_2^2}=0$ beschreibt den Punkt $(0,0)$.
			\end{itemize}
	\end{enumerate}
	\begin{note}
		Alle Quadriken im $\R^2$ sind Kegelschnitte.
	\end{note}
\end{ex}

\begin{ex}[Hyperboloid]
	Sei $c_1=c_2=c_3=1$ und die Quadrik gegeben durch
	\[
		x_1^2 + x_2^2 - x_3^2 = 1
	\]
	Der Hyperboloid entsteht durch Rotation einer Hyperbel $x_1^2 - x_3^2 = 1$.
\end{ex}



\chapter{Projektive Geometrie}


\section{Parallelprojektion}


Sei $U_1$ ein Unterraum von $V$ (z.B. eine Ebene, auf die projiziert wird).
Zerlege
\[
	V = U_1 \oplus U_2
\]
Definiere eine Projektion
\[
	\pi : V \to U_1 : v = u_1 + u_2 \mapsto u_1
\]
$\pi$ ist linear und idempotent.
Insbesondere hat $\pi$ nur Eigenwerte $0$ und $1$ und die Eigenräume sind genau $U_2$ und $U_1$.


\section{Zentralprojektion}


Wähle ein Projektionszentrum $Z$ und eine Projektionsebene $E$.
Ein Punkt $P$ wird dann abgebildet auf
\[
	\{\lambda \vec{PZ} : \lambda\in \R\} \cap E
\]
Die Abbildung ist nicht überall wohldefininiert ($Z$ selbst hat z.B. kein Bild und jeder Punkt auf einer zu $E$ parallelen Ebene durch $Z$).
Wie definieren wir diese Projektion?

Um die Abbildung überall definieren zu können, muss die Zielmenge vergrößert werden.
Statt Punkte im $\R^3$ betrachten wir Richtungen, bzw. Geraden durch $z$.
Die Projektion schickt $P$ auf die Gerade durch $P$ und $Z$, bzw. auf einen Richtungsvektor dieser Geraden.


\subsection{Kugelmodell}


Bilde die Kugeloberfläche $S^22$ (2-Sphäre).
$P$ und $Z$ (oder der \emph{antipodale} Punkt $P'$ und $Z$) bestimmen Gerade $g$ und damit den Bildpunkt auf der Projektionsebene.
Die Punkte auf der oberen Halbkugel entsprechen Richtungen (mit dem Mittelpunkt) und den Punkten auf $E$.
Die Äquatorebene $E'$ parallel zu $E$, geschnitten mit der Sphäre $S$ ergeben Richtungen parallel zu $E$l

Die Punkte auf der offenen oberen Halbkugel entsprechen genau Punkten auf der Projektionsebene $E$.
Die Punkte auf dem Äquator ($S^2\cap E'$ mit $E'\|E$) haben keine Entsprechung auf $E$, diese Punkte entsprechen genau den Richtungen, die bei der Zentralprojektion nicht abgebildet werden können.
Abgeschlossene obere Halbkugel „vervollständigt“ die Ebene.

Damit ist die Regel für die vervollständigte Zentralprojektion für gegebenes $P\neq Z$.
\begin{enumerate}[a.]
	\item
		Gerade $g$ durch $P$ und $Z$
	\item
		Der normierte Richtungsvektor der Geraden $g$ vom Mittelpunkt $M$ von $S^2$ zur oberen Halbkugel und trifft dort den Bildpunkt $Q$.
\end{enumerate}
\fixme[Was ist mit der Projektionsebene?]

Also wird $P$ auf einen Richtungsvektor abgebildet.
Damit wird $\R^3\setminus \{Z\}$ abgebildet.


\subsection{Geradenmodell}


Gegeben $P\neq Z$.
Bilde $P$ auf eine Gerade $g$ durch $P$ und $Z$ ab.

Die Bildmenge ist die Menge aller Geraden im Raum.

\begin{df}
	\label{df:15.1}
	Sei $V$ ein $K$-Vektorraum.
	Die Menge aller eindimensionalen linearen Teilräume von $V$ heißt der zu $V$ gehörende \emph{projektive Raum}.
	\[
		\P = \P(V) 
	\]
	Ist $\dim V=n+1$, so heißt $\dim \P = n$ die \emph{Dimension} von $\P$.
	$V$ heißt der $\P$ zugrunde liegende Vektorraum.
	Eine Teilmenge $X\subset \P$ heißt \emph{projektiver Unterraum} von $\P$ genau dann, wenn ein Unterraum $U\subset V$ existiert, so dass $X=\P(U)$.
\end{df}

\begin{note}
	Ein projektiver Unterraum ist selbst ein projektiver Raum
\end{note}

\begin{ex}
	Für $\dim V=0$ ist $\P(V) = \emptyset$  und $\P(V)$ hat Dimension $-1$.
	Für $\dim V=1$ ist $\P(V) = \{v\}$ ein Punkt mit Dimension $0$.
\end{ex}

\begin{df}
	\label{df:15.2}
	Seien $U_1,\dotsc,U_l$ projektive Teilräume von $\P$.
	Dann ist die \emph{projektive Hülle}
	\[
		\<U_1,\dotsc,U_l\>
	\]
	der kleinste projektive Teilraum von $\P$, der $U_1,\dotsc,U_l$ enthält.
\end{df}

\begin{lem}
	\label{lem:15.3}
	Seien $U_j$ ($j\in J$) projektive Teilräume mit zugrundeliegenden Vektorräumen $V_j$.
	Dann ist $\bigcap_{j\in J} U_j$	ein projektiver Teilraum und es gilt
	\[
		\bigcap_{j\in J}U_j = \P\left(\bigcap_{j\in J}V_j\right)
	\]
	\begin{proof}
		Die $V_j$ sind Untervektorräume, also ist auch $\bigcap_{j\in J}V_j$ Untervektorraum.
		\begin{align*}
			\P\left(\bigcap_{j\in J}V_j\right) &= \left\{ g \big| \dim g = 1 \land g\subset \bigcap_{j\in J}V_j\right\}\\
			&= \{g \big| \dim g=1 \land \forall j\in Jg\subset V_j \}\\
			&= \bigcap_{j\in J}\{ g\big| \dim g=1 \land g\subset V_j\}\\
			&= \bigcap_{j\in J}\P(V_j) = \bigcap_{j\in J}U_j
		\end{align*}
	\end{proof}
\end{lem}

Damit ist $\<U_1,\dotsc, U_l\>$ wohldefiniert.
Der kleinste Untervektorraum, der $V_1,\dotsc, V_l$ enthält ist die Summe $V_1 + \dotsb + V_l$.
Also
\[
	\<U_1,\dotsc,U_l\> = \P(V_1+\dotsb +V_l)
\]

Bei den Vektorräumen galt die Dimensionsformel
\[
	\dim V_1 + \dim V_2 = \dim (V_1 + V_2) + \dim (V_1\cap V_2)
\]
Wie sieht die Dimensionsformel für projektive Räume aus?

\begin{prop}
	\label{prop:15.4}
	Seien $U_1, U_2$ projektive Teilräume.
	Dann gilt
	\[
		\dim U_1 + \dim U_2 = \dim \<U_1,U_2\> + \dim ( U_1\cap U_2)
	\]
	\begin{note}
		Keine Fallunterscheidung, wie bei der Dimensionsformel für affine Räume.
	\end{note}
	\begin{proof}
		Sei $U_1 = \P(V_1), U_2 = \P(V_2)$ für Untervektorräume $V_1, V_2$.
		Dann ist
		\begin{align*}
			\dim U_1 &= \dim V_1 - 1\\
			\dim U_2 &= \dim V_2 - 1\\
			\dim \<U_1,U_2\> &= \dim (V_1 + V_2) - 1\\
			\dim U_1\cap U_2 &= \dim (V_1 \cap V_2) - 1\\
		\end{align*}
		Also
		\[
			\dim U_1 + \dim U_2 = \dim V_1 + \dim V_2 - 2 = \dim(V_1 + V_2) + \dim (V_1 \cap V_2) - 2 = \dim \<U_1,U_2\> + \dim (U_1\cap U_2)
		\]
	\end{proof}
\end{prop}

\begin{ex}
	Sei $V$ ein Vektorraum mit $\dim V = 3$.
	Seien $U_1, U_2$ projektive Geraden in einer projektiven Ebene $P$.
	Dann ist
	\[
		\dim (U_1 \cap U_2) = \dim U_1 + \dim U_2 - \dim \<U_1,U_2\> = 1 + 1 - 2 = 0
	\]
	Also schneiden sich beide Geraden auf der Ebene $P$ immer.
	Es gibt also in der projektiven Geometrie keine parallelen Geraden.
\end{ex}


\section{Koordinaten im projektiven Raum}


Sei $V = \K^{n+1}$ ein Vektorraum mit $\dim V=n+1$ und $\P = \P(V)$.
$x\in \P$ ist eine Gerade in $V$, also
\[
	x = \left\{ \lambda \begin{pmatrix}x_0\\\vdots \\ x_n\end{pmatrix}\neq 0 \Big| \lambda \in \K\right\}
\]
$\left(\begin{smallmatrix}x_0\\\vdots \\x_n\end{smallmatrix}\right)$ ist nicht eindeutig bestimmt.

Wir betrachten \emph{jeden} dieser Vektoren als Koordinaten von $x$.

\begin{df}
	\label{df:15.5}
	Das $n+1$-Tupel $(x_0:x_1:\dotsc: x_n)$ heißt die \emph{homogenen Koordinaten} von
	\[
		x = \left\{ \lambda \begin{pmatrix}x_0\\\vdots \\ x_n\end{pmatrix}\neq 0 \Big| \lambda \in \K\right\} \in \P
	\]

	Zwei Punkte $x,y$ sind gleich genau dann, wenn
	\[
		\exists \lambda\in \K\setminus\{0\} \forall k=0,\dotsc,n : y_k = \lambda x_k
	\]
	\begin{note}
		$(0:0:\dotsc:0)$ kommt nicht als Koordinate vor.
	\end{note}
\end{df}

\[
U := \left\{ \begin{pmatrix}x_0\\\vdots\\x_n\end{pmatrix}: x_i \in\K, x_0=0\right\} \subset V
\]
ist ein Untervektorraum von $V$.
$\P(U)$ ist ein projektiver Teilraum von $\P(V)$.
Wir bezeichnen $H:= \P(U)$ als Hyperebene.

Betrachte $\P(V) \setminus H$.
Sei $\P(V) \setminus H \ni x = (x_0:x_1:\dotsc:x_n)$.
Dann ist $x_0\neq 0$.
Wir können annehmen $x_0=1$ (wegen der Uneindeutigkeit der Koordinaten).
Also ist
\[
	\P(V) \setminus H = \{ x\in \P(V) \big| x_0 = 1\} = \{ (1:x_1:\dotsc, x_n) \}
\]
Also habe wir einen Isomorphismus
\[
	\P(V) \setminus H \isomorph \K^n
\]
definiert durch eine Bijektion
\begin{align*}
	K^n &\to \P(V) \setminus H\\
\begin{pmatrix}x_1\\\vdots\\ x_n\end{pmatrix} &\mapsto (1:x_1:\dotsc:x_n)
\end{align*}

Also ist $\P(V) \setminus H$ der affine Raum $\K^n$.
		
\fixme[Verdeutlichung durch Zeichnung]

Sei $V$ ein Vektorraum mit $\dim V=n+1$, $\P = \P(V)$ und $U < V$ ein Untervektorraum von $V$ mit $\dim U =n$.
Sei $H := \P(U)$ eine beliebige Hyperebene in $\P(V)$.
Sei $A := \P \setminus H$.

Wir wollen zeigen, dass allgemein $A$ ein affiner Raum (bezüglich $\K^n$) der Dimension $n$ ist.

Wähle einen Punkt $A\ni p_0 = \<v_0\>$ (mit festem $v_0\in V\setminus \{0\}$) als Basispunkt.
Da $A = \P(V) \setminus \P(U)$ ist $v_0 \not \in U$.
Sei $A \ni q = \<w\>$ mit $w\in V \setminus U$.
Es gilt
\[
	V = \<v_0\> \oplus U
\]
also können wir $w$ schreiben als
\[
	w = a v_0 + z \qquad z\in U, a\in \K \setminus \{0\}
\]
Es gilt 
\[
	q=\left\<\f 1a w\right\> = \left\<v_0 + \tilde z\right\> \qquad \tilde z \in U
\]
Diese Zerlegung ist eindeutig.
Damit ist $\tilde z \in U = \K^n$  ein Kandidat für den Vektor $\vec{p_0q}$.
Die Zerlegung $\f 1a w = v_0 + \tilde z$ bestimmt $\f 1a w$ eindeutig.
Also haben wir eine Abbildung
\begin{align*}
	A &\to V
	q &\mapsto \f 1a w := v_q
\end{align*}

\begin{df*}
	Wir definieren eine Operation
	\begin{align*}
		K^n \times A &\to A\\
		(x,q) &\mapsto q+x := \<v_q + x\>
	\end{align*}
\end{df*}

Wir zeigen die Axiome für affine Räume
\begin{enumerate}[({A}1)]
	\item
		$q+(x+y) = \<v_q + (x + y)\> = \<(vq+x) +y\> = (q+x) +y$
	\item
		$q + 0 = q$ 
	\item
		$v_q$ ist eindeutig in $\<v_q\>$ mit $v_q = 1\cdot v_0 + u_0$ für ein $u_0\in U=\K^n$
		\[
			q = \<v_q\> \<v_0 + \underbrace{(u_0 + x)}_{\in U}\>
		\]
		$v_q +x$ hat die selbe Eigenschaft, also ist wegen der Eindeutigkeit
		\[
			v_q  = v_q + x \implies x = 0
		\]
	\item
		Wir setzen für $q_1,q_2 \in A$
		\[
			x := v_{q_2} - v_{q_1} = v_0 + u_0 - (v_0+u_1) = u_0 -u_1 \in U
		\]
		dann ist
		\[
			q_1 + x = \<v_{q_1}+x\> = \<v_{q_1} + (v_{q_2}-v_{q_1})\> = \<v_{q_2}\> = q_2
		\]
\end{enumerate}


\begin{thm}
	\label{thm:15.6}
	Sei $\P=\P(V)$ ein $n$-dimensionaler projektiver Raum und $H=\P(U)$ eine Hyperebene.
	Sei $A := \P\setminus H$.
	Dann ist $A$ ein $n$-dimensionaler affiner Raum bezüglich $U$.
	\begin{note}
		Die Punkte von $A$ heißen \emph{eigentliche Punkte}, die von $H$ \emph{unegentliche Punkte}.
		$H$ ist die „unendlich ferne“ Hyperebene.

		Ein $n$-dimensionaler projektiver Raum ist also eine disjunkte Vereinigung eines $n$-dimensionalen affinen Raums und eines $(n-1)$-dimensionalen projektiven Raums. 
	\end{note}
\end{thm}

\begin{ex}[Quadriken]
	Was ist der Unterschied zwischen einer Ellipse $E$ und einer Hyperbel?

	Im euklidischen Raum ist die Ellipse beschränkt, also
	\[
		\exists N \forall p_1,p_2 \in E : d(p_1,p_2) < N
	\]
	Die Hyperbel ist dagegen nicht beschränkt.

	Im projektiven Raum gibt es keinen Unterschied.
	Eine Hyperbel ist eine Ellipse.

	Betrachte die projektive Ebene $\P = \P(\R^3)$.
	Wir wählen homogene Koordinatenn $x_0,x_1,x_2$ und betrachten
	\[
		x_0^2 + x_1^2 - x_2^2 = 0
	\]
	Sei $Q$ die Lösungsmenge dieser Gleichung.
	Wir betrachten $Q$ in verschiedenen affinene Ebenen.
	\begin{seg}{$x_2=0$}
		$x_2=0$ definiert einen $2$-dimensionalen Unterraum in $\R^3$, also eine projektive Gerade $H$.
		Das Komplement $\P\setminus H$ ist eine affine Ebene A, definiert durch $x_2 \neq 0$, z.B. $x_2=1$.
		\[
			Q \cap H : \quad x_2 = 0 \;\land\; x_0^2+x_1^2 = 0 \implies Q \cap H = \emptyset \implies Q \subset A
		\]
		Also ist $Q = Q \cap A$, definiert durch $x_0^2 + x_1^2 = 1$ also ist $Q$ ein Kreis.
	\end{seg}
	\begin{seg}{$x_0=0$ (oder $x_1=0$)}
		$x_0=0$ definiert eine andere projektive Gerade $H'$, $\P\setminus H'$ sei die affine Ebene $A'$.
		\[
			Q \cap H': \quad x_0 = 0 \;\land\; x_1^2 = x_2^2 \implies (0:1:1), (0:1:-1)
		\]
		Also ist
		\[
			Q \cap A' : \quad x_0=1 \;\land\; x_2^2-x_1^2=1
		\]
		eine Hyperbel in $A'$.
		Die beiden unendlich fernen Punkte schließen die Hyperbel zu einem Kreis.
	\end{seg}
	\begin{seg}{$x_1=x_2$}
		$x_1=x_2$ definiere $H''$.
		\[
			Q \cap H'' : \quad x_0^2 + x_1^2 - x_2^2 = 0 \;\land\; x_1=x_2 \implies  x_0=0 \;\land\; x_1=x_2=1
		\]
		Also $Q\cap H'' = \{(0:1:1)\}$.
		Die affine Ebene $A''$ ist definiert durch $x_1\neq x_2$.
		\[
			Q: \quad 0 = x_0^2 + x_1^2 -x_2^2 = x_0^2 + (x_1+x_2)(x_1-x_2)
		\]
		Führe Koordinatentransformation durch: $x_0' = x_0, x_1'=x_1+x_2, x_2'=x_1-x_2$ (diese ist nicht orthogonal!).
		Dann ist
		\[
			Q: \quad 0 =x_0'^2 + x_1'x_2' \qquad x_2'=1
		\]
		Wir erhalten die affine Gleichung
		\[
			x_1' = -x_0'^2
		\]
		was einer Parabel entspricht.
		$(0:1:1)$ schließt die Parabel zu einem Kreis.
	\end{seg}
\end{ex}

\begin{ex}
	Punkte der projektiven Ebene $\P=\P(\R^3)$ sind Geraden im Vektorraum $\R^3$.
	Wir betrachten voriges Beispiel im $\R^3$ gesehen:
	\[
		\R^3 \supset \left\{\begin{pmatrix}x_0\\x_1\\x_2\end{pmatrix}\Big| x_0^2+x_1^2-x_2^2=0\right\}
	\]
	Es ergibt sich ein Kegel.
	\fixme[Zeichnung]
	Die Punkte von $Q$ sind die Geraden auf dem Kreiskegel.
	Sei $Z$ der Nullpunkt als Zentrum einer Zentralprojektion.
	Die Bildebene sei eine affine Ebene, die wir variieren lassen.
	Jede Bildebene definiert eine Projektion.
	Das Bild unter dieser Projektion ist der Schnitt des Kreiskegels mit der Bildebene.
	
	Für $x_2=1$ ergibt sich als Schnitt ein Kreis, für $x_2=0$ ein Punkt.
	Für $x_0=1$ ergibt sich eine Hyperbel, für $x_0=0$ zwei Geraden.
	Für $x_0+x_2=1$ ergibt sich eine Parabel, für $x_0+x_2=0$ ergibt sich eine Gerade.

	Also sind alle Quadriken in der affinen Ebene sind Kegelschnitte.
\end{ex}


\section{Normalformen und Klassifikation von projektiven Quadriken}

\begin{df}
	\label{df:15.7}
	Sei $V$ ein $n$-dimensionaler $\R$-Vektorraum und $s:V\times V\to \R$ eine symmetrische Bilinearform.
	Definiere $q:V\to \R$ durch $q(v) = s(v,v)$.
	$q$ heißt die zu $s$ gehörende \emph{quadratische Form}.
	Die Menge der Projektiven Punkte im Raum
	\[
		\{ \<v\> \big| v\in V\setminus\{0\} \land q(v) = 0\} \subset \P(V)
	\]
	heißt \emph{Quadrik} (oder Hyperfläche zweiter Ordnung).
	\begin{note}
		\[
			q(\lambda v) = s(\lambda v, \lambda v) = \lambda^2 s(v,v) = \lambda^2 q(v) = 0 \iff q(v)=0
		\]
		Damit hängt die Definition nicht von der Wahl von $v\neq 0$ ab.

		Wir betrachten nur den reellen Fall.

		Die quadratische Form ist durch $s$ bestimmt.
		Umgekehrt bestimmt $q$ die Bilinearform $s$:
		\[
			s(x,y) = \f 12 (q(x+y) - q(x) -q(y))
		\]
	\end{note}
\end{df}

\begin{df}
	\label{df:15.8}
	Sei $V$ ein $n$-dimenisonaler $\R$-Vektorraum und $q_1,q_2 : V\to \R$ quadratische Formen.
	Dann heißen $q_1$ und $q_2$ \emph{äquivalent} genau dann, wenn ein $\R$-linearer Automorphismus $\phi$ existiert, so dass
	\[
		q_2(v) = q_1(\phi(v)) \qquad \forall v\in V
	\]
\end{df}

Welche Daten können wir für eine Klassifikation nutzen?

$q_1$ lässt sich eindeutig zu einer symmetrischen Bilinearform $s_1$ zuordnen und diese wiederum zu einer symmetrischen Matrix $A_1$
\[
	q_1(x) = s(x,x) = x^TA_1x
\]
$\phi$ ist durch eine invertierbare Matrix $T$ gegeben.
\[
	q_2(x) = (Tx)^TA_1Tx = x^T T^TA_1Tx
\]
Also definiert $T^TA_1T$ die symmetrische Matrix für die quadratisch Form $q_2$.
Es gilt
\[
	\rg(A_1) = \rg(T^TA_1T) = \rg(A_2)
\]
Der Rang der Matrizen ist also invariant unter Äquivalenz der quadratischen Formen.
Dieser reicht aber offensichtlich nicht aus zu eindeutigen Klassifikation, betrachte das Gegenbeispiel
\[
	A_1 = \begin{pmatrix}1&0\\0&1\end{pmatrix}\qquad A_2 = \begin{pmatrix}1&0\\0&-1\end{pmatrix}
\]


\begin{df}
	\label{df:15.9}
	Sei $q:V\to \R$ eine quadratische Form.
	Der \emph{Index} (oder \emph{Trägheitsindex}) von $q$ ist definiert als
	\[
		\Index(q) = \max\{s \in \N_0 \Big| \exists U < V : \dim U=s \; \land\; q\big|_U \text{ ist positiv definit}\}
	\]
	Die \emph{Signatur} von $q$ ist
	\[
		2 \cdot \Index(q) - \rg
	\]
	\begin{note}
		Es gilt stets
		\[
			0 \le \Index(q) \le \dim V
		\]
		Ist $q$ positiv definit, so ist
		\[
			\Index(q) = \dim V
		\]
		$q:V\to \R: v\mapsto 0$ hat $\Index(q)=0$
	\end{note}
\end{df}

\begin{ex}
	\[
		x_1^2 + \dotsb x_s^2 - x_{s+1}^2 - \dotsb - x_r^2
	\]
	Die zugehörige Matrix hat $\rg(A) = r = \texp{Anzahl Eigenwerte $\neq 0$}$ und $\Index=s$
	
	\begin{proof}
		Für $U = \<e_1,\dotsc, e_s\>$ ist $q|_U$ positiv definit..
		Also ist $\Index \ge s$.
		Zeige $\Index \le s$.
		Sei $W< V$ mit $q|_W$ positiv definit.
		Sei $X:= \<e_{s+1},\dotsc, e_n\>$.
		Dann ist $q(v) \le 0$ für alle $v$, also $W\cap X = \{0\}$.
		Dann gilt mit der Dimensionsformel für Vektorräume:
		\[
			\dim W + \dim X = \dim (W+x) + \dim(W\cap X)
		\]
		und damit $\dim W \le s$, also $\Index \le s$.
	\end{proof}	
\end{ex}

\begin{thm}[Trägheitssatz von Sylvester]
	\label{thm:15.10}
	Zwei quadratische Formen auf $V$ sind äquivalent genau dann, wenn sie gleichen Rang und gleichen Index haben.
	\begin{note}
		Für Rang $r$ und Index $s \le r \le n$ ist
		\[
			x_1^2 + \dotsb + x_s^2 - x_{s-1}^2 - \dotsb - x_r^2
		\]
		ein Beispiel, also eine Normalform.
	\end{note}
	\begin{proof}
		\begin{seg}{„$\Longrightarrow$“}
			Seien $q_1$ und $q_2$ äquivalent durch den Automorphismus $\phi: V \to V$.
			Dann ist
			\[
				q_1(\phi(v)) = v^T(T^TA_1^TT)v  \implies A_2 = T^TA_1T
			\]
			und damit $\rg(q_1) = \rg(A_1) = \rg(A_2) = \rg(q_2)$.
			
			$q_1|_U$ und $q_2|_W$ sind positiv definit und damit auch $q_2|_{\phi^{-1}(U)}$, bzw. $q_1|_{\phi(W)}$.
			Also ist $\Index(q_1) = \Index(q_2)$.
		\end{seg}
		\begin{seg}{„$\Longleftarrow$“}
			Seien $q_1$ und $q_2$ quadratische Formen auf $V$ mit dem selben Rang und selbem Index.

			Diagonalisiere die zu $q_1$ zugehörige symmetrische Matrix $A_1$ bezüglich einer Basis aus Eigenvektoren, so sortiert, dass $\lambda_1, \dotsc, \lambda_s > 0$, $\lambda_{s+1},\dotsc,\lambda_r < 0$, $\lambda_{r+1},\dotsc,\lambda_n = 0$.
			Der Rang entspricht dann der Anzahl Eigenwerte $\neq 0$ und der Index der Anzahl positiver Eigenwerte.

			Verfahre genauso mit $A_2$ und den Eigenwerte $\my_1,\dotsc,\my_s,\my_{s+1},\dotsc,\my_{r},\my_{r+1},\dotsc,\my_{n}$.
			Bezüglich diesen Diagonalisierungsbasen haben wir die Form
			\[
				\lambda_1x_1^2 + \dotsb + \lambda_sx_s^2 - |\lambda_{s+1}|x_{s+1}^2 - \dotsb - |\lambda_r|x_r^2
			\]
			Wir skalieren diese Basis $b_i$ zu einer neuen Basis $b_i'$ durch
			\[
				b_i' := \begin{cases} \f 1{\sqrt{\lambda_i}}b_i & 1\le i \le s\\
					\f 1{\sqrt{-\lambda_i}} b_i & s+1\le i\le r\\
					b_i & r+1 \le i \le n
				\end{cases}
			\]
			Dann ist für $1\le i \le s$
			\[
				q_1(b_i') =  q_1\Big( \f 1{\sqrt{\lambda_i}} b_i\Big) = \f 1{\lambda_i} q_1(b_i) = 1
			\]
			für $s+1\le i \le r$ entsprechend: $q_1(b_i') = -1$.

			Bezüglich der Basis $b_i'$ hat $q_1$ die Form
			\[
				x_1^2 + \dotsb + x_s^2 - x_{s+1}^2 - \dotsb - x_r^2
			\]
			Verfahre genauso mit $A_2$ bezüglich eine Basis $c_i'$.

			Beide Matrizen haben dann gleiches Aussehen.
			Der Automorphismus $\phi$ ist gegeben durch die Kompositions der Basistransformationen.
			Damit sind $q_1$ und $q_2$ äquivalent.
		\end{seg}
	\end{proof}
\end{thm}

Damit sind die quadratischen Formen algebraisch klassifiziert.
\[
	Q_1 = \{v: q_1(v) = 0\} \qquad Q_2=\{u: q_2(u) = 0\}
\]
Sind $q_1,q_2$ äquivalent, dann beschreiben $Q_1$ und $Q_2$ dieselbe Quadrik bezüglich verschiedenen Koordinaten.
Gilt auch die Umkehrung?
Gibt es eine geometrische Klassifikation?
\[
	x_1 + x_2^2 - x_3^2 =0
\]
mit Index $2$ beschreibt die selbe Quadrik wie
\[
	x_3^2 - x_1^2 -x_2^2 = 0
\]
mit Index $1$.

Was ist die geometrische Bedeutung von $r$ und von $\max\{s,r-s\}$?

\begin{prop}
	\label{prop:15.11}
	Sei $Q$ eine Quadrik und $g\subset \P$ ein Gerade.
	Dann tritt einer der folgenden Fälle auf:
	\begin{enumerate}[(a)]
		\item
			$g\cap Q = \emptyset$
		\item
			$g\cap Q$ besteht aus zwei Punkten
		\item
			$g \cap Q$ ist ein Punkt $P$ (dann heißt $g$ \emph{Tangente} im \emph{Berührungspunkt} $P$)
		\item
			$g \subset Q$ (dann heißt $g$ ebenfalls \emph{Tangente} und alle Punkte von $g$ heißen \emph{Berührungspunkte})
	\end{enumerate}
\end{prop}

\begin{prop}
	\label{prop:15.12}
	Die Quadrik $Q$ sei definiert durch die quadrische Form $q$.
	Sei $P$ ein Punkt auf $Q$.
	Dann tritt einer der folgenden Fälle auf.
	\begin{enumerate}[(a)]
		\item 
			Die Meneg alle Tangenten von $Q$ mit Berührungspunkt $P$ bildet eine Hyperebene von $\P$.
			Sei heißt \emph{Tangentialhyperebene} von $Q$ in $P$.
		\item
			Jede Gerad durch $P$ ist eine Tangente von $Q$ ($P$ heißt dann \emph{Doppelpunkt} von $Q$).
	\end{enumerate}
	Die Menge der Doppelpunkte von $Q$ ist ein projektiver Teilraum der Dimension $n-\rg(q)$.
\end{prop}

Bestimme $\max\{s,r-s\}$ geometrisch
\[
	q(x) = x_1^2 + \dotsb + x_s^2 - x_{s+1}^2 - \dotsb - x_r^2 = 0
\]
gesucht ist $s$.
$x$ liegt auf $Q$, wenn $q(x)=0$

Wir können bezüglich $x_1,\dotsc,x_s$ beliebige Vektoren wählen mit entsprechenden Koeffizienten, die alle $Q$ nicht schneiden.

\begin{prop}
	\label{prop:15.13}
	Sei $Q$ ein Quadrik, definiert durch $q$, wobei $q$ den Rang $r$ und den Index $s$ hat.
	Sei
	\[
		m := \max\{ a\in \Z \Big| \exists U \subset \P, U \text{ projektiver Teilraum}\; \land\; \dim U = a \;\land\; U \cap Q = \emptyset\}
	\]
	Dann ist $m=\max\{s,r-s\}-1$.
\end{prop}


\end{document}

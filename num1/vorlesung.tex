\documentclass[a4paper,11pt]{scrartcl}
\usepackage{mathe-vorlesung}

\title{Numerische Mathematik 1}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Approximation und Interpolation}

Sei $V= \{\Phi(x;a_0,\dotsc, a_n) \in C(\R) : a_0,\dotsc,a_n\in \R\}$ die Menge der stetigen Funktionen für $x\in \R$, deren Elemente durch Parameter $a_0,\dotsc,a_n$ parametrisiert sind.

Seien weiter Stützstellen $\{x_i\}_{i=0}^n\subset \R$ und Zielwerte $\{f_i\}_{i=0}^n \subset \R$ gegeben (z.B. Messungen oder Funktionwerte einer komplizierten (deren Auswertung kostspielig ist) Funktion $f_i=f(x_i)$, oder das Ergebnis eines Computerprogramms).

Ziel ist es, Parameter $a_0,\dotsc, a_n$ zu finden, so dass
\[
	\Phi(x_i;a_0,\dotsc, a_n) = f_i \qquad \forall i= 0,\dotsc,n \qquad \text{„Interpolation“}
\]
oder allgemeiner und obig:
\[
	\sum_{i=0}^n(\Phi(x_i; a_0,\dotsc a_m) -f_i)^2 \qquad \text{minimal}
\]
(„Least-squares-Approximation“)

\begin{ex}
	\begin{enumerate}
		\item 
			Polynominterpolation:
			\[
				\Phi(x; a_0,\dotsc, a_n) = \sum_{k=0}^na_kx^k \in \P_n
			\]
		\item
			Trigonometrische Interpolation:
			\[
				\Phi ( x;a_0,\dots,a_m,b_1,\dots, b_m ) = \frac{a_{0}}{2} + \sum_{k=1}^m ( a_k \cos kx + b_k \sin kx )
			\]
		\item
			Spline-Interpolation:
			Sei $a=x_0\lt x_1 \lt \dotsb \lt x_n = b$, $q,r\in \N_0$, $q\le r$
			\[
			V := \{f\in \C^q([a,b]) | f_{[x_i,x_{ibm}]}\in \P_r, i=0,\dotsc,n-1\}
			\]
			(global $q$-mal stetig differenzierbare Fuktionen, stückweise polynomial vom Grad $r$)
		\item
			Exponentielle Interpolation (nichtlinear)
			\[
			\Phi(x;a_0,\dotsc, a_m,\lambda_0,\dotsc, \lambda_m) = \sum_{k=0}^m a_ke^{\lambda_k x}
			\]
		\item
			Rationale Interpolation
			\[
				\Phi(x; a_0,\dotsc, a_m, b_0,\dotsc, b_{\_m}) = \f {a_0 + a_1x + \dotsb + a_m x^m}{b_0 + b_1x + \dotsb + b_{\_m}x^{\_m}}
			\]
	\end{enumerate}
\end{ex}

Potentielle Fragestellungen:
\begin{itemize}
	\item 
		Ist die Approximations-/Interpolationsaufgabe zu gegebenem $V$ und Daten $(x_i,f_i)$ lösbar? eindeutig?
	\item
		Wie finden wir algorithmisch die Koeffizienten?
	\item
		Können wir Fehleraussagen treffen?
	\item
		Was ist die optimale Stützstellenwahl?
	\item
		\dots
\end{itemize}


\subsection{Polynominterpolation}

Sei $\{x_i\}_{i=0}^n$, ${f_i}_{i=0}^n$ gegeben mit $x_i\neq x_j$ für $i\neq j$.

\begin{st}[Existenz und Eindeutigkeit]
	\label{st:1.1}
	Es existiert genau ein Polynom $p\in \P$ mit $p(x_i)=f_i$ für $i=0,\dotsc,n$.
	\begin{proof}
		$p(x) = \sum_{k=0}^n a_k x^k$ löst das Interpolationsproblem $p(x_i)=f_i$ genau dann, wenn $a=(a_k)_{k=0}^n \in \R^{n+1}$ löst $a_0 +a_1x_i^1 + \dotsb a_nx_i^n = f_i$ für $i=0,\dotsc,n$.
		Das ist äquivalent zu $Aa=f$ mit
		\[
		A= \begin{pmatrix} 1 & x_0 & x_0^2 & \hdots & x_0^n \\
		\vdots  \\
		1 & x_n & x_n^2 & \hdots & x_n^n\end{pmatrix}, f= \begin{pmatrix}f_0 \\ \vdots \\ f_n\end{pmatrix}
		\]
		Wir zeigen, dass $A$ regulär ist, bzw. $\ker(A) = \{0\}$.<br />
		Sei $\_a \in \R^{n+1}, A\_a=0$, dann erfüllt
		$\_p(x) := \sum_{k=0}^n\_a_k x^k$ die Gleichung $\_p(x_i)=0$.
		$\_p$ ist Polynom von Grad $n$ und hat $n+1$ Nullstellen. Also $\_p=0 \implies \_a_k=0 \implies \_a=0$
	\end{proof}
	\begin{note}
		\begin{enumerate}
			\item 
				Darstellung in Monombasis $p(x)= \sum_{k=0}^n a_kx^k$ heißt \emph{Normalform} des Interpolationsproblems.
			\item
				Die Matrix $A$ aus dem Beweis ist die \emph{Vandermondematrix}, typischerweis schlecht konditionirt und voll besetzt.
				Daher ist das zugehörige LGS $Aa=f$ recht aufwändig zu lösen.
			\item
				Bei Verwendung anderer Basen ist das Interpolationsproblem leichter zu lösen.
			\item
				Allgemeine lineare Interpolation:
				Sei $\{\phi_k\}_{k=0}^n$ linear unabhängig. Dann gilt
				\begin{align*}
					p(x) = \sum_{k=0}^n a_k \phi_k(x) \text{löst die Interpolationsaufgabe}
					\iff a\in \R^{n+1} \text{löst} Aa=f \text{mit} f=(f_i)_{i=0}^n \text{und} A= \begin{pmatrix}\phi_0(x_0) & \hdots & \phi_n(x_0)\\
					\end{pmatrix}
				\end{align*}
		\end{enumerate}
		
	\end{note}
\end{st}

\begin{st}[Lagrange-Form]
	\label{1.2}
	Die \emph{Lagrange-Polynome}
	\[\boxed{
		L_k^n(x) = \prod_{\substack{i=0 \\ i\neq k}}^n \f {x-x_i}{x_k-x_i} \qquad k=0,\dotsc,n
	}\]
	erfüllen
	\[
	L_k^n(x_i) = \delta_{ik} \qquad i,k=0,\dotsc, n
	\]
	und bilden eine Basis für $\P_n$ und das Interpolationspolynom hat sogenannte \emph{Lagrange-Form}
	\[
		\boxed{p(x) = \sum_{k=0}^n f_k L_k^n(x)}
	\]
	\begin{proof}
		siehe NLA.		
	\end{proof}
	\begin{note}
		\begin{enumerate}
			\item Wegen $L_k^n(x_i)=\delta_{ik}$ nennt man die $\{L_k\}_{k=0}^n$ eine nodale Basis
			\item Lösen eines LGS entfällt, da zugehörige Matrix $A=I$.
			\item $L_k^n(x)$ recht aufwändig auszuwerten
			\item Die Basen sind nicht hierarchisch, d.h. ${L_k^n} \not\subset \{L_k^{n'}\}$ für $n'\gt n$.
		\end{enumerate}
	\end{note}
\end{st}

\begin{df}[Newton-Polynome, Newton-Form]
	\label{df:1.3}
	Wir nennen
	\[
		\boxed{N_k^n(x) := \prod_{j=0}^{k-1}(x-x_j) \qquad k=0,\dotsc,n}
	\]
	\emph{Newton-Polynome} und die Darstellung des Interpolationspolynoms
	\[
		\boxed{p(x) = \sum_{k=0}^n a_k N_k^n(x)}
	\]
	\emph{Newton-Form} der Interpolierenden
\end{df}

\begin{lem}[Eigenschaften der Newton-Polynome]
	\label{lem:1.4}
	\begin{enumerate}[i)]
		\item $N_k^n(x_j) = 0$ für $0\le j \le k$ 
		\item $\{N_k^n\}_{k=0}^n$ bilden eine Basis für $\P_n$
		\item Die Basen sind hierarchisch, d.h. $\{N_k^n\}\subset \{N_k^{n'}\}$ für $n'\gt n$
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item klar
			\item
				$N_k^n \in \P_n$ ist klar, zeige lineare Unabhängigkeit.
				Sei $a\in \R^{n+1}$ mit $0=\sum_{k=0}^n a_k N_k^n(x)$.
				Angenommen $a\neq 0$, wähle $k_0 := \max\{k : a_k \neq 0\}$.
				Also
				\[
					0 \neq N_{k_0}^n = - \f 1{a_{k_0}} \sum_{k=0}^{k_0-1}a_k N_k^n(x)
				\]
				Also hat $N_{k_0}$ Grad $k_0$ und ist Summe von Polynomen niedrigeren Grades, ein Widerspruch.
				Damit ist $a=0$ und $\{N_k^n\}$ linear unabhängig.
			\item klar
		\end{enumerate}
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item Die Interpolationsmatrix ist eine untere Dreiecksmatrix
			\item LGS ist durch Vorwärtseinsetzen in $\mathcal O(n^2)$ lösbar
			\item Alternative Berechnung: Dividierte Differenzen (später)
		\end{itemize}
	\end{note}
\end{lem}

\begin{ex*}
	Sei $n=2$ und folgende Datenmenge gegeben.	
	\begin{table}[h]
		\centering
		\begin{tabular}{l|rrr}
			$x_i$ & 1 & 2 & 3  \\ \hline
			$f_i$ & 2 & 3 & 6 
		\end{tabular}
	\end{table}

	Wegen $N_0^2(x) = 1, N_1^2(x) = (x-1), N_2^2(x) = (x-1)(x-2)$ ist das zugehörige LGS von der Form
	\[
	\begin{pmatrix}1&0&0\\1&1&0\\1&2&2\end{pmatrix}\begin{pmatrix}a_0\\a_1\\a_2\end{pmatrix} = \begin{pmatrix}2\\3\\6\end{pmatrix}
	\]
	Es ergibt sich $a_0=2,a_1=1, a_2=1$ und damit das Interpolationspolynom
	\[
		p(x) = 2\cdot 1 + 1\cdot(x-1) + 1\cdot(x-1)(x-2) = x^2 - 2x+3
	\]
\end{ex*}

\begin{st}[Rekursionsformel]
	\label{st:1.5}
	Sei $p_{ij}\in \P_j$ das Interpolationspolynom zu den Daten $(x_i,f_l)$, $l=i,\dotsc,i+j$.
	Dann gilt für $j\ge 1$
	\[
		p_{i,j}(x) = \f {(x-x_i)p_{i+1,j-1} - (x-x_{i+j})p_{i,j-1}}{x_{i+j}-x_i}
	\]
	\begin{proof}
		Es gilt
		\begin{align*}
			p_{i+1,j-1}(x_i) &= f_l \qquad l=i+1,\dotsc,i+j\\
			p_{i,j-1}(x_i) &= f_l \qquad l=i,\dotsc,i+j-1
		\end{align*}
		Mit
		\[
			q(x) := \f {(x-x_i)p_{i+1,j-1} - (x-x_{i+j})p_{i,j-1}}{x_{i+j}-x_i}
		\]
		gilt $q\in \P_j$ und die Randpunkte werden interpoliert:
		\begin{align*}
			q(x_i) &= p_{i,j-1}(x_i) = f_i\\
			q(x_{i+j}) &= p_{i+1,j-1}(x_{i+j}) = f_{i+j}
		\end{align*}
		für die Zwischenstellen $l=i+1,\dotsc,i+j-1$ gilt
		\begin{align*}
			q(x_l) &= \f {(x_l-x_i)p_{i+1,j-1}(x_l) - (x_l-x_{i+j})p_{i,j-1}(x_i)}{x_{i+j}-x_l}\\
				   &= \f {(x_l-x_i)f_l - (x_l-x_{i+j})f_l}{x_{i+j}-x_i} = f_l
		\end{align*}
		Damit ist $q(x)$ ein Interpolationspolynom und wegen der Eindeutigkeit $p_{i,j}=q$.
	\end{proof}
\end{st}

\begin{df}[Dividierten Differenzen]
	\label{df:1.6}
	Wir definieren rekursiv
	\begin{align*}
		f[x_i] &:= f_i\\
		f[x_i,\dotsc,x_{i+j}] &:= \f {f[x_{i+1},\dotsc,x_{i+j}] - f[x_i,\dotsc,x_{i+j-1}]}{x_{i+j}-x_i}
	\end{align*}
\end{df}

\begin{st}[Newton-Form über Dividierten Differenzen]
	\label{st:1.7}
	Das Interpolationspolynom hat Darstellung
	\[
		p(x) = f[x_0]N_0^n(x) + f[x_0,x_1]N_1^n(x) + \dotsb + f[x_0,\dotsc,x_n]N_n^n(x)
	\]
	\begin{proof}
		Sei die Newton-Form
		\[
			p(x) := a_0 + a_1(x-x_0) + a_2(x-x_0)(x-x_1) + \dotsb + a_n\prod_{i=0}^{n-1}(x-x_i)
		\]
		gegeben.
		Dann ist
		\[
			p_{0,k}(x) := a_0 + a_1(x-x_0) + \dotsb + a_k\prod_{i=0}^{k-1}(x-x_i)
		\]
		Interpolierende zu den Daten $(x_j,f_j)$ für $j=0,\dotsc,k$, denn
		\begin{align*}
			f_j = p(x_j) &= \underbrace{\sum_{k'=0}^ka_{k'}N_{k'}^n(x_j)}_{p_{0,k(x_j)}} + \sum_{k'=k+1}^n a_{k'}\underbrace{\prod_{i=0}^{k'-1}(xj-x_i)}_{=0}\\
			&= p_{0,k}(x_j)
		\end{align*}
		also ist $a_k$ Koeffizient vor dem höchsten Term in $p_{0,k}$.
		Sei $a_{i,j}$ der Koeffizient des höchsten Terms in $p_{i,j}$.

		Es ergibt sich mit \ref{st:1.5} für den höchsten Koeffizienten:
		\begin{align*}
			a_{i,j} &= \f {a_{i+1,j-1} - a_{i,j-1}}{x_{i+j}-x_i}\\
			a_{i,0} &= f_i
		\end{align*}
		Per Induktion über $j$ folgt leicht, dass
		\[
			a_{i,j} = f[x_i,\dotsc, x_{i+j}]
		\]
		also insbesondere
		\[
			a_k = a_{0,k} = f[x_0,\dotsc, x_k]
		\]
	\end{proof}
\end{st}

\subsubsection{Neville-Schema}

Die Rekursion aus \ref{df:1.6} kann man als Schema darstellen:

\begin{tabular}{cccc}
$x_0$ & $f[x_0]=f_0$ & $f[x_0,x_1]$ & $f[x_0,x_1,x_2]$ \\
$x_1$ & $f[x_1]=f_1$ & $f[x_1,x_2]$ &  \\
\vdots & \vdots & \vdots\\
$x_n$ & $f[x_n]=f_n$ &\\
\end{tabular}

\begin{ex*}
	Sei $n=2$.	
	\begin{tabular}{l|rrr}
		$x_i$ & 1 & 2 & 3  \\ \hline
		$f_i$ & 2 & 3 & 6 
	\end{tabular}
	
	\fixme[Neville-Schema]
	Es ergibt sich das Interpolationspolynom
	\[
		p(x) = 2+ 1(x-1) + 1(x-1)(x-2) = x^2 -2x+3
	\]
\end{ex*}

\begin{note}
	\begin{itemize}
		\item Falls nach der Interpolation ein neues Datenpaar $(x_{n+1},f_{n+1})$ zur Verfügung steht, kann das alte Neville-Schema wiederverwendet werden.
			Es muss nur unterhalb der Diagonalen jeweils ein neuer Wert berechnet werden.
		\item
			Dividierte Differenzen $f[x_i,\dotsc,x_{i+j}]$ sind symmetrisch bezüglich den Daten, d.h. bei Permutation $\tau: \{i,\dotsc,i+j\} \to \{i,\dotsc,i+j\}$ gilt
			\[
				f[x_{\tau(i)},\dotsc,x_{\tau(i+j)}] = f[x_i,\dotsc,x_{i+j}]
			\]
	\end{itemize}
\end{note}

\subsubsection{Punkterweiterung der Interpolierenden}

\begin{enumerate}[a)]
	\item 
		Falls die Newton-Form vorliegt, erlaubt das \emph{Horner-Schema} die effiziente Auswertung durch geschickte Klammerung.
		\begin{align*}
			p(x) &= a_0 + a_1(x-x_0) + \dotsb + a_n\prod_{i=0}^{n-1}(x-x_i)
				&= (\dotso(a_n(x-x_{n-1} + a_{n-1})(x-x_{n-2}) + a_n-2) + \dotsb + a_0
		\end{align*}
		Satt $\mathcal(n^2)$ Produkte sind nur  $\O(n)$ Produkte erforderlich.
	\item
		Man kann das Interpolationspolynom auswerten, ohne es vorliegen zu haben.
		Verwende dazu das Neville-Schema für die Rekursion aus Satz \ref{st:1.5} nur für Stelle $x$.
		\begin{tabular}{cccc}
		$x_0$ & $f_0=p_{0,0}(x)$ & $p_{0,1}(x)$ & $p_{0,2}(x)$ \\
		$x_1$ & $f_1=p_{1,0}(x)$ & $p_{1,1}(x)$ &  \\
		\vdots & \vdots & \vdots\\
		$x_n$ & $f_n=p_{n,0}(x)$ &\\
		\end{tabular}
\end{enumerate}

\begin{ex}
	\begin{enumerate}[a)]
		\item 
			Horner-Schema mit Newton-Form
			\[
				p(x) = (1(x-2)+1)(x-1) +2
			\]
			und damit
			\[
				f(4) = 11
			\]
		\item
			Mit dem Neville-Schema
			\begin{tabular}{cccc}
				$x_i$ & $f_i$ &  & \\ \hline
				1 & 2 & 5 & 11\\
				2 & 3 & 9\\
				3 & 6
			\end{tabular}
	\end{enumerate}
\end{ex}

\subsubsection{Zusammenfassung}

\begin{tabular}{l|c|c}
	 & Lösen des LGS & Punktauswertung\\ \hline
	 Monombasis  & $\mathcal O(n^3)$ & $\mathcal O(n)$ via Horner \\ \hline
	 Lagrange-Basis & $\mathcal O(n)$ & $\mathcal O(n^2)$ \\ \hline
	 Newton-Basis & $\mathcal O(n^2)$ & $\mathcal O(n)$ via Horner
\end{tabular}

\subsubsection{Anwendung: Richardson-Extrapolation}

Für eine gegebene Funktion $b:(0,\infty)\to \R$ ist $\lim_{h\to 0} b(h)$ gesucht, bzw. eine Approximation.

\begin{seg}{Ansatz:}
Wähle $h_0,\dotsc, h_n \in \R^+$, berechne $b_k := b(h_k)$.
Sei $p(h)$ das Interpolationspolynom zu den Daten $(h_k,b_k)$.

Werte $p(0)$ aus als Approximation von $\lim_{h\to 0} b(h)$.
Nach vorigem Abschnitt b) ist $p(0)$ ohne Interpolationspolynom auswertbar.
Vereinfachung der Rekursion aus \ref{st:1.5} für $x=0$:
\begin{align*}
	p_{i,j}(0) &= \f {(0-h_i)p_{i+1,j-1}(0) - (0-h_{i+j})p_{i,j-1}(0)}{h_{i+j}-h_i} \\
	&= \f {(h_{i+j}-h_i)p_{i+1,j-1}(0)}{h_{i+j}-h_i} - \f {h_{i+j}(p_{i+1,j-1}(0)-p_{i,j-1}(0))}{h_{i+j}-h_i}
	&= p_{i+1,j-1}(0) + \f {p_{i+j,j-1}(0) -p_{i,j-1}(0)}{\f {h_i}{h_{i+j}} - 1}
\end{align*}
\end{seg}

\begin{df}[Richardson-Extrapolation]
	Zu ${h_i}_{i=0}^n \subset \R^+$ definiere
	\begin{align*}
		b_i := b_{i,0} &:= b(h_i)\\
		b_{i,j} &:= b_{i+1,j-1} + \f {b_{i+1,j-1}-b_{i,j-1}}{\f {h_i}{h_{i+j}}-1}
	\end{align*}
	Dann ist $b_{0,n}$ eine Approximation für $\lim_{n\to \infty} b(h)$.
\end{df}

\begin{ex}
	Berechnung von $e=\lim_{n\to \infty} (1 +\f 1n)^n = \lim_{n\to 0} (1+h)^{\f 1n}$.
	D.h. $b(h)=(1+h)^{\f 1h}$.
	Wähle $h_k=2^{-k} \implies b_{k,0}=b(h_k) = (1+2^{-k})^{2^k}$
	\[
		\implies b_{0,0} = 2, \qquad b_{1,0} = \f 94, \qquad b_{2,0} = \f {625}{256} \approx 2,44
	\]
	Neville-Schema für Rekursion aus 1.8, verwende $\f {h_i}{h_{i+j}} = 2^j$.
	\begin{table}[h]
		\centering
		\begin{tabular}{c|ccc}
			$h_0=1$ & $b_{0,0}=2$  & $b_{0,1}=\f 52$ & $b_{0,2}=\f {257}{96}$\\
			$h_1=\f12$ & $b_{1,0}=\f 94$  & $b_{1,1}=\f {337}{128}$ & $b_{0,2}=\f {257}{96}$\\
			$h_2=\f14$ & $b_{2,0}=\f {625}{256}$  \\
		\end{tabular}
	\end{table}
\end{ex}

\begin{note}
	\begin{enumerate}
		\item 
			Die Richardson-Extrapolation lohnt sich insbesondere für Funktionen, deren Auswertung für kleine $h$ teuer ist, z.B. $\mathcal O(\f 1n)$.
	\end{enumerate}
\end{note}

\begin{note}
	Verbesserungsmöglichkeit bei Kenntnis der Form der asymptotischen Entwicklung.
	Falls $b(h) = \sum_{n=0}^\infty a_nh^{qn}$ für $q>1$, dann ist die Interpolation mit Polynomen angebracht, die nur diese Terme $h^{qn}$ enthalten, diese werden anschließend extrapoliert.

	Setze dazu $\_h = h^q$, d.h. $\_{h_{i}}:= h_i^q$ und
	\[
		\_b(\_h) := \sum a_n\_h^n
	\]
	Dann ist $\_b(\_h_i) = b(h_i)$.
	Führe die Richardson-Extrapolation für $\{\_h_i\}$ und $\{\_b(\_h_i)\}$ durch, dannn ist das Ergebnis eine Aproximation für
	\[
		\lim_{\_h\to 0}\_b(\_h) = \lim_{h\to 0}b(h)
	\]
\end{note}

\subsubsection{Fehleranalysis}

Wir nehmen an, die Daten $\{t_i\}_{i=0}^n$ stammen von Funktionsauswertungen $f_i=f(x_i)$.

Wie gut apporximiert die Interpolierende die Funktion?

\begin{df}
	\label{df:1.9}
	Sei $I\subset \R$ ein abgeschlossenes Intervall.
	Wir definieren
	\[
		C(I) = C^0(I) := \{f: I\to \R : f \text{ stetig}\}
	\]
	als Raum der stetigen Funktionen und
	\[
		C^m(I) := \{f: I\to \R : f,f',\dotsc,f^{(m)} \text{ existieren und stetig}\}
	\]
	als Raum der $m$-mal stetig differenzierbaren Funktionen ($m\in \N$) und
	\[
		C^\infty(I) := \bigcap_{m\in \N}C^m (I)
	\]
	als Raum der unendlich oft stetig differenzierbaren Funktionen.

	Für $f\in C(I)$, $I$ beschränkt, definieren wir die Supremums-Norm
	\[
		\|f\|_\infty := \|f\|_{C(I)} := \sup_{x\in I}|f(x)|
	\]
\end{df}

\begin{note}
	\begin{itemize}
		\item 
			Es gilt $C^\infty(I) \subsetneq \dotsb \subsetneq C^0(I)$.
		\item
			$(C(I), \|\cdot\|_\infty)$ ist normierter Raum für $I$ beschränkt.
		\item
			$(C(I),\|\cdot\|_\infty)$ ist vollständig, d.h. jede Cauchy-Folge konvergiert in $C(I)$.
		\item
			Eine vollständigen, normierten Vektorraum nennen wir Banachraum, $(C(I),\|\cdot\|_\infty)$ Banachraum.
	\end{itemize}
\end{note}

\begin{st}[Punktweise Interpolationsfehler]
	\label{st:1.10}
	Sei $f\in C^{n+1}(\R)$ und $p\in \P_n$ Interpolierende.
	\[
		p(x_i) = f_i \qquad i=0,\dotsc,n \;\land\; x_i\neq x_j \text{ für } i\neq j
	\]
	Dann gilt für alle $x\in \R$
	\[
		p(x) - f(x) = \f 1{(n+1)!}f^{(n+1)}(\xi) (x-x_0)(x-x_1)\dotsb(x-x_1)
	\]
	mit geeignetem $\xi = \xi(x)\in I = \xi \in [\min(x,x_0,\dotsc,x_n),\max(x,x_0,\dotsc,x_n)]$.

	\begin{proof}
		Für $x=x_i$ ist 
		\[
			p(x) - f(x) = \f 1{(n+1)!}f^{(n+1)}(\xi) (x-x_0)(x-x_1)\dotsb(x-x_1)
		\]
		erfüllt ($0=0$).
		Für $x\neq x_i$, $i=0,\dotsc,n$.
		Setze
		\[
			w(x) := \prod_{i=0}^n (x-x_i) \qquad \text{„Knotenpolynom“}
		\]
		und definiere $g(t):= f(t)-p(t)-\f {w(t)}{w(x)}(f(x)-p(x))$.
		
		Es gilt $g(x_i)=0$, $i=0,\dotsc,n$ (wegen $f(x_i)=p(x_i)$) und $g(x)=0$.
		Also hat $g$ mindestens $n+2$verschiedene Nullstellen in $I$.
		Nach dem Satz von Rolle hat $g'$ mindestens $n+1$ verschiedene Nullstellen, usw.
		
		$g^{(n+1)}$ hat mindestens $1$ Nullstelle $\xi\in I$. 

		Mit $p^{(n+1)}(\xi) = 0$, $w^{(n+1)}(\xi) = (n+1)!$ folgt
		\begin{align*}
			0 = g^{(n+1)}(\xi) &= f^{(n+1)}(\xi) - p^{(n+1)}(\xi) - \f {((n+1)!}{w(x)}(f(x)-p(x))\\
			&= f^{(n+1)}(\xi) - 0 - \f {((n+1)!}{w(x)}(f(x)-p(x))\\
		\end{align*}
		Also 
		\[
			f(x) -p(x) = \f 1{(n+1)!}f^{(n+1)}(\xi) w(x)
		\]
	\end{proof}
\end{st}

\begin{kor}[Fehlerschranke]
	\label{kor:1.11}
	Für alle beschränkte Intervalle $I\subset \R$, mit $\{x_0,\dotsc,x_n\}\subset I$ gilt
	\[
		\|p-f\|_\infty \le \f 1{(n+1)!}\|w\|_\infty \|f^{(n+1)}\|_\infty
	\]
	\begin{proof}
		klar
	\end{proof}
\end{kor}

\begin{st}[Gleichmäßige Konvergenz]
	\label{st:1.12}
	Sei $I=[a,b]$, $f\in C^\infty(I)$ mit $\|f^{(n)}\|_\infty \le M$ für alle $n\in \N$.
	Sei $\Delta_n := \{x_0^n,\dotsc, x_n^n\}\subset I$ eine Menge $n$ disjunkter Stützstellen und $p_n\in \P_n$ eine zugehörige Interpolierende.
	Dann gilt
	\[
		\lim_{n\to \infty} \|p_n -f\| = 0
	\]
	\begin{proof}
		Für $w_n(x):=\prod_{i=0}^n(x-x_i^n)$ gilt $|w_n(x)| \le (b-a)^{n+1}$.
		Aus \ref{kor:1.11} folgt
		\[
			\|p_n-f\|_\infty \le \f {\|w_n\|_\infty}{(n+1)!}\|f^{(n+1)}\|_\infty \le \f{(b-a)^{n+1}}{(n+1)!}M \to 0
		\]
	\end{proof}
\end{st}

\begin{ex*}
	Sei $f(x)=e^x$ auf $I=[-1,1]$, $f\in C^\infty(I)$.
	Es gilt offensichtlich
	\[
		\|f^{(n)}\| \le e \qquad n\in \N
	\]
	Selbst wenn man nur Stützstellen auf $[-1,-0.75]$ wählt, konvergiert die Folge gleichmäßig gegen die Zielfunktion.
\end{ex*}

\begin{note}
	Vergleiche Stoer-Bulirsch.
	\begin{itemize}
		\item
			ohne Einschränkungen an $(\Delta_n)_{n\in \N}$ oder $f\in C(I)$. kann man \emph{keine} gleichmäßige Konvergenz und nicht einmal punktweise Konvergenz erwarten.
			Das Problem ist, dass $f^{(n+1)}(\xi)$ kann schneller mit $n$ wachsen, als $\left(\f {w(x)}{(n+1)!}\right)^{-1}$.	
		\item
			Für jedes $f\in C(I)$ existiert Folge $(\Delta_n)_{n\in \N}$ sodass $p_n\to f$ gleichmäßig.
		\item
			Satz von Faber: Zu jeder Folge $(\Delta_n)_{n\in \N}$ gibt es ein $f\in C([a,b])$ so dass $p_n\not\to f$ gleichmäßig.
		\item
			In der Praxis kommt es zu häufig Oszillationen von $p_n$, vor allem am Rand von $[\min\{x_0,\dotsc, x_n\},\max\{x_0,\dotsc,x_n\}]$.
	\end{itemize}
\end{note}

\begin{ex}[Runge]
	Sei $f(x)= \f 1{1+x^2}$ mit $I=[-5,5]$ und $x_k^n := -5+k*k+h_n$ ($k=0,\dotsc,n$) mit $h_n:= \f {10}n$ äquidistanten Stützstellen.

	Sei $p_n\in \P_n$ mit $p_n(x_k^n) = f(x_k^n)$, dann ergibt sich \fixme[Oszillationen].
	
	Man kann zeigen, dass $\tilde x \approx 3,68$ existiert, so dass in $(-\tilde x,\tilde x)$ punktweise und nicht punktweise Konvergenz für $|x|\ge \tilde x$. Sogar: $\|p_n-f\|_\infty \to \infty$.
\end{ex}

\subsubsection{Optimale Wahl von Stützstellen}

\begin{seg}{Idee}
Finde Stützstellen $\{x_0,\dotsc,x_n\}\subset I := [-1,1]$, so dass $\|w\|_\infty$ minimal wird.
Dann ist die Schranke in \ref{1.11} am kleinsten..
\end{seg}

\begin{df}[Tschebyscheff-Polynome]
	Wir definieren Tschebyscheff Polynome auf $I$, durch
	\begin{align*}
		T_0(x) &:= 1\\
		T_1(x) &:= x\\
		T_{n+1}(x) &:= 2xT_n - T_{n-1}(x)
	\end{align*}
	und normierte Tschebyscheff Polynome
	\[
		\hat T_n(x) := 2^{1-n} T_n(x)
	\]
\end{df}

\begin{ex}
	\begin{align*}
		T_2(x) &= 2x^2 - 1\\
		T_3(x) &= 4x^3 - 3x\\
		\hat T_2(x) &= x^2 -\f 12\\
		\hat T_3(x) &= x^3 -  \f 34 x
	\end{align*}
\end{ex}

\begin{st}
	\label{1.14}
	Für $n\in \N_0$ gilt
	\begin{enumerate}[i)]
		\item 
			$T_n\in \P_n$
		\item
			Die $\hat T_n$ sind normiert.
		\item
			Für $x\in [-1,1]$ gilt
			\[
				T_n(x) = \cos(n\cos^{-1}(x))
			\]
		\item
			$|T_n(x)| \le 1$
		\item
			$T_n(\cos(\f {j\pi}n)) = (-1)^j$ Extrema für $j=0,\dotsc,n$.
		\item
			$T_n(\cos(\pi \f {2j-1}{2n})) = 0$ Nullstellen für $j=1,\dotsc,n$.
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item klar
			\item klar
			\item
				Ein Additionstheorem liefert
				\[
					\cos(\alpha+\beta) = \cos(\alpha)\cos(\beta) - \sin(\alpha)\sin(\beta)
				\]
				Also
				\begin{align*}
					\cos((n+1)\theta) &= \cos(n\theta) \cos(\theta) - \sin(n\theta) \sin(\theta)\\
					\cos((n-1)\theta) &= \cos(n\theta) \cos(\theta) + \sin(n\theta) \sin(\theta)
				\end{align*}
				und damit
				\[
					\cos((n+1)\theta) + \cos((n-1)\theta) = 2 \cos(n\theta)\cos(\theta)
				\]
				Setze $\theta:= \cos^{-1}(x)$, dann ergibt sich
				\begin{align*}
					F_{n+1}(x) = \cos((n+1)\cos^{-1}(x)) &= 2\cos(n\cos^{-1}(x)) \cos(\cos^{-1}x) - \cos((n-1)\cos^{-1}(x))\\
					&= 2x \underbrace{\cos(n\cos^{-1}(x))}_{F_n(x)} - \underbrace{\cos((n-1)\cos^{-1}(x))}_{F_{n-1}(x)}
				\end{align*}
				d.h. $F_n(x):= \cos(n\cos^{-1}(x))$ erfüllt die Rekursion der Tschebyscheff Polynome aus Definition \ref{1.13}.
				Außerdem  $F_0(x)=1, F_1(x)=x$, also $F_n(x)=T_n(x)$.
			\item
				klar mit iii)
			\item
				Durch Nachrechnen mit iii)
			\item
				Durch Nachrechnen mit iii)
		\end{enumerate}
	\end{proof}
\end{st}

\begin{st}[Optimalität]
	\label{1.15}
	\begin{enumerate}[i)]
		\item 
			Sei $p\in \P_n$ normiert auf $[-1,+1] =: I$, dann gilt
			\[
				\|p\|_\infty \ge 2^{1-n}
			\]
		\item
			$\displaystyle \|\hat T_n \|_\infty = 2^{1-n}$
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item 
				Wir nehmen an, ein normiertes $p\in \P_n$ mit $|p(x)|<2^{1-n}$ für alle $x\in I$.
				Sei $x_j= \cos(\f {j\pi}{n})$.
				Mit \ref{1.15} und der Definition von $\hat T_n$ folgt
				\[
					(-1)^j p(x_j) \le |p(x_j)| < 2^{1-n} = 2^{1-n}\underbrace{(-1)^j}_{T_n(x_j)}(-1)^j = \hat T(x_j)(-1)^j
				\]
				und
				\[
					(-1)^j(\hat T_n(x_j)-p(x_j)) > 0 \qquad j=0,\dotsc,n
				\]
				also hat $\hat T_n -p$ mindestens $n$ verschiedene Nullstellen. $\hat T_n-p\in \P_{n-1}$, da beide normiert sind, ein Widerspruch.
				Also gilt
				\[
					|p(x)| \ge 2^{1-n} \qquad \forall p\in \P_n \text{ normiert}
				\]
			\item
				aus i) folgt wegen $\hat T_n$ normiert, dass
				\[
					\|\hat T_n\|_\infty \ge 2^{1-n}
				\]
				Mit Satz \ref{1.14} iv) ist $|T_n(x)| \le 1$ also $|\hat T_n(x)| \le 2^{1-n}$ für alle $x\in I$.
				Also 
				\[
					\|\hat T_n\|_\infty = 2^{1-n}
				\]
		\end{enumerate}
	\end{proof}
\end{st}

\begin{kor}[Optimale Stützstellen]
	\label{1.16}
	Setze $\{x_0,\dotsc,x_n\}$ als Nullstellen von den Tschebyscheff-Polynomen $T_{n+1}$:
	\[
		x_i := \cos\left(\pi\f{2i+1}{2(n+1)}\right) \qquad i=0,\dotsc,n
	\]
	Dann ist $\omega(x) := \prod_{i=0}^n (x-x_i) = \hat T_{n+1}$ Knotenpolynom mit minimalem $\|\omega\|_\infty$.
\end{kor}


\subsubsection{Stabilität der Interpolation}


Wie sensitiv ist die Interpolation bei Störungen in den Zielwerten $\{f_i\}_{i=0}^n$?.

\begin{df}[Lebesgue-Konstante]
	\label{1.17}
	Sei $I\subset \R$ ein kompaktes Intervall, $V\subset C(I)$ ein Unterraum der Dimension $n+1$, paarweise verschiedene Stützstellen $\{x_0,\dotsc, x_n\}\subset I$ gegeben und $\{L_k^n(x)\}_{k=0}^n$ nodale Basis von $V$, d.h. $L_k^n(x_j) = \delta_{kj}$.
	Dann nennen wir
	\[
		\Lambda_n := \max_{x\in I}\sum_{k=0}^n |L_k^n(x)|
	\]
	die \emph{Lebesgue-Konstante} der Interpolation.
\end{df}

\begin{note}
	$\Lambda_n$ ist insbesondere auch für Interpolation mit anderen Funktionen als Polynome definiert.

	Sei $V=\<\{\phi_k\}_{k=0}^n\> \subset C(I)$ mit Basis $\{\phi_k\}_{k=0}^n$.
	Falls die Interpolationsmatrix regulär ist, so existiert eine nodale Basis von $V$:
	\[
		A=\begin{pmatrix}
			\phi_0(x_0) & \hdots & \phi_n(x_0)\\
			\vdots & \ddots & \vdots\\
			\phi_0(x_n) & \hdots & \phi_n(x_n)
		\end{pmatrix}
		\qquad \text{regulär}
	\]
	Dann ist $B=(b_{ik})_{i,k=0}^n := A^{-1}$ und $L_k^n(x) := \sum_{i=0}^n b_{ik}\phi_i(x)$ wohldefiniert und
	\[
		L_k^n (x_j) = \sum_{i=0}^n b_{ik}\phi_i(x_j) = (A B)_{jk} = I_{jk} = \delta_{jk}
	\]
\end{note}

\begin{st}[Störungs-Aussage]
	\label{1.18}
	Seien $\{f_i\}_{i=0}^n$ und $\{\tilde f_i\}_{i=0}^n$ zwei Mengen von Zielwerten über Stützpunkten $\{x_i\}_{i=0}^n\subset I$ und $p$ und $\tilde p$ zugehörige Interpolationsfunktionen.
	Dann gilt
	\[
		\|p-\tilde p\|_\infty \le \Lambda_n \max_{i=0,\dotsc,n}|f_i-\tilde f_i|
	\]
	Diese Abschätzung ist scharf (d.h. $\Lambda_n$ kann nicht kleiner gewählt werden, wir finden Beispiele $\{f_i\},\{\tilde f_i\}$ für die Gleichheit gilt).
	\begin{proof}
		Zeige zunächst: $\forall \{\_{f_i}\}\subset \R$ und interpolierende Funktion $\_p\in V$, gilt:
		\[
			\|\_p\|\infty \le \Lambda_n \max_{i=0,\dotsc,n}|\_{f_i}| \qquad \text{und ist scharf}
		\]
		Für $t\in I$ gilt
		\begin{align*}
			|\_p(t)| &= \left| \sum_{i=0}^n \_{f_i} L_i^n(t)\right| \\
			&\le \sum_{i=0}^n |\_{f_i}||L_i^n(t)|\\
			&\le \sum_{i=0}^n (\max_{j=0,\dotsc,n}|\_{f_j})|L_i^n(t)|\\
			&\le \max_{j=0,\dotsc,n}|\_{f_j}| \sum_{i=0}^n|L_i^n(t)|
		\end{align*}
		Also ist
		\[
			\|\_p\|_\infty \le \Lambda_n \max_{j=0,\dotsc,n}|\_{f_j}|
		\]

		Sei $\tau := \argmax_{x\in I} \sum_{i=0}^n|L_i^n(x)|$ und $\_{f_i} := \sgn(L_i^n(\tau))$.
		Dann gilt
		\begin{align*}
			\_p(\tau) &= \left| \sum_{i=0}^n \underbrace{\_{f_i}L_i^n(\tau)}_{\ge 0}\right| \\
			&= \sum_{i=0}^n |L_i^n(\tau)| \\
			&= \max_{x\in I} \sum_{i=0}^n |L_i^n(x)| \\
			&= \Lambda_n \cdot \underbrace{1}_{=\max_{i=0,\dotsc,n}|\_{f_i}|}
		\end{align*}
		also ist 
		\[
			\|\_p\|_\infty \le \Lambda_n \max_{j=0,\dotsc,n}|\_{f_j}|
		\]
		scharf.

		Der Satz folgt, da $\_p := p-\tilde p$ das Interpolationspolynom zu den Daten $\_{f_i} := f_i - \tilde {f_i}$
	\end{proof}
\end{st}

\begin{ex*}[Polynominterpolation, vgl. Deuflhard/Hohmann]~

	\begin{table}[h]
		\centering
		\begin{tabular}{r|r|r}
			$n$ & $\Lambda_n$ für äquidistante $\Delta_n$ & $\Lambda_n$ für Tschebyscheff-Knoten \\ \hline
			5 & 3,106292 & 2,104398 \\
			10 & 29,890695 & 2,489430 \\
			15 & 512,052451 & 2,727778 \\
			20 & 10986,533993 & 2,900825
		\end{tabular}
	\end{table}		
	Also auch quantitativ führen die Tschebyscheff-Knoten zu stabilerer Interpolation.
\end{ex*}


\subsubsection{Hermite-Interpolation}


Seien $\{x_i\}_{i=0}^m \subset \R$ paarweise verschiedene Knoten und $\{f_i^{(j)}\}_{j=0}^{m_i-1}$ Zielwerte für $i=0,\dotsc, m$.
Gesucht ist ein Polynom $p\in \P_n$ mit $p^{(j)}(x_i) =f_i^{(j)}$ mit $i=0,\dotsc,m$ und $j=0,\dotsc,m_i-1$ und 
\[
	n := \sum_{i=0}^m m_i - 1
\]

\begin{st}[Existenz und Eindeutig]
	\label{1.19}
	Es existiert ein eindeutiges Polynom $p\in \P_n$ als Lösung des Hermite-Interpolationspolynom
	\begin{proof}
		Wir verfahren ähnlich wie in \ref{1.1}.
		Zeige, dass die Interpolationsmatrix $A$ bezüglich $\{p_i\}_{i=0}^n$ regulär ist.
		\[
			A= \begin{pmatrix}
				\phi_0^{(0)}(x_0) &\hdots &\phi_n^{(0)}(x_0)\\
				\phi_0^{(1)}(x_0) &\hdots &\phi_n^{(1)}(x_0)\\
				\vdots &  & \vdots \\
				\phi_0^{(m_0)}(x_0)& \hdots &\phi_n^{(m_0)}(x_0)\\
				\phi_0^{(0)}(x_1) &\hdots &\phi_n^{(m_0)}(x_1)\\
				\vdots& & \vdots \\
				\phi_0^{(m_m)}(x_m) &\hdots &\phi_n^{(m_m)}(x_m)
			\end{pmatrix}
			\qquad \text{ist quadratisch}
		\]
		Wir zeigen jetzt $\ker(A) = 0$.

		Sei $A\_a = 0$ für $a\in \R^{n+1}$, dann erfüllt $\_p(x) := \sum_{i=0}^n \_a_i \phi_i(x)$ die Gleichung $\_p^{(j)}(x_i) = 0$ für $i=0,\dotsc,m$ und $j=0,\dotsc,m_i-1$.
		
		Also ist $(x-x_i)^{m_i}$ Teiler von $\_p$ für jedes $x_i$.
		Insgesamt ist also
		\[
			\omega(x) := \prod_{i=0}^m (x-x_i)^{m_i}
		\]
		ein Teiler von $\_p$ in $\P_n$, d.h.
		\[
			\exists q(x)\in \P_n : \_p(x) = q(x) \omega(x)
		\]
		Es gilt $\deg \_p \le n$ und $\deg \omega = \sum_{i=0}^m m_i = n+1$.
		Also muss $q(x)=0$ und damit $\_p(x) = 0$.

		Also $\_a=0$.
	\end{proof}
\end{st}

\begin{note}
	Falls nicht alle Ableitungen $p^{(j)}(x_i) =f_i^{(j)}$ für $j=0,\dotsc,m_i-1$ vorgeschrieben sind, ist die Interpolations-Aufgabe nicht immer lösbar.

	\begin{ex*}
		Sei $m=1$, $n=1$.
		Gesucht ist $p\in \P_1$ mit
		\[
			p'(x_0) = 1 \qquad \land \qquad p'(x_2) = 2
		\]
		Dazu gibt es keine Lösung (keine Existenz).

		\[
			p'(x_0) = 1 \qquad \land \qquad p'(x_2) = 1
		\]
		Hierzu sind alle $(x) = x+a$, $a\in \R$ Lösungen (keine Eindeutigkeit).
	\end{ex*}
\end{note}

\begin{st}[Fehlerdarstellung]
	\label{1.20}
	Sei $f\in C^{n+1}(I)$, $\{x_i\}_{i=0}^n\subset I$ und $p\in \P_n$ das Interpolations-Polynom mit
	\[
		p^{(j)}(x_i) = f^{(j)}(x_i) \qquad i=0,\dotsc,m \quad j=0,\dotsc,m_i-1
	\]
	Dann existiert für alle $x\in I$ ein $\xi \in I$ mit
	\[
		f(x) - p(x) = \f 1{(n+1)!} f^{(n+1)} (\xi) \omega(x)
	\]
	für $\omega(x) = \prod_{i=0}^n(x-x_i)^{m_i}$.
	\begin{proof}
		Analog zu Satz \ref{1.10}.
	\end{proof}
\end{st}

\begin{st}[Newton-Form]
	\label{1.21}	
	Das Hermite-Interpolationspolynom ist gegeben durch
	\[
		p(x) = \sum_{k=0}^n a_k \prod_{j=0}^{k-1}(x-z_j)
	\]
	mit Stützstellen
	\[
		z_k := x_i \qquad k=n_{i-1},\dotsc, n_i-1
	\]
	wobei $n_i = \sum_{i=0}^{i-1}m_r$.

	Die Koeffizienten $a_k := a_{0,k}$ für $k=0,\dotsc,n$ werden erhalten aus der Rekursion
	\begin{align*}
		a_{k,j} := \begin{cases}
			\displaystyle \f 1{j!}f_i^{(j)} 			&  \begin{aligned}i&=0,\dotsc,m \\ j&=0,\dotsc,m_i-1 \\ k&=n_{i-1},\dotsc, n_i-1-j\end{aligned} \\
			\displaystyle \f {a_{k+1,j-1} - a_{k,j-1}}{z_{k+j} - z_k} & \text{sonst}
		\end{cases}
	\end{align*}
\end{st}

\begin{ex*}
	Sei vorgegeben

	\begin{table}[h]
		\centering	
		\begin{tabular}{l|c|c}
			 & $x_0=1$ & $x_1 =2$ \\ \hline
			$f_i^{(0)}$ & -1 & 14 \\
			$f_i^{(1)}$ & 4 & 32 \\
			$f_i^{(12)}$ & 12 & 
		\end{tabular}		
	\end{table}

	Dann ist $m=1$, $m_0=3$, $m_1=2$, und damit $n=3+2-1=4$.
	Es werden die Stützstellen $z_0=z_1=z_2=x_0$, $z_3=z_4=x_1$ gewählt.
	Die Basispolynome sind dann
	\[
		1, (x-1), (x-1)^2, (x-1)^3, (x-1)^3(x-2)
	\]
	Bestimme die Koeffizienten mit Hilfe des Neville-Schemas

	\begin{table}[h]
		\centering	
		\begin{tabular}{l|c|c|c|c|c|c}
			$z_k$  & $a_{k,0}$ & $a_{k,1}$ & $a_{k,2}$ \\ \hline
			$z_0=1$ & $f_0^{(0)} = -1$  & $\f 1{1!}f_0^{(1)} = 4$ & $\f 1{2!} f_0^{(2)} = 6$ & $\f {11-6}{2-1}=5$ & $\f {6-5}{2-1}=1$ \\
			$z_1=1$ & $f_0^{(0)} = -1$ & $4$ 						&	$\f {15-4}{2-1} = 11$ & $\f {17-11}{2-1}=6$ \\
			$z_2=1$ & $f_0^{(0)} = -1$ & $\f {14-(-1)}{2-1} = 15$ & $\f {32-15}{2-1} = 17$ \\
			$z_3=2$ & $f_0^{(0)} = -1$ & $\f 1 {1!} f_1^{(1)} = 32$ &  \\
			$z_4=2$ & $f_0^{(0)} = -1$ & 
		\end{tabular}		
	\end{table}
	Es ergibt sich dann
	\begin{align*}
		p(x)& = -1\cdot 1 + 4\cdot (x-1) + 6\cdot(x-1)^2 + 5\cdot(x-1)^3 + 1\cdot(x-1)^3 (x-2) = x^4-2\\
		p'(x) &= 4x^3\\
		p''(x) &= 12x
	\end{align*}
\end{ex*}


\subsection{Trigonometrische Interpolation}

Für gegebenes $n\in \N$, $x_k = \f {2\pi}{n+1}k \in [0,2\pi)$, $k=0,\dotsc,n$ äquidistant und $\{f_k\}_{k=0}^n\subset \R$ist eine $2\pi$-periodische Funktion $t(x)$ der Gestalt
\[
	t(x) = \f {a_0}2 + \sum_{k=1}^n a_k  \cos(kx) + b_k \sin(kx) \qquad a_k,b_k \in \R
\]
mit $t(x_k)=f_k$ gesucht.

\begin{note}
	\begin{itemize}
		\item
			Diese Darstellung ist zunächst nur für gerade $n$ sinnvoll.
			Für ungerade $n$ wird eine leichte Modifikation notwendig sein.
		\item
			Bestimmung von $t(x)$ wird über komplexe trigonometrische Polynome erfolgen.
	\end{itemize}
\end{note}

\begin{lem}[Reelle und komplexe Fourier-Summe]
	\label{1.22}
	Seien $\{a_k,b_k\}_{k=0}^m\subset \R$, $\{c_k\}_{k=-m}^m\subset \C$ mit $c_0=\f {a_0}2$, $c_k=\f 12(a_k-ib_k)$ und $c_{-k}=\f 12(a_k+ib_k)$, $k=1,\dotsc, m$.

	Dann gilt
	\[
		\f {a_0}2 + \sum_{k=1}^m (a_k\cos(kx) + b_k\sin(kx)) = \sum_{k=-m}^m c_k e^{ikx}
	\]
	\begin{proof}
		Einsetzen und die Eulersche Formel $e^{iz}=\cos(z) +i\sin(z)$ liefert
		\begin{align*}
			\sum_{k=-m}^m c_ke^{ikx} &= \f {a_0}2 (\cos(0x) + i\sin(0x)) \\
			& \qquad + \sum_{k=1}^m \f 12 (a_k-ib_k)(\cos(kx) +i\sin(kx)) + \sum_{k=1}^m \f 12 (a_k+ib_k)(\cos(-kx)+i\sin(-kx)) \\
			&= \f {a_0}2 + \sum_{k=1}^m a_k\cos(kx) + b_k \sin(kx)
		\end{align*}
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				Die reelle Fourier-Summe ergibt also die komplexe Fouriersumme mit entsprechend gewählten $c_k$.

				Das funktioniert auch umgekehrt für eine gegebene Fouriersumme $\{c_k\}_{k=-1}^m\subset \C$ mit $c_k=\_{c_{-k}}$, $c_0\in \R$.
				Die Wahl
				\[
					a_k := c_k +c_{-k}, \qquad b_k := i(c_k-c_{-k}) \qquad k=0,\dotsc,m
				\]
				erfüllt alle Bedingungen aus Lemma \ref{1.22}.
		\end{itemize}
	\end{note}
\end{lem}

\begin{df}[Trigonometrische Polynome]
	\label{1.23}
	\[
		T_n := \left\{ q: \C \to\C : q(z) = \sum_{k=0}^n c_k e^{ikz}\right\}
	\]
	ist der Raum der trigonometrischen Polynome von Grad $n$.
	\begin{note}
		Mit $\omega(z) = e^{iz}$ ist $q(z) = \sum_{k=0}^n c_k\omega(z)^k$.
	\end{note}
\end{df}

\begin{st}[Trigonometrische Interpolation in $\C$]
	\label{1.24}
	Zu $\{f_k\}_{k=0}^n \subset \C$ existiert genau ein $q\in T_n$ mit $q(x_k) = f_k$, $k=0,\dotsc, n$.

	Die Koeffizienten sind gegeben durch
	\[
		c_k = \f 1{n+1}\sum_{j=0}^n f_j \omega_k^{-j}
	\]
	mit $\omega_k := e^{ix_k}$.
	\begin{proof}
		Satz \ref{1.1} gilt auch im Komplexen, also existiert genau ein Polynom $p(z)\in \P_n$ mit $p(z)=\sum_{k=0}^n c_kz^k$, $c_k\in \C$ und $p(\omega_k)=f_k$.

		Wir setzen $q(z) := \sum_{k=0}^n c_ke^{ikz} \in T_n$.
		$q(z)$ erfüllt die Interpolationsbedingungen:
		\[
			q(x_l) = \sum_{k=0}^n c_k e^{ikx_l} = \sum_{k=0}^n c_k \omega_l^k = p(\omega_l) = f_l \qquad l=0,\dotsc,n
		\]
		Es gilt
		\begin{enumerate}[i)]
			\item
				$\displaystyle \omega_k^{-j} = e^{-ij\f {2\pi}{n+1}k} = \omega_j^{-k}$
			\item
				$\displaystyle \sum_{j=0}^n (\omega_{l-k})^j = (n+1)\delta_{lk}$
				\begin{proof}
					\[
						\sum_{j=0}^n (\omega_{l-k})^j = \begin{cases}
							n+1 & l=k\\
							\f{(\omega_{l-k})^{n+1}-1}{\omega_{l-k}-1} & l\neq k
						\end{cases}
					\]
					Da jedoch 
					\[
						(\omega_{l-k})^{n+1e} = e^{i\f {2\pi}{n+1}(l-k)(n+1)} = e^{i2\pi(l-k)} = 1
					\]
					folgt
					\[
						\sum_{j=0}^n (\omega_{l-k})^j = \begin{cases}
							n+1 & l=k\\
							0 & l\neq k
						\end{cases}
					\]
				\end{proof}
		\end{enumerate}
		Daher gilt für Koeffizienten von $q(z)$
		\begin{align*}
			\sum_{j=0}^n f_j \omega_k^{-j} &= \sum_{j=0}^n p(\omega_j) \omega_k^{-j}\\
			&= \sum_{j=0}^n\left( \sum_{l=0}^n c_l \omega_j^l\right) \omega_k^{-j}\\
			&= \sum_{j,l=0}^n c_l \omega_j^{l-k} \\
			&= \sum_{l=0}^n c_l \sum_{j=0}^n \omega_{l-k}^j\\
			&= \sum_{l=0}^n c_l (n+1) \delta_{lk} = c_k (n+1)
		\end{align*}
		Dividieren durch $n+1$ liefert die Behauptung.
	\end{proof}
\end{st}

\begin{st}[Trigonometrische Interpolation in $\R$]
	\label{1.25}
	Für $n\in \N$ setze 
	\[
		m:= \begin{cases} \f n2 & \text{falls $n$ gerade} \\
			\f {n-1}2 & \text{falls $n$ ungerade}\end{cases} 
		\qquad 
		\theta := \begin{cases} 0 & \text{falls $n$ gerade} \\ 1 & \text{falls $n$ ungerade}\end{cases}
	\]
	Zu $x_k:=\f {2\pi}{n+1}k$ und Daten $\{f_k\}_{k=0}^n\subset \R$ existiert dann genau eine Funktion
	\[
		t(x) = \f {a_0}2 + \sum_{k=1}^m \Big(a_k \cos(kx) + b_k \sin(kx)\Big) + \f{\theta}2 a_{m+1}\cos\big((m+1)x\big)
	\]
	mit $t(x_k) = f_k$, $k=0,\dotsc,n$.

	Für die Koeffizienten gilt
	\begin{align*}
		a_k &:= \f 2{n+1} \sum_{j=0}^n f_j(\cos(jx_k))\\
		b_k &:= \f 2{n+1} \sum_{j=0}^n f_j(\sin(jx_k))
	\end{align*}
	\begin{proof}
		Nach \ref{1.24} existiert ein eindeutiges $q\in T_n$ mit $q(x_k)=f_k$,
		\[
			q(z) = \sum_{k=0}^nn c_k e^{ikz}, \qquad c_k = \f 1{n+1}\sum_{j=0}^n f_j \omega_k^{-j}
		\]
		Setze $c_{-k} := c_{n+1-k}$ für $k=1,\dotsc,m$.
		Wegen
		\[
			\_{w_k} = e^{-i \f {2\pi}{n+1}k} = e^{i \f{2\pi}{n+1}(n+1-k)} = \omega_{n+1-k}
		\]
		gilt
		\[
			c_{-k} = c_{n+1-k} = \f 1{n+1}\sum_{j=0}^n f_j \omega_{n+1-k}^{-j} = \f 1{n+1}\sum_{j=0}^n f_j\_{w_k}^{-j} = \_{c_k}
		\]
		Also ist Lemma \ref{1.22} anwendbar und mit der Wahl $a_k := c_k +c_{-k}$, $b_k := i(c_k-c_{-k})$, $k=0,\dotsc, m$ gilt:
		\[
			\f{a_0}2 + \sum_{k=1}^m \Big(a_k \cos(kx) + b_k \sin(kx) \Big) = \sum_{k=-m}^m c_k e^{ikx}
		\]
		Es gilt außerdem
		\[
			f_l = \sum_{k=0}^n c_k \omega_l^k = \sum_{k=0}^m c_k \omega_l^k + \sum_{k=1}^m c_{n+1-k}\omega_l^{-k} + \theta c_{m+1} \omega_l^{m+1}
		\]
		Für $n$ ungerade ist $m+1 = \f {n+1}2$ und 
		\[
			\omega_{l}^{m+1} = e^{i(m+1)\f{2\pi}{n+1}l} = e^{i\pi l} = \cos(\pi l) = \cos\left( \f {2\pi}{n+1} l \cdot \f{n+1}2\right) = \cos((m+1)x_l)
		\]
		Zusammen mit obigem ergibt sich mit der Wahl $a_{m+1}:= 2c_{m+1}$.
		\begin{align*}
			f_l = \f {a_0}2 + \sum_{k=1}^m \Big(a_k \cos(kx_l) + b_k \sin(kx_l)\Big) + \theta \f {a_{m+1}}2 \cos((m+1)l) 
			=  t(x_l)
		\end{align*}

		Für die Koeffizienten gilt 
		\begin{align*}
			a_k:= c_k + c_{-k} &= \f 1{n+1}\sum_{j=0}^n f_j (\omega_k^{-j}+\omega_k^j) \\
			&= \f 1{n+1} \sum_{j=0}^n f_j(e^{ijx_k} + e^{ijx_k})\\
			&= \f 2{n+1} \sum_{j=0}^n f_j \cos (jx_k)
		\end{align*}
		für $b_k$ analog mit $b_k := i(c_k -c_{-k})$.
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				Es gilt zwar $t(x_l)=q(x_l)$, aber im Allgemeinen ist $t(x)\neq q(x)$ und auch $t(x)\neq \Re(q(x))$ für $x\neq x_l$.
		\end{itemize}		
	\end{note}
\end{st}

\begin{ex*}
	Sei $n=2$ und
	\begin{table}[h]
		\centering
		\begin{tabular}{c|c|c|c}
			$x_j$ & $x_0=0$ & $x_1=\f 23 \pi$ & $x_2=\f 43 \pi$\\\hline
			$f_j$ & $0$ & $\f 32$ & $\f 32$
		\end{tabular}
	\end{table}
	Es ist $\cos(\pm x_1) = \cos(\pm x_2) = -\f 12$, $\sin(x_1) = -\sin(x_2) = i3$
	\begin{align*}
		c_0 &= \f 13 (f_0e^{-i0} + f_1 e^{-i0} + f_2e^{-i0}) = 1\\
		c_1 &= \f 13 (f_0e^{-i0} + f_1 e^{-i\f 23 \pi} + f_2 e^{-i \f 43 \pi}) = \dotsb = -\f 12\\
		c_2 &= \f 13 (f_0e^{-i0} + f_1 e^{-i \f 43 \pi} + f_2 e^{-i \f 83 \pi}) = \dotsb = -\f 12
	\end{align*}
	Das komplexe trigonometrische Interpolationspolynom ergibt sich also durch
	\[
		q(z) = 1 - \f 12 e^{iz} - \f 12 e^{i2z}
	\]
	Für den Realteil gilt
	\[
		\Re(q(z)) = 1 - \f 12 \cos(z) - \f 12 \cos(2z)
	\]
	Für die reelle Interpolierende gilt
	\[
		m = \f n2 = 1,\quad \theta =0
	\]
	Die Koeffizienten ergeben sich durch
	\begin{align*}
		a_0 &= c_0 + c_{-0} = 2c_0 = 2\\
		a_1 &= c_1 + c_{-1} = c_1 + c_2 = -1\\
		b_1 &= i(c_1-c_{-1}) = i(c_1-c_2) = 0
	\end{align*}
	Das reelle trigonometirsche Interpolationspolynom ergibt sich dann durch
	\[
		t(x) = 1 - 1 \cos(x)
	\]
	Offensichtlich ist $q(x)\neq t(x) \neq \Re(q(x))$, obwohl $q(x_k) = t(x_k) = f_k$ erfüllt ist.
\end{ex*}

\subsubsection{Diskrete Fourier Transformation}

\begin{df}[Diskrete Fourier Transformation]
	Sei $N\in \N$ und $f\in \C^N$ ein Vektor mit komplexen Funktionswerten.
	Wir nennen $\scr F_N: \C^N \to \C^N$, definiert durch
	\[
		\scr F_N(f) = \f 1N \cdot W_N \cdot f
	\]
	die \emph{diskrete Fourier-Transformation (DFT)}, wobei
	\[
		W_N = \big(w_{k}^{-j}\big)_{k,j=0}^{N-1} \in \C^{N\times N} \qquad w_{k}^j = e^{ijk \f {2\pi}N}
	\]
	\begin{note}
		\begin{itemize}
			\item
				Man findet in der Literatur verschiedene Skalierungsfaktoren, hier $\f 1N$.
			\item
				Wir indizieren Vektoren/Matrizen bei $0$ beginnend.		
			\item
				Die Beziehung zur trigonometrischen Interpolation in Satz \ref{1.24} besteht in
				\begin{align*}
					f &= (f_j)_{j=0}^n, \qquad N:= n+1
				\end{align*}
				und
				\begin{align*}
					(\scr F_n)(f))_k = \left( \f 1N W_n f\right)_k &= \f 1N \sum_{j=0}^{N-1}w_k^{-j} f_j \\
					&= \f 1{n+1} \sum_{j=0}^{N-1}w_k^{-j} f_j = c_k
				\end{align*}
				also werden die Koeffizienten $c_k$ durch die DFT erhalten:
				\[
					(c_k)_{k=0}^n = \scr F_N(f)
				\]
		\end{itemize}
	\end{note}
\end{df}

\begin{lem}[Eigenschaften]
	\begin{enumerate}[i)]
		\item
			$\displaystyle W_N$ ist regulär
		\item
			$\displaystyle W_N = W_N^T$, also $W_N$ symmetrisch
		\item
			$\displaystyle W_NW_N^{*} = N\cdot I$
		\item
			$\displaystyle \scr F_N^{-1}(c) = W^*_N\cdot c = N \cdot \_{\scr F_N(\_c)}$
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item
				$W_N$ ist eine komplexe Vandermonde-Matrix zu den paarweise verschiedenen Stellen
				\[
					x_k = e^{-ik\f {2\pi}N}
				\]
				Also ist die Matrix regulär.
			\item
				$\displaystyle W_N = (w_k^{-j}) \stackrel{\ref{1.24} i)}=  (w_j^{-k}) = W_N^T$
			\item
				$\displaystyle (W_NW_N^*)_{k,l} = \sum_{m=0}^{N-1} w_k^{-m}w_l^m = \sum_{m=0}^{N-1} w_{l-k}^m \stackrel{\ref{1.24} ii)}= N\cdot \delta_{kl} = (N\cdot I)_{k,l}$
			\item
				Wegen
				\[
					\scr F_N (W_n^* c) = \f 1N W_N W_N^* c \stackrel{iii)}= \f N N \cdot c = c
				\]
				also $\scr F_N^{-1} (c) = W_N^* c$ gilt
				\[
					N \_{\scr F_N(\_c)}
					= N \_{\left(\f {1}{N} W_N \_c\right)} 
					\stackrel{ii)}= \f{N}{N} \_{W_N^T \_c} 
					= W_N^* c 
					= \scr F_N^{-1}(c)
				\]
		\end{enumerate}
	\end{proof}
\end{lem}


\subsubsection{Schnelle Fourier-Transformation}

Der Berechnungsaufwand der DFT ist $\mathcal O(N^2)$ (für die Matrixvektormultiplikation).

Für $N=2^Q$, $Q\in \N$ lässt sich die Berechnung auf $\mathcal O(N\log N)$ beschleunigen.
Die Idee ist “Divide and Conquer”: Zerlege das Problem der Größe $N$ rekursiv in 2 Teilprobleme der Größe $\f N2$ und führe diese Lösungen zur Gesamtlösung zusammen.

Sei im Folgenden immer $N=2^Q$, $Q\in \N$ und $x=(x_i)_{i=0}^{N-1} \in \C^N$.
\newcommand{\xeven}{\ensuremath{x_{\text{even}}}}
\newcommand{\xodd}{\ensuremath{x_{\text{odd}}}}
Außerdem sei $P_N \in \C^{\N\times \N}$ eine Permutationsmatrix und $\xeven, \xodd \in \C^{\f N2}$ Teilvektoren von $x$, die jeweils nur die geraden (bzw. ungeraden) Einträge besitzen.
Sei $P_N$ so gewählt, dass folgender Zusammenhang besteht:
\[
	P_N \begin{pmatrix}x_0\\ \vdots \\ x_{N-1}\end{pmatrix}
	= \begin{pmatrix} x_0 \\ x_2 \\ \vdots \\ x_{N-2} \\ x_1 \\ x_3 \\\vdots \\ x_{N-1}\end{pmatrix}
	= \begin{pmatrix} \xeven \\ \xodd \end{pmatrix}
\]
Mit anderen Worten: $P_N$ vertauscht die Zeilen des Vektors $x$ so, dass in der ersten Hälfte erst die geraden Einträge und dann in der zweiten Hälfte die geraden Einträge auftreten.

\begin{lem}[Rekursion für $W_N$]
	\label{1.28}
	Seien $W_N$, $W_{\f N2}$ Matrizen der DFT aus , setze 
	\[
		\omega := e^{i\f {2\pi}N}, \quad D_{\f N2} := \diag(\omega^0, \dotsc, \omega^{\f N2 -1}) \in \C^{\f N2 \times \f N2}
	\]
	Dann ist
	\[
		W_N = \begin{pmatrix}W_{\f N2} & D_{\f N2}W_{\f N2} \\
			W_{\f N2} & - D_{\f N2} W_{\f N2} \end{pmatrix}
		\cdot P_N
	\]
	\begin{proof}
		Da $P_NP_N^T = I$ (da orthogonal), reicht es zu zeigen, dass
		\[
			W_NP_N^T = \begin{pmatrix}W_{\f N2} & D_{\f N2}W_{\f N2} \\
			W_{\f N2} & - D_{\f N2} W_{\f N2} \end{pmatrix}
		\]
		Wobei die Multiplikation mit $P_N^T$ ein Spaltentausch durchführt.
		Sei 
		\[
			W_NP_N^T = \begin{pmatrix}
				A_{11} & A_{12}\\
				A_{21} & A_{22}
			\end{pmatrix}
		\]
		Wir zeigen die Übereinstimmung für die einzelnen Blöcke.
		\begin{align*}
			(A_{11})_{k,j} &= (W_NP_N^T)_{k,j} \\
				&= (W_N)_{k,2j} = \omega_k^{-2j} = e^{i\f {2\pi}{\f N2}jk} = (W_{\f N2})_{k,j}\\
			(A_{21})_{k,j} &= (W_NP_N^T)_{k+\f{N}{2},j} \\
				&= (W_N)_{k+\f N2,2j} = e^{-i \f {2\pi}N (k+\f N2) 2j} = e^{-i \f {2\pi}{\f N2}kj} \cdot e^{-2\pi i j} = (W_{\f N2})_{k,j}\\
			(A_{12})_{k,j} &= (W_NP_N^T)_{k,j+\f N2} \\
				&= (W_N)_{k,2j+1} = \omega_{k}^{-(2j+1)} = \omega^{-k(2j+1)} = \omega^{-2kj} \cdot \omega^{-k} = (D_{\f N2} \cdot W_{\f N2})_{k,j}\\
			(A_{22})_{k,j} &= (W_NP_N^T)_{k+\f N2, j+\f N2} \\
				&= (W_N)_{k+\f N2, 2j +1} = \omega^{-(k+\f N2)(2j+1)} = \omega^{-2kj-k-Nj - \f N2} = \omega^{-2kj}\cdot \omega^{-jN}\cdot \omega^{-k} \omega^{-\f N2} = (-D_{\f N2}W_{\f N2})_{k,j}
		\end{align*}
	\end{proof}
\end{lem}

\begin{st}[Fast-Fourier-Transform]
	Die DFT lässt sich für $N=2^Q$ rekursiv berechnen über
	\[
		F_N(f) = \f 12 
		\begin{pmatrix}I & I \\ I & -I\end{pmatrix} 
		\begin{pmatrix} F_{\f N2}(f_{\text{even})} \\ D_{\f N2} F_{\f N2}(f_{\text{odd}})\end{pmatrix}
	\]
	Dies nennt man \emph{schnelle Fouriertransformation} (Fast Fourier Transform).
	\begin{proof}
		\begin{align*}
			\scr F_N(f) &= \f 1N W_N f 
			\stackrel{\ref{1.28}}= \f 1N \begin{pmatrix}W_{\f N2} & D_{\f N2}W_{\f N2}\\ W_{\f N2} & - D_{\f N2}W_{\f N2} \end{pmatrix} P_n f\\
			&= \f 1N \begin{pmatrix}I & I \\ I & -I\end{pmatrix}\begin{pmatrix}W_{\f N2} & 0 \\ 0 & D_{\f N2}W_{\f N2} \end{pmatrix} \begin{pmatrix} f_{\text{even}} \\ f_{\text{odd}} \end{pmatrix}\\
			&= \f 1N \begin{pmatrix}
				I & I\\ I & -I
			\end{pmatrix}
			\begin{pmatrix}
				W_{\f N2} f_{\text{even}} \\
				D_{\f N2}W_{\f N2} f_{\text{odd}}
			\end{pmatrix}\\
			&= \f 1N \begin{pmatrix}
				I & I \\ I & -I
			\end{pmatrix}
			\f N2
			\begin{pmatrix}
				\f 1{\f N2} W_{\f N2} f_{\text{even}}\\
				D_{\f N2} \f 1{\f N2} W_{\f N2} f_{\text{odd}}
			\end{pmatrix}
			= \f 12 \begin{pmatrix}
				I & I \\ I & -I
			\end{pmatrix}
			\begin{pmatrix}
				\scr F_{\f N2}(f_{\text{even}})\\
				D_{\f N2} \scr F_{\f N2}( f_{\text{odd}})
			\end{pmatrix}
		\end{align*}
	\end{proof}
\end{st}

\begin{nt*}[Aufwandsbetrachtung]
		\begin{itemize}
			\item
				Sei der Berechnungsaufwand für $\scr F_n(f)$ gegeben durch $C(N)$ und $C(1)=0$ (denn für $N=1$ ist nichts zu tun).
				Für den Aufwand gilt dann in etwa:
				\[
					C(N) = \underbrace{2C\left(\f N2\right)}_{\text{Teilprobleme der Größe $\f N2$}} + \underbrace{\f N2}_{\text{Multiplikation mit $D_{\f N2}$}} + \underbrace{\f N2}_{\text{Addition}} + \underbrace{\f N2}_{\text{Substraktion}} \le 2 C\left(\f N2\right) + K\cdot N
				\]
				für ein $K\in \R$.
				Für $N=2^Q$ folgt
				\begin{align*}
					C(N) &= 2C\l(\f N2\r) + KN = 2\l(2C\l(\f N4\r) + K\f N2\r) + KN \\
					&= 4C\l(\f N4\r) + KN + KN \\
					&= NC(1) + QKN = N + K(\log_2 N)N 
				\end{align*}
				also $\mathcal O(N\log N)$.
			\item
				Dies ist eine wesentliche Beschleunigung der DFT.
			\item
				Damit sind wesentlich größere Probleme behandelbar denn $W_N$ muss nicht aufgestellt werden.
		\end{itemize}
\end{nt*}

\begin{nt*}
	\begin{itemize}
		\item
			Bei der Interpolation von Punkten im $\R^2$ mit geschlossener Kurve ergibt sich eine $2\pi$-periodische Funktion $r(\phi)$, die den Rand eines sternförmigen Gebiets beschreibt.
		\item
			Entrauschen von Signalen (eindimensional) und Bildern (zweidimensional).
			Dazu führt man eine FFT durch und setzt $c_k= 0$ für $k$, die zu hohen Frequenzen gehören, inverse FFT.
	\end{itemize}
\end{nt*}


\subsection{Spline-Interpolation}


Sei ein kompaktes Intervall $I=[a,b]$, $a=x_0<x_1 < \dotsb < x_n = b$ und Zielwerte $\{f_j\}_{j=0}^n$.
Die normale Polynominterpolation führt häufig zu Oszillationen.
Wir suchen deshalb eine interpolierende Funktion, welche auf Teilintervallen aus unterschiedlichen Polynomen besteht, aber trotzdem eine gewissen globale Differenzierbarkeit aufweist.

\begin{df}[Spline-Räume]
	\label{1.30}
	Zu Stützstellen $a=x_0 < x_1 < \dotsb < x_n = b$ und $m\in \N$ definieren wir den Spline-Raum der Ordnung $n$ als
	\[
		S_m := \big\{ f\in C^{m-1}(I) : f\big|_{[x_{i-1},x_i]}\in \P_m\big\}
	\]
	Zu einem Spline $s\in S_m$ bezeichnen wir die Teilpolynome als
	\[
		p_i(x) = s(x)\big|_{[x_{i-1},x_i]} \in \P_m \qquad i=1,\dotsc,n
	\]
\end{df}


\section{Numerische Integration}	


\section{Nichtlineare Gleichungsysteme}

\section{Optimierung}


\end{document}

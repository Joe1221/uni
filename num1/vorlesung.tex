\documentclass[a4paper,11pt]{scrartcl}
\usepackage{mathe-vorlesung}

\title{Numerische Mathematik 1}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Approximation und Interpolation}

Sei $V= \{\Phi(x;a_0,\dotsc, a_n) \in C(\R) : a_0,\dotsc,a_n\in \R\}$ die Menge der stetigen Funktionen für $x\in \R$, deren Elemente durch Parameter $a_0,\dotsc,a_n$ parametrisiert sind.

Seien weiter Stützstellen $\{x_i\}_{i=0}^n\subset \R$ und Zielwerte $\{f_i\}_{i=0}^n \subset \R$ gegeben (z.B. Messungen oder Funktionwerte einer komplizierten (deren Auswertung kostspielig ist) Funktion $f_i=f(x_i)$, oder das Ergebnis eines Computerprogramms).

Ziel ist es, Parameter $a_0,\dotsc, a_n$ zu finden, so dass
\[
	\Phi(x_i;a_0,\dotsc, a_n) = f_i \qquad \forall i= 0,\dotsc,n \qquad \text{„Interpolation“}
\]
oder allgemeiner und obig:
\[
	\sum_{i=0}^n(\Phi(x_i; a_0,\dotsc a_m) -f_i)^2 \qquad \text{minimal}
\]
(„Least-squares-Approximation“)

\begin{ex*}
	\begin{enumerate}
		\item 
			Polynominterpolation:
			\[
				\Phi(x; a_0,\dotsc, a_n) = \sum_{k=0}^na_kx^k \in \P_n
			\]
		\item
			Trigonometrische Interpolation:
			\[
				\Phi ( x;a_0,\dots,a_m,b_1,\dots, b_m ) = \frac{a_{0}}{2} + \sum_{k=1}^m ( a_k \cos kx + b_k \sin kx )
			\]
		\item
			Spline-Interpolation:
			Sei $a=x_0\lt x_1 \lt \dotsb \lt x_n = b$, $q,r\in \N_0$, $q\le r$
			\[
			V := \{f\in \C^q([a,b]) | f_{[x_i,x_{ibm}]}\in \P_r, i=0,\dotsc,n-1\}
			\]
			(global $q$-mal stetig differenzierbare Fuktionen, stückweise polynomial vom Grad $r$)
		\item
			Exponentielle Interpolation (nichtlinear)
			\[
			\Phi(x;a_0,\dotsc, a_m,\lambda_0,\dotsc, \lambda_m) = \sum_{k=0}^m a_ke^{\lambda_k x}
			\]
		\item
			Rationale Interpolation
			\[
				\Phi(x; a_0,\dotsc, a_m, b_0,\dotsc, b_{\_m}) = \f {a_0 + a_1x + \dotsb + a_m x^m}{b_0 + b_1x + \dotsb + b_{\_m}x^{\_m}}
			\]
	\end{enumerate}
\end{ex*}

Potentielle Fragestellungen:
\begin{itemize}
	\item 
		Ist die Approximations-/Interpolationsaufgabe zu gegebenem $V$ und Daten $(x_i,f_i)$ lösbar? eindeutig?
	\item
		Wie finden wir algorithmisch die Koeffizienten?
	\item
		Können wir Fehleraussagen treffen?
	\item
		Was ist die optimale Stützstellenwahl?
	\item
		\dots
\end{itemize}


\subsection{Polynominterpolation}

Sei $\{x_i\}_{i=0}^n$, ${f_i}_{i=0}^n$ gegeben mit $x_i\neq x_j$ für $i\neq j$.

\begin{st}[Existenz und Eindeutigkeit]
	\label{1.1}
	Es existiert genau ein Polynom $p\in \P$ mit $p(x_i)=f_i$ für $i=0,\dotsc,n$.
	\begin{proof}
		$p(x) = \sum_{k=0}^n a_k x^k$ löst das Interpolationsproblem $p(x_i)=f_i$ genau dann, wenn $a=(a_k)_{k=0}^n \in \R^{n+1}$ löst $a_0 +a_1x_i^1 + \dotsb a_nx_i^n = f_i$ für $i=0,\dotsc,n$.
		Das ist äquivalent zu $Aa=f$ mit
		\[
		A= \begin{pmatrix} 1 & x_0 & x_0^2 & \hdots & x_0^n \\
		\vdots  \\
		1 & x_n & x_n^2 & \hdots & x_n^n\end{pmatrix}, f= \begin{pmatrix}f_0 \\ \vdots \\ f_n\end{pmatrix}
		\]
		Wir zeigen, dass $A$ regulär ist, bzw. $\ker(A) = \{0\}$.<br />
		Sei $\_a \in \R^{n+1}, A\_a=0$, dann erfüllt
		$\_p(x) := \sum_{k=0}^n\_a_k x^k$ die Gleichung $\_p(x_i)=0$.
		$\_p$ ist Polynom von Grad $n$ und hat $n+1$ Nullstellen. Also $\_p=0 \implies \_a_k=0 \implies \_a=0$
	\end{proof}
	\begin{note}
		\begin{enumerate}
			\item 
				Darstellung in Monombasis $p(x)= \sum_{k=0}^n a_kx^k$ heißt \emph{Normalform} des Interpolationsproblems.
			\item
				Die Matrix $A$ aus dem Beweis ist die \emph{Vandermondematrix}, typischerweis schlecht konditionirt und voll besetzt.
				Daher ist das zugehörige LGS $Aa=f$ recht aufwändig zu lösen.
			\item
				Bei Verwendung anderer Basen ist das Interpolationsproblem leichter zu lösen.
			\item
				Allgemeine lineare Interpolation:
				Sei $\{\phi_k\}_{k=0}^n$ linear unabhängig. Dann gilt
				\begin{align*}
					&p(x) = \sum_{k=0}^n a_k \phi_k(x) \text{löst die Interpolationsaufgabe}\\
					&\iff a\in \R^{n+1} \text{löst} Aa=f \text{mit} f=(f_i)_{i=0}^n \text{und} A= \begin{pmatrix}\phi_0(x_0) & \hdots & \phi_n(x_0)\\
					\end{pmatrix}
				\end{align*}
		\end{enumerate}
		
	\end{note}
\end{st}

\begin{st}[Lagrange-Form]
	\label{1.2}
	Die \emph{Lagrange-Polynome}
	\[\boxed{
		L_k^n(x) = \prod_{\substack{i=0 \\ i\neq k}}^n \f {x-x_i}{x_k-x_i} \qquad k=0,\dotsc,n
	}\]
	erfüllen
	\[
	L_k^n(x_i) = \delta_{ik} \qquad i,k=0,\dotsc, n
	\]
	und bilden eine Basis für $\P_n$ und das Interpolationspolynom hat sogenannte \emph{Lagrange-Form}
	\[
		\boxed{p(x) = \sum_{k=0}^n f_k L_k^n(x)}
	\]
	\begin{proof}
		siehe NLA.		
	\end{proof}
	\begin{note}
		\begin{enumerate}
			\item Wegen $L_k^n(x_i)=\delta_{ik}$ nennt man die $\{L_k\}_{k=0}^n$ eine nodale Basis
			\item Lösen eines LGS entfällt, da zugehörige Matrix $A=I$.
			\item $L_k^n(x)$ recht aufwändig auszuwerten
			\item Die Basen sind nicht hierarchisch, d.h. ${L_k^n} \not\subset \{L_k^{n'}\}$ für $n'\gt n$.
		\end{enumerate}
	\end{note}
\end{st}

\begin{df}[Newton-Polynome, Newton-Form]
	\label{1.3}
	Wir nennen
	\[
		\boxed{N_k^n(x) := \prod_{j=0}^{k-1}(x-x_j) \qquad k=0,\dotsc,n}
	\]
	\emph{Newton-Polynome} und die Darstellung des Interpolationspolynoms
	\[
		\boxed{p(x) = \sum_{k=0}^n a_k N_k^n(x)}
	\]
	\emph{Newton-Form} der Interpolierenden
\end{df}

\begin{lem}[Eigenschaften der Newton-Polynome]
	\label{1.4}
	\begin{enumerate}[i)]
		\item $N_k^n(x_j) = 0$ für $0\le j \le k$ 
		\item $\{N_k^n\}_{k=0}^n$ bilden eine Basis für $\P_n$
		\item Die Basen sind hierarchisch, d.h. $\{N_k^n\}\subset \{N_k^{n'}\}$ für $n'\gt n$
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item klar
			\item
				$N_k^n \in \P_n$ ist klar, zeige lineare Unabhängigkeit.
				Sei $a\in \R^{n+1}$ mit $0=\sum_{k=0}^n a_k N_k^n(x)$.
				Angenommen $a\neq 0$, wähle $k_0 := \max\{k : a_k \neq 0\}$.
				Also
				\[
					0 \neq N_{k_0}^n = - \f 1{a_{k_0}} \sum_{k=0}^{k_0-1}a_k N_k^n(x)
				\]
				Also hat $N_{k_0}$ Grad $k_0$ und ist Summe von Polynomen niedrigeren Grades, ein Widerspruch.
				Damit ist $a=0$ und $\{N_k^n\}$ linear unabhängig.
			\item klar
		\end{enumerate}
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item Die Interpolationsmatrix ist eine untere Dreiecksmatrix
			\item LGS ist durch Vorwärtseinsetzen in $\mathcal O(n^2)$ lösbar
			\item Alternative Berechnung: Dividierte Differenzen (später)
		\end{itemize}
	\end{note}
\end{lem}

\begin{ex*}
	Sei $n=2$ und folgende Datenmenge gegeben.	
	\begin{table}[h]
		\centering
		\begin{tabular}{l|rrr}
			$x_i$ & 1 & 2 & 3  \\ \hline
			$f_i$ & 2 & 3 & 6 
		\end{tabular}
	\end{table}

	Wegen $N_0^2(x) = 1, N_1^2(x) = (x-1), N_2^2(x) = (x-1)(x-2)$ ist das zugehörige LGS von der Form
	\[
	\begin{pmatrix}1&0&0\\1&1&0\\1&2&2\end{pmatrix}\begin{pmatrix}a_0\\a_1\\a_2\end{pmatrix} = \begin{pmatrix}2\\3\\6\end{pmatrix}
	\]
	Es ergibt sich $a_0=2,a_1=1, a_2=1$ und damit das Interpolationspolynom
	\[
		p(x) = 2\cdot 1 + 1\cdot(x-1) + 1\cdot(x-1)(x-2) = x^2 - 2x+3
	\]
\end{ex*}

\begin{st}[Rekursionsformel]
	\label{1.5}
	Sei $p_{ij}\in \P_j$ das Interpolationspolynom zu den Daten $(x_i,f_l)$, $l=i,\dotsc,i+j$.
	Dann gilt für $j\ge 1$
	\[
		p_{i,j}(x) = \f {(x-x_i)p_{i+1,j-1} - (x-x_{i+j})p_{i,j-1}}{x_{i+j}-x_i}
	\]
	\begin{proof}
		Es gilt
		\begin{align*}
			p_{i+1,j-1}(x_i) &= f_l \qquad l=i+1,\dotsc,i+j\\
			p_{i,j-1}(x_i) &= f_l \qquad l=i,\dotsc,i+j-1
		\end{align*}
		Mit
		\[
			q(x) := \f {(x-x_i)p_{i+1,j-1} - (x-x_{i+j})p_{i,j-1}}{x_{i+j}-x_i}
		\]
		gilt $q\in \P_j$ und die Randpunkte werden interpoliert:
		\begin{align*}
			q(x_i) &= p_{i,j-1}(x_i) = f_i\\
			q(x_{i+j}) &= p_{i+1,j-1}(x_{i+j}) = f_{i+j}
		\end{align*}
		für die Zwischenstellen $l=i+1,\dotsc,i+j-1$ gilt
		\begin{align*}
			q(x_l) &= \f {(x_l-x_i)p_{i+1,j-1}(x_l) - (x_l-x_{i+j})p_{i,j-1}(x_i)}{x_{i+j}-x_l}\\
				   &= \f {(x_l-x_i)f_l - (x_l-x_{i+j})f_l}{x_{i+j}-x_i} = f_l
		\end{align*}
		Damit ist $q(x)$ ein Interpolationspolynom und wegen der Eindeutigkeit $p_{i,j}=q$.
	\end{proof}
\end{st}

\begin{df}[Dividierten Differenzen]
	\label{1.6}
	Wir definieren rekursiv
	\begin{align*}
		f[x_i] &:= f_i\\
		f[x_i,\dotsc,x_{i+j}] &:= \f {f[x_{i+1},\dotsc,x_{i+j}] - f[x_i,\dotsc,x_{i+j-1}]}{x_{i+j}-x_i}
	\end{align*}
\end{df}

\begin{st}[Newton-Form über Dividierten Differenzen]
	\label{1.7}
	Das Interpolationspolynom hat Darstellung
	\[
		p(x) = f[x_0]N_0^n(x) + f[x_0,x_1]N_1^n(x) + \dotsb + f[x_0,\dotsc,x_n]N_n^n(x)
	\]
	\begin{proof}
		Sei die Newton-Form
		\[
			p(x) := a_0 + a_1(x-x_0) + a_2(x-x_0)(x-x_1) + \dotsb + a_n\prod_{i=0}^{n-1}(x-x_i)
		\]
		gegeben.
		Dann ist
		\[
			p_{0,k}(x) := a_0 + a_1(x-x_0) + \dotsb + a_k\prod_{i=0}^{k-1}(x-x_i)
		\]
		Interpolierende zu den Daten $(x_j,f_j)$ für $j=0,\dotsc,k$, denn
		\begin{align*}
			f_j = p(x_j) &= \underbrace{\sum_{k'=0}^ka_{k'}N_{k'}^n(x_j)}_{p_{0,k(x_j)}} + \sum_{k'=k+1}^n a_{k'}\underbrace{\prod_{i=0}^{k'-1}(xj-x_i)}_{=0}\\
			&= p_{0,k}(x_j)
		\end{align*}
		also ist $a_k$ Koeffizient vor dem höchsten Term in $p_{0,k}$.
		Sei $a_{i,j}$ der Koeffizient des höchsten Terms in $p_{i,j}$.

		Es ergibt sich mit \ref{1.5} für den höchsten Koeffizienten:
		\begin{align*}
			a_{i,j} &= \f {a_{i+1,j-1} - a_{i,j-1}}{x_{i+j}-x_i}\\
			a_{i,0} &= f_i
		\end{align*}
		Per Induktion über $j$ folgt leicht, dass
		\[
			a_{i,j} = f[x_i,\dotsc, x_{i+j}]
		\]
		also insbesondere
		\[
			a_k = a_{0,k} = f[x_0,\dotsc, x_k]
		\]
	\end{proof}
\end{st}

\subsubsection{Neville-Schema}

Die Rekursion aus \ref{1.6} kann man als Schema darstellen:

\begin{tabular}{cccc}
$x_0$ & $f[x_0]=f_0$ & $f[x_0,x_1]$ & $f[x_0,x_1,x_2]$ \\
$x_1$ & $f[x_1]=f_1$ & $f[x_1,x_2]$ &  \\
\vdots & \vdots & \vdots\\
$x_n$ & $f[x_n]=f_n$ &\\
\end{tabular}

\begin{ex*}
	Sei $n=2$.	
	\begin{tabular}{l|rrr}
		$x_i$ & 1 & 2 & 3  \\ \hline
		$f_i$ & 2 & 3 & 6 
	\end{tabular}
	
	\fixme[Neville-Schema]
	Es ergibt sich das Interpolationspolynom
	\[
		p(x) = 2+ 1(x-1) + 1(x-1)(x-2) = x^2 -2x+3
	\]
\end{ex*}

\begin{note}
	\begin{itemize}
		\item Falls nach der Interpolation ein neues Datenpaar $(x_{n+1},f_{n+1})$ zur Verfügung steht, kann das alte Neville-Schema wiederverwendet werden.
			Es muss nur unterhalb der Diagonalen jeweils ein neuer Wert berechnet werden.
		\item
			Dividierte Differenzen $f[x_i,\dotsc,x_{i+j}]$ sind symmetrisch bezüglich den Daten, d.h. bei Permutation $\tau: \{i,\dotsc,i+j\} \to \{i,\dotsc,i+j\}$ gilt
			\[
				f[x_{\tau(i)},\dotsc,x_{\tau(i+j)}] = f[x_i,\dotsc,x_{i+j}]
			\]
	\end{itemize}
\end{note}

\subsubsection{Punkterweiterung der Interpolierenden}

\begin{enumerate}[a)]
	\item 
		Falls die Newton-Form vorliegt, erlaubt das \emph{Horner-Schema} die effiziente Auswertung durch geschickte Klammerung.
		\begin{align*}
			p(x) &= a_0 + a_1(x-x_0) + \dotsb + a_n\prod_{i=0}^{n-1}(x-x_i)\\
				&= (\dotso(a_n(x-x_{n-1} + a_{n-1})(x-x_{n-2}) + a_n-2) + \dotsb + a_0
		\end{align*}
		Satt $\mathcal O(n^2)$ Produkte sind nur  $\mathcal O(n)$ Produkte erforderlich.
	\item
		Man kann das Interpolationspolynom auswerten, ohne es vorliegen zu haben.
		Verwende dazu das Neville-Schema für die Rekursion aus Satz \ref{1.5} nur für Stelle $x$.
		\begin{table}[h]
		\begin{tabular}{cccc}
		$x_0$ & $f_0=p_{0,0}(x)$ & $p_{0,1}(x)$ & $p_{0,2}(x)$ \\
		$x_1$ & $f_1=p_{1,0}(x)$ & $p_{1,1}(x)$ &  \\
		\vdots & \vdots & \vdots\\
		$x_n$ & $f_n=p_{n,0}(x)$ &\\
		\end{tabular}
		\end{table}
\end{enumerate}

\begin{ex*}
	\begin{enumerate}[a)]
		\item 
			Horner-Schema mit Newton-Form
			\[
				p(x) = (1(x-2)+1)(x-1) +2
			\]
			und damit
			\[
				f(4) = 11
			\]
		\item
			Mit dem Neville-Schema
			\begin{tabular}{cccc}
				$x_i$ & $f_i$ &  & \\ \hline
				1 & 2 & 5 & 11\\
				2 & 3 & 9\\
				3 & 6
			\end{tabular}
	\end{enumerate}
\end{ex*}

\subsubsection{Zusammenfassung}

\begin{tabular}{l|c|c}
	 & Lösen des LGS & Punktauswertung\\ \hline
	 Monombasis  & $\mathcal O(n^3)$ & $\mathcal O(n)$ via Horner \\ \hline
	 Lagrange-Basis & $\mathcal O(n)$ & $\mathcal O(n^2)$ \\ \hline
	 Newton-Basis & $\mathcal O(n^2)$ & $\mathcal O(n)$ via Horner
\end{tabular}

\subsubsection{Anwendung: Richardson-Extrapolation}

Für eine gegebene Funktion $b:(0,\infty)\to \R$ ist $\lim_{h\to 0} b(h)$ gesucht, bzw. eine Approximation.

\begin{seg}{Ansatz:}
Wähle $h_0,\dotsc, h_n \in \R^+$, berechne $b_k := b(h_k)$.
Sei $p(h)$ das Interpolationspolynom zu den Daten $(h_k,b_k)$.

Werte $p(0)$ aus als Approximation von $\lim_{h\to 0} b(h)$.
Nach vorigem Abschnitt b) ist $p(0)$ ohne Interpolationspolynom auswertbar.
Vereinfachung der Rekursion aus \ref{1.5} für $x=0$:
\begin{align*}
	p_{i,j}(0) &= \f {(0-h_i)p_{i+1,j-1}(0) - (0-h_{i+j})p_{i,j-1}(0)}{h_{i+j}-h_i} \\
	&= \f {(h_{i+j}-h_i)p_{i+1,j-1}(0)}{h_{i+j}-h_i} - \f {h_{i+j}(p_{i+1,j-1}(0)-p_{i,j-1}(0))}{h_{i+j}-h_i}\\
	&= p_{i+1,j-1}(0) + \f {p_{i+j,j-1}(0) -p_{i,j-1}(0)}{\f {h_i}{h_{i+j}} - 1}
\end{align*}
\end{seg}

\begin{df}[Richardson-Extrapolation] \label{1.8}
	Zu $\{h_i\}_{i=0}^n \subset \R^+$ definiere
	\begin{align*}
		b_i := b_{i,0} &:= b(h_i)\\
		b_{i,j} &:= b_{i+1,j-1} + \f {b_{i+1,j-1}-b_{i,j-1}}{\f {h_i}{h_{i+j}}-1}
	\end{align*}
	Dann ist $b_{0,n}$ eine Approximation für $\lim_{n\to \infty} b(h)$.
\end{df}

\begin{ex*}
	Berechnung von $e=\lim_{n\to \infty} (1 +\f 1n)^n = \lim_{n\to 0} (1+h)^{\f 1n}$.
	D.h. $b(h)=(1+h)^{\f 1h}$.
	Wähle $h_k=2^{-k} \implies b_{k,0}=b(h_k) = (1+2^{-k})^{2^k}$
	\[
		\implies b_{0,0} = 2, \qquad b_{1,0} = \f 94, \qquad b_{2,0} = \f {625}{256} \approx 2,44
	\]
	Neville-Schema für Rekursion aus 1.8, verwende $\f {h_i}{h_{i+j}} = 2^j$.
	\begin{table}[h]
		\centering
		\begin{tabular}{c|ccc}
			$h_0=1$ & $b_{0,0}=2$  & $b_{0,1}=\f 52$ & $b_{0,2}=\f {257}{96}$\\
			$h_1=\f12$ & $b_{1,0}=\f 94$  & $b_{1,1}=\f {337}{128}$ & $b_{0,2}=\f {257}{96}$\\
			$h_2=\f14$ & $b_{2,0}=\f {625}{256}$  \\
		\end{tabular}
	\end{table}
\end{ex*}

\begin{note}
	\begin{enumerate}
		\item 
			Die Richardson-Extrapolation lohnt sich insbesondere für Funktionen, deren Auswertung für kleine $h$ teuer ist, z.B. $\mathcal O(\f 1n)$.
	\end{enumerate}
\end{note}

\begin{note}
	Verbesserungsmöglichkeit bei Kenntnis der Form der asymptotischen Entwicklung.
	Falls $b(h) = \sum_{n=0}^\infty a_nh^{qn}$ für $q>1$, dann ist die Interpolation mit Polynomen angebracht, die nur diese Terme $h^{qn}$ enthalten, diese werden anschließend extrapoliert.

	Setze dazu $\_h = h^q$, d.h. $\_{h_{i}}:= h_i^q$ und
	\[
		\_b(\_h) := \sum a_n\_h^n
	\]
	Dann ist $\_b(\_h_i) = b(h_i)$.
	Führe die Richardson-Extrapolation für $\{\_h_i\}$ und $\{\_b(\_h_i)\}$ durch, dannn ist das Ergebnis eine Aproximation für
	\[
		\lim_{\_h\to 0}\_b(\_h) = \lim_{h\to 0}b(h)
	\]
\end{note}

\subsubsection{Fehleranalysis}

Wir nehmen an, die Daten $\{t_i\}_{i=0}^n$ stammen von Funktionsauswertungen $f_i=f(x_i)$.

Wie gut apporximiert die Interpolierende die Funktion?

\begin{df}
	\label{1.9}
	Sei $I\subset \R$ ein abgeschlossenes Intervall.
	Wir definieren
	\[
		C(I) = C^0(I) := \{f: I\to \R : f \text{ stetig}\}
	\]
	als Raum der stetigen Funktionen und
	\[
		C^m(I) := \{f: I\to \R : f,f',\dotsc,f^{(m)} \text{ existieren und stetig}\}
	\]
	als Raum der $m$-mal stetig differenzierbaren Funktionen ($m\in \N$) und
	\[
		C^\infty(I) := \bigcap_{m\in \N}C^m (I)
	\]
	als Raum der unendlich oft stetig differenzierbaren Funktionen.

	Für $f\in C(I)$, $I$ beschränkt, definieren wir die Supremums-Norm
	\[
		\|f\|_\infty := \|f\|_{C(I)} := \sup_{x\in I}|f(x)|
	\]
\end{df}

\begin{note}
	\begin{itemize}
		\item 
			Es gilt $C^\infty(I) \subsetneq \dotsb \subsetneq C^0(I)$.
		\item
			$(C(I), \|\cdot\|_\infty)$ ist normierter Raum für $I$ beschränkt.
		\item
			$(C(I),\|\cdot\|_\infty)$ ist vollständig, d.h. jede Cauchy-Folge konvergiert in $C(I)$.
		\item
			Eine vollständigen, normierten Vektorraum nennen wir Banachraum, $(C(I),\|\cdot\|_\infty)$ Banachraum.
	\end{itemize}
\end{note}

\begin{st}[Punktweise Interpolationsfehler]
	\label{1.10}
	Sei $f\in C^{n+1}(\R)$ und $p\in \P_n$ Interpolierende.
	\[
		p(x_i) = f_i \qquad i=0,\dotsc,n \;\land\; x_i\neq x_j \text{ für } i\neq j
	\]
	Dann gilt für alle $x\in \R$
	\[
		p(x) - f(x) = \f 1{(n+1)!}f^{(n+1)}(\xi) (x-x_0)(x-x_1)\dotsb(x-x_1)
	\]
	mit geeignetem $\xi = \xi(x)\in I = \xi \in [\min(x,x_0,\dotsc,x_n),\max(x,x_0,\dotsc,x_n)]$.

	\begin{proof}
		Für $x=x_i$ ist 
		\[
			p(x) - f(x) = \f 1{(n+1)!}f^{(n+1)}(\xi) (x-x_0)(x-x_1)\dotsb(x-x_1)
		\]
		erfüllt ($0=0$).
		Für $x\neq x_i$, $i=0,\dotsc,n$.
		Setze
		\[
			w(x) := \prod_{i=0}^n (x-x_i) \qquad \text{„Knotenpolynom“}
		\]
		und definiere $g(t):= f(t)-p(t)-\f {w(t)}{w(x)}(f(x)-p(x))$.
		
		Es gilt $g(x_i)=0$, $i=0,\dotsc,n$ (wegen $f(x_i)=p(x_i)$) und $g(x)=0$.
		Also hat $g$ mindestens $n+2$verschiedene Nullstellen in $I$.
		Nach dem Satz von Rolle hat $g'$ mindestens $n+1$ verschiedene Nullstellen, usw.
		
		$g^{(n+1)}$ hat mindestens $1$ Nullstelle $\xi\in I$. 

		Mit $p^{(n+1)}(\xi) = 0$, $w^{(n+1)}(\xi) = (n+1)!$ folgt
		\begin{align*}
			0 = g^{(n+1)}(\xi) &= f^{(n+1)}(\xi) - p^{(n+1)}(\xi) - \f {((n+1)!}{w(x)}(f(x)-p(x))\\
			&= f^{(n+1)}(\xi) - 0 - \f {((n+1)!}{w(x)}(f(x)-p(x))\\
		\end{align*}
		Also 
		\[
			f(x) -p(x) = \f 1{(n+1)!}f^{(n+1)}(\xi) w(x)
		\]
	\end{proof}
\end{st}

\begin{kor}[Fehlerschranke]
	\label{1.11}
	Für alle beschränkte Intervalle $I\subset \R$, mit $\{x_0,\dotsc,x_n\}\subset I$ gilt
	\[
		\|p-f\|_\infty \le \f 1{(n+1)!}\|w\|_\infty \|f^{(n+1)}\|_\infty
	\]
	\begin{proof}
		klar
	\end{proof}
\end{kor}

\begin{st}[Gleichmäßige Konvergenz]
	\label{1.12}
	Sei $I=[a,b]$, $f\in C^\infty(I)$ mit $\|f^{(n)}\|_\infty \le M$ für alle $n\in \N$.
	Sei $\Delta_n := \{x_0^n,\dotsc, x_n^n\}\subset I$ eine Menge $n$ disjunkter Stützstellen und $p_n\in \P_n$ eine zugehörige Interpolierende.
	Dann gilt
	\[
		\lim_{n\to \infty} \|p_n -f\| = 0
	\]
	\begin{proof}
		Für $w_n(x):=\prod_{i=0}^n(x-x_i^n)$ gilt $|w_n(x)| \le (b-a)^{n+1}$.
		Aus \ref{1.11} folgt
		\[
			\|p_n-f\|_\infty \le \f {\|w_n\|_\infty}{(n+1)!}\|f^{(n+1)}\|_\infty \le \f{(b-a)^{n+1}}{(n+1)!}M \to 0
		\]
	\end{proof}
\end{st}

\begin{ex*}
	Sei $f(x)=e^x$ auf $I=[-1,1]$, $f\in C^\infty(I)$.
	Es gilt offensichtlich
	\[
		\|f^{(n)}\| \le e \qquad n\in \N
	\]
	Selbst wenn man nur Stützstellen auf $[-1,-0.75]$ wählt, konvergiert die Folge gleichmäßig gegen die Zielfunktion.
\end{ex*}

\begin{note}
	Vergleiche Stoer-Bulirsch.
	\begin{itemize}
		\item
			ohne Einschränkungen an $(\Delta_n)_{n\in \N}$ oder $f\in C(I)$. kann man \emph{keine} gleichmäßige Konvergenz und nicht einmal punktweise Konvergenz erwarten.
			Das Problem ist, dass $f^{(n+1)}(\xi)$ kann schneller mit $n$ wachsen, als $\left(\f {w(x)}{(n+1)!}\right)^{-1}$.	
		\item
			Für jedes $f\in C(I)$ existiert Folge $(\Delta_n)_{n\in \N}$ sodass $p_n\to f$ gleichmäßig.
		\item
			Satz von Faber: Zu jeder Folge $(\Delta_n)_{n\in \N}$ gibt es ein $f\in C([a,b])$ so dass $p_n\not\to f$ gleichmäßig.
		\item
			In der Praxis kommt es zu häufig Oszillationen von $p_n$, vor allem am Rand von\\ $[ \min\{x_0,\dotsc, x_n\},\max\{x_0,\dotsc,x_n\} ]$.
	\end{itemize}
\end{note}

\begin{ex*}[Runge]
	Sei $f(x)= \f 1{1+x^2}$ mit $I=[-5,5]$ und $x_k^n := -5+k*k+h_n$ ($k=0,\dotsc,n$) mit $h_n:= \f {10}n$ äquidistanten Stützstellen.

	Sei $p_n\in \P_n$ mit $p_n(x_k^n) = f(x_k^n)$, dann ergibt sich \fixme[Oszillationen].
	
	Man kann zeigen, dass $\tilde x \approx 3,68$ existiert, so dass in $(-\tilde x,\tilde x)$ punktweise und nicht punktweise Konvergenz für $|x|\ge \tilde x$. Sogar: $\|p_n-f\|_\infty \to \infty$.
\end{ex*}

\subsubsection{Optimale Wahl von Stützstellen}

\begin{seg}{Idee}
Finde Stützstellen $\{x_0,\dotsc,x_n\}\subset I := [-1,1]$, so dass $\|w\|_\infty$ minimal wird.
Dann ist die Schranke in \ref{1.11} am kleinsten..
\end{seg}

\begin{df}[Tschebyscheff-Polynome] \label{1.13}
	Wir definieren \emph{Tschebyscheff Polynome auf $I$}, durch
	\begin{align*}
		T_0(x) &:= 1\\
		T_1(x) &:= x\\
		T_{n+1}(x) &:= 2x\cdot T_n(x) - T_{n-1}(x)\\
	\intertext{und \emph{normierte Tschebyscheff Polynome} durch}
		\hat T_n(x) &:= 2^{1-n} \cdot T_n(x)
	\end{align*}
\end{df}

\begin{ex*}
	\begin{alignat*}{2}
		T_2(x) &= 2x^2 - 1\qquad&  \hat T_2(x) &= x^2 -\f 12\\
		T_3(x) &= 4x^3 - 3x \qquad& 		\hat T_3(x) &= x^3 -  \f 34 x
	\end{alignat*}
\end{ex*}

\begin{st}
	\label{1.14}
	Für $n\in \N_0$ gilt
	\begin{enumerate}[i)]
		\item 
			$T_n\in \P_n$
		\item
			Die $\hat T_n$ sind normiert.
		\item
			Für $x\in [-1,1]$ gilt
			\[
				T_n(x) = \cos(n\cos^{-1}(x))
			\]
		\item
			$|T_n(x)| \le 1$
		\item
			$T_n(\cos(\f {j\pi}n)) = (-1)^j$ Extrema für $j=0,\dotsc,n$.
		\item
			$T_n(\cos(\pi \f {2j-1}{2n})) = 0$ Nullstellen für $j=1,\dotsc,n$.
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item klar
			\item klar
			\item
				Ein Additionstheorem liefert
				\[
					\cos(\alpha+\beta) = \cos(\alpha)\cos(\beta) - \sin(\alpha)\sin(\beta)
				\]
				Also
				\begin{align*}
					\cos((n+1)\theta) &= \cos(n\theta) \cos(\theta) - \sin(n\theta) \sin(\theta)\\
					\cos((n-1)\theta) &= \cos(n\theta) \cos(\theta) + \sin(n\theta) \sin(\theta)
				\end{align*}
				und damit
				\[
					\cos((n+1)\theta) + \cos((n-1)\theta) = 2 \cos(n\theta)\cos(\theta)
				\]
				Setze $\theta:= \cos^{-1}(x)$, dann ergibt sich
				\begin{align*}
					F_{n+1}(x) = \cos((n+1)\cos^{-1}(x)) &= 2\cos(n\cos^{-1}(x)) \cos(\cos^{-1}x) - \cos((n-1)\cos^{-1}(x))\\
					&= 2x \underbrace{\cos(n\cos^{-1}(x))}_{F_n(x)} - \underbrace{\cos((n-1)\cos^{-1}(x))}_{F_{n-1}(x)}
				\end{align*}
				d.h. $F_n(x):= \cos(n\cos^{-1}(x))$ erfüllt die Rekursion der Tschebyscheff Polynome aus Definition \ref{1.13}.
				Außerdem  $F_0(x)=1, F_1(x)=x$, also $F_n(x)=T_n(x)$.
			\item
				klar mit iii)
			\item
				Durch Nachrechnen mit iii)
			\item
				Durch Nachrechnen mit iii)
		\end{enumerate}
	\end{proof}
\end{st}

\begin{st}[Optimalität]
	\label{1.15}
	\begin{enumerate}[i)]
		\item 
			Sei $p\in \P_n$ normiert auf $[-1,+1] =: I$, dann gilt
			\[
				\|p\|_\infty \ge 2^{1-n}
			\]
		\item
			$\displaystyle \|\hat T_n \|_\infty = 2^{1-n}$
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item 
				Wir nehmen an, ein normiertes $p\in \P_n$ mit $|p(x)|<2^{1-n}$ für alle $x\in I$.
				Sei $x_j= \cos(\f {j\pi}{n})$.
				Mit \ref{1.15} und der Definition von $\hat T_n$ folgt
				\[
					(-1)^j p(x_j) \le |p(x_j)| < 2^{1-n} = 2^{1-n}\underbrace{(-1)^j}_{T_n(x_j)}(-1)^j = \hat T(x_j)(-1)^j
				\]
				und
				\[
					(-1)^j(\hat T_n(x_j)-p(x_j)) > 0 \qquad j=0,\dotsc,n
				\]
				also hat $\hat T_n -p$ mindestens $n$ verschiedene Nullstellen. $\hat T_n-p\in \P_{n-1}$, da beide normiert sind, ein Widerspruch.
				Also gilt
				\[
					|p(x)| \ge 2^{1-n} \qquad \forall p\in \P_n \text{ normiert}
				\]
			\item
				aus i) folgt wegen $\hat T_n$ normiert, dass
				\[
					\|\hat T_n\|_\infty \ge 2^{1-n}
				\]
				Mit Satz \ref{1.14} iv) ist $|T_n(x)| \le 1$ also $|\hat T_n(x)| \le 2^{1-n}$ für alle $x\in I$.
				Also 
				\[
					\|\hat T_n\|_\infty = 2^{1-n}
				\]
		\end{enumerate}
	\end{proof}
\end{st}

\begin{kor}[Optimale Stützstellen]
	\label{1.16}
	Setze $\{x_0,\dotsc,x_n\}$ als Nullstellen von den Tschebyscheff-Polynomen $T_{n+1}$:
	\[
		x_i := \cos\left(\pi\f{2i+1}{2(n+1)}\right) \qquad i=0,\dotsc,n
	\]
	Dann ist $\omega(x) := \prod_{i=0}^n (x-x_i) = \hat T_{n+1}$ Knotenpolynom mit minimalem $\|\omega\|_\infty$.
\end{kor}


\subsubsection{Stabilität der Interpolation}


Wie sensitiv ist die Interpolation bei Störungen in den Zielwerten $\{f_i\}_{i=0}^n$?.

\begin{df}[Lebesgue-Konstante] \label{1.17}
	Sei $I\subset \R$ ein kompaktes Intervall und $V\subset C(I)$ ein Unterraum der stetigen Funktionen auf $I$ mit $\dim(V)=n+1$.
	Seien paarweise verschiedene Stützstellen $\{x_0,\dotsc, x_n\}\subset I$ gegeben und $\{L_k^n(x)\}_{k=0}^n$ eine nodale Basis von $V$ (d.h. $L_k^n(x_j) = \delta_{kj}$).
	Dann nennen wir
	\[
		\Lambda_n := \max_{x\in I}\sum_{k=0}^n \big|L_k^n(x)\big|
	\]
	die \emph{Lebesgue-Konstante} der Interpolation.
\end{df}

\begin{note}
	$\Lambda_n$ ist insbesondere auch für Interpolation mit anderen Funktionen als Polynome definiert.

	Sei $V=\<\{\phi_k\}_{k=0}^n\> \subset C(I)$ mit Basis $\{\phi_k\}_{k=0}^n$.
	Falls die Interpolationsmatrix regulär ist, so existiert eine nodale Basis von $V$:
	\[
		A=\begin{pmatrix}
			\phi_0(x_0) & \hdots & \phi_n(x_0)\\
			\vdots & \ddots & \vdots\\
			\phi_0(x_n) & \hdots & \phi_n(x_n)
		\end{pmatrix}
		\qquad \text{regulär}
	\]
	Dann ist $B=(b_{ik})_{i,k=0}^n := A^{-1}$ und $L_k^n(x) := \sum_{i=0}^n b_{ik}\phi_i(x)$ wohldefiniert und
	\[
		L_k^n (x_j) = \sum_{i=0}^n b_{ik}\phi_i(x_j) = (A B)_{jk} = I_{jk} = \delta_{jk}
	\]
\end{note}

\begin{st}[Störungs-Aussage]
	\label{1.18}
	Seien $\{f_i\}_{i=0}^n$ und $\{\tilde f_i\}_{i=0}^n$ zwei Mengen von Zielwerten über Stützpunkten $\{x_i\}_{i=0}^n\subset I$ und $p$ und $\tilde p$ zugehörige Interpolationsfunktionen.
	Dann gilt
	\[
		\|p-\tilde p\|_\infty \le \Lambda_n \max_{i=0,\dotsc,n}|f_i-\tilde f_i|
	\]
	Diese Abschätzung ist scharf (d.h. $\Lambda_n$ kann nicht kleiner gewählt werden, wir finden Beispiele $\{f_i\},\{\tilde f_i\}$ für die Gleichheit gilt).
	\begin{proof}
		Zeige zunächst: $\forall \{\_{f_i}\}\subset \R$ und interpolierende Funktion $\_p\in V$, gilt:
		\[
			\|\_p\|\infty \le \Lambda_n \max_{i=0,\dotsc,n}|\_{f_i}| \qquad \text{und ist scharf}
		\]
		Für $t\in I$ gilt
		\begin{align*}
			|\_p(t)| &= \left| \sum_{i=0}^n \_{f_i} L_i^n(t)\right| \\
			&\le \sum_{i=0}^n |\_{f_i}||L_i^n(t)|\\
			&\le \sum_{i=0}^n (\max_{j=0,\dotsc,n}|\_{f_j})|L_i^n(t)|\\
			&\le \max_{j=0,\dotsc,n}|\_{f_j}| \sum_{i=0}^n|L_i^n(t)|
		\end{align*}
		Also ist
		\[
			\|\_p\|_\infty \le \Lambda_n \max_{j=0,\dotsc,n}|\_{f_j}|
		\]

		Sei $\tau := \argmax_{x\in I} \sum_{i=0}^n|L_i^n(x)|$ und $\_{f_i} := \sgn(L_i^n(\tau))$.
		Dann gilt
		\begin{align*}
			\_p(\tau) &= \left| \sum_{i=0}^n \underbrace{\_{f_i}L_i^n(\tau)}_{\ge 0}\right| \\
			&= \sum_{i=0}^n |L_i^n(\tau)| \\
			&= \max_{x\in I} \sum_{i=0}^n |L_i^n(x)| \\
			&= \Lambda_n \cdot \underbrace{1}_{=\max_{i=0,\dotsc,n}|\_{f_i}|}
		\end{align*}
		also ist 
		\[
			\|\_p\|_\infty \le \Lambda_n \max_{j=0,\dotsc,n}|\_{f_j}|
		\]
		scharf.

		Der Satz folgt, da $\_p := p-\tilde p$ das Interpolationspolynom zu den Daten $\_{f_i} := f_i - \tilde {f_i}$
	\end{proof}
\end{st}

\begin{ex*}[Polynominterpolation, vgl. Deuflhard/Hohmann]~

	\begin{table}[h]
		\centering
		\begin{tabular}{r|r|r}
			$n$ & $\Lambda_n$ für äquidistante $\Delta_n$ & $\Lambda_n$ für Tschebyscheff-Knoten \\ \hline
			5 & 3,106292 & 2,104398 \\
			10 & 29,890695 & 2,489430 \\
			15 & 512,052451 & 2,727778 \\
			20 & 10986,533993 & 2,900825
		\end{tabular}
	\end{table}		
	Also auch quantitativ führen die Tschebyscheff-Knoten zu stabilerer Interpolation.
\end{ex*}


\subsubsection{Hermite-Interpolation}

Wir wollen jetzt nicht nur Funktionswerte für die Interpolierende vorgeben, sondern auch Werte für ihre Ableitungen.

Seien $\{x_i\}_{i=0}^m \subset \R$ paarweise verschiedene Knoten und $\{f_i^{(j)}\}_{j=0}^{m_i-1}$ Zielwerte für $i=0,\dotsc, m$ (d.h. $m_i$ viele Zielwerte für $i=0,\dotsc, m$, macht $n := \sum_{i=0}^m m_i$ viele Zielwerte).

Gesucht ist ein Polynom $p\in \P_n$ mit 
\[
	p^{(j)}(x_i) =f_i^{(j)}
\]
mit $i=0,\dotsc,m$ (Stützstellen) und $j=0,\dotsc,m_i-1$ (Ableitungen an Stützstelle $i$) und 
\[
	n := \sum_{i=0}^m m_i - 1
\]
($\sum_{i=0}^m m_i$ war die Anzahl der vorgegebenen Zielwerte, also $\sum_{i=0}^m m_i -1$ der Grad des Interpolationspolynoms)

\begin{st}[Existenz und Eindeutig]
	\label{1.19}
	Es existiert ein eindeutiges Polynom $p\in \P_n$ als Lösung des Hermite-Interpolationsproblems
	\begin{proof}
		Wir verfahren ähnlich wie in \ref{1.1}.
		Zeige, dass die Interpolationsmatrix $A$ bezüglich $\{p_i\}_{i=0}^n$ regulär ist.
		\[
			A= \begin{pmatrix}
				\phi_0^{(0)}(x_0) &\hdots &\phi_n^{(0)}(x_0)\\
				\phi_0^{(1)}(x_0) &\hdots &\phi_n^{(1)}(x_0)\\
				\vdots &  & \vdots \\
				\phi_0^{(m_0)}(x_0)& \hdots &\phi_n^{(m_0)}(x_0)\\
				\phi_0^{(0)}(x_1) &\hdots &\phi_n^{(0)}(x_1)\\
				\vdots& & \vdots \\
				\phi_0^{(m_1)}(x_1)& \hdots &\phi_n^{(m_1)}(x_1)\\
				\phi_0^{(0)}(x_2) &\hdots &\phi_n^{(m_2)}(x_2)\\
				\vdots& & \vdots \\
				\phi_0^{(m_m)}(x_m) &\hdots &\phi_n^{(m_m)}(x_m)
			\end{pmatrix}
			\qquad \text{ist quadratisch}
		\]
		Wir zeigen jetzt $\ker(A) = 0$.

		Sei $A\_a = 0$ für $\_a\in \R^{n+1}$, dann erfüllt $\_p(x) := \sum_{i=0}^n \_a_i \phi_i(x)$ die Gleichung $\_p^{(j)}(x_i) = 0$ für $i=0,\dotsc,m$ und $j=0,\dotsc,m_i-1$.
		
		Also ist $(x-x_i)^{m_i}$ Teiler von $\_p$ für jedes $x_i$.
		Insgesamt ist also
		\[
			\omega(x) := \prod_{i=0}^m (x-x_i)^{m_i}
		\]
		ein Teiler von $\_p$ in $\P_n$, d.h.
		\[
			\exists q(x)\in \P_n : \_p(x) = q(x) \omega(x)
		\]
		Es gilt $\deg \_p \le n$ und $\deg \omega = \sum_{i=0}^m m_i = n+1$.
		Also muss $q(x)=0$ und damit $\_p(x) = 0$.

		Also $\_a=0$.
	\end{proof}
\end{st}

\begin{note}
	Falls nicht alle Ableitungen $p^{(j)}(x_i) =f_i^{(j)}$ für $j=0,\dotsc,m_i-1$ vorgeschrieben sind, ist die Interpolations-Aufgabe nicht immer lösbar.

	\begin{ex*}
		Sei $m=1$, $n=1$.
		Gesucht ist $p\in \P_1$ mit
		\[
			p'(x_0) = 1 \qquad \land \qquad p'(x_2) = 2
		\]
		Dazu gibt es keine Lösung (keine Existenz).

		\[
			p'(x_0) = 1 \qquad \land \qquad p'(x_2) = 1
		\]
		Hierzu sind alle $(x) = x+a$, $a\in \R$ Lösungen (keine Eindeutigkeit).
	\end{ex*}
\end{note}

\begin{st}[Fehlerdarstellung]
	\label{1.20}
	Sei $f\in C^{n+1}(I)$, $\{x_i\}_{i=0}^n\subset I$ und $p\in \P_n$ das Interpolations-Polynom mit
	\[
		p^{(j)}(x_i) = f^{(j)}(x_i) \qquad i=0,\dotsc,m \quad j=0,\dotsc,m_i-1
	\]
	Dann existiert für alle $x\in I$ ein $\xi \in I$ mit
	\[
		f(x) - p(x) = \f 1{(n+1)!} f^{(n+1)} (\xi) \omega(x)
	\]
	für $\omega(x) = \prod_{i=0}^n(x-x_i)^{m_i}$.
	\begin{proof}
		Analog zu Satz \ref{1.10}.
	\end{proof}
\end{st}

\begin{st}[Newton-Form]
	\label{1.21}	
	Das Hermite-Interpolationspolynom ist gegeben durch
	\[
		p(x) = \sum_{k=0}^n a_k \prod_{j=0}^{k-1}(x-z_j)
	\]
	mit Stützstellen
	\[
		z_j := x_i \qquad j=n_{i},\dotsc, n_{i+1}-1
	\]
	wobei $n_i = \sum_{r=0}^{i-1}m_r$.
	Mit anderen Worten: $z_j$ durchläuft die Stützstellen $x_i$, jede davon jedoch genau so oft, wie es Zielwertbedingung für diese Stützstelle hat.

	Die Koeffizienten $a_k := a_{0,k}$ für $k=0,\dotsc,n$ werden erhalten aus der Rekursion
	\begin{align*}
		a_{k,j} := \begin{cases}
			\displaystyle \qquad \f 1{j!}f_i^{(j)} 			&  \begin{aligned}i&=0,\dotsc,m \\ j&=0,\dotsc,m_i-1 \\ k&=n_{i},\dotsc, n_{i+1}-1-j\end{aligned} \\
			\displaystyle \f {a_{k+1,j-1} - a_{k,j-1}}{z_{k+j} - z_k} & \text{sonst}
		\end{cases}
	\end{align*}
\end{st}

\begin{ex*}
	Sei vorgegeben

	\begin{table}[h]
		\centering	
		\begin{tabular}{l|c|c}
			 & $x_0=1$ & $x_1 =2$ \\ \hline
			$f_i^{(0)}$ & -1 & 14 \\
			$f_i^{(1)}$ & 4 & 32 \\
			$f_i^{(2)}$ & 12 & 
		\end{tabular}		
	\end{table}

	Dann ist $m=1$, $m_0=3$, $m_1=2$, und damit $n=3+2-1=4$.
	Es werden die Stützstellen $z_0=z_1=z_2=x_0$, $z_3=z_4=x_1$ gewählt.
	Die Basispolynome sind dann
	\[
		1, (x-1), (x-1)^2, (x-1)^3, (x-1)^3(x-2)
	\]
	Bestimme die Koeffizienten mit Hilfe des Neville-Schemas

	\begin{table}[h]
		\centering	
		\begin{tabular}{l|c|c|c|c|c|c}
			$z_k$  & $a_{k,0}$ & $a_{k,1}$ & $a_{k,2}$ \\ \hline
			$z_0=1$ & $f_0^{(0)} = -1$  & $\f 1{1!}f_0^{(1)} = 4$ & $\f 1{2!} f_0^{(2)} = 6$ & $\f {11-6}{2-1}=5$ & $\f {6-5}{2-1}=1$ \\
			$z_1=1$ & $f_0^{(0)} = -1$ & $4$ 						&	$\f {15-4}{2-1} = 11$ & $\f {17-11}{2-1}=6$ \\
			$z_2=1$ & $f_0^{(0)} = -1$ & $\f {14-(-1)}{2-1} = 15$ & $\f {32-15}{2-1} = 17$ \\
			$z_3=2$ & $f_1^{(0)} = 14$ & $\f 1 {1!} f_1^{(1)} = 32$ &  \\
			$z_4=2$ & $f_1^{(0)} = 14$ & 
		\end{tabular}		
	\end{table}
	Es ergibt sich dann
	\begin{align*}
		p(x)& = -1\cdot 1 + 4\cdot (x-1) + 6\cdot(x-1)^2 + 5\cdot(x-1)^3 + 1\cdot(x-1)^3 (x-2) = x^4-2\\
		p'(x) &= 4x^3\\
		p''(x) &= 12x
	\end{align*}
\end{ex*}


\subsection{Trigonometrische Interpolation}

Für gegebenes $n\in \N$, $x_k = \f {2\pi}{n+1}k \in [0,2\pi)$, $k=0,\dotsc,n$ ($n+1$ äquidistante Stützstellen) und $\{f_k\}_{k=0}^n\subset \R$ ist eine $2\pi$-periodische Funktion der Gestalt
\[
	t(x) = \f {a_0}2 + \sum_{k=1}^n \Big(a_k \cos(kx) + b_k \sin(kx) \Big) \qquad a_k,b_k \in \R
\]
mit $t(x_k)=f_k$ gesucht.

\begin{note}
	\begin{itemize}
		\item
			Diese Darstellung ist zunächst nur für gerade $n$ sinnvoll.
			Für ungerade $n$ wird eine leichte Modifikation notwendig sein.
		\item
			Die Bestimmung von $t(x)$ wird über komplexe trigonometrische Polynome erfolgen.
	\end{itemize}
\end{note}

\begin{lem}[Reelle und komplexe Fourier-Summe]
	\label{1.22}
	Seien $\{a_k,b_k\}_{k=0}^m\subset \R$, $\{c_k\}_{k=-m}^m\subset \C$ mit $c_0=\f {a_0}2$, $c_k=\f 12(a_k-ib_k)$ und $c_{-k}=\f 12(a_k+ib_k)$ für $k=1,\dotsc, m$.

	Dann gilt
	\[
		\f {a_0}2 + \sum_{k=1}^m \Big(a_k\cos(kx) + b_k\sin(kx)\Big) = \sum_{k=-m}^m c_k e^{ikx}
	\]
	\begin{proof}
		Einsetzen und die Eulersche Formel $e^{iz}=\cos(z) +i\sin(z)$ liefert
		\begin{align*}
			\sum_{k=-m}^m c_ke^{ikx} &= \f {a_0}2 (\cos(0x) + i\sin(0x)) \\
			& \qquad + \sum_{k=1}^m \f 12 (a_k-ib_k)(\cos(kx) +i\sin(kx)) + \sum_{k=1}^m \f 12 (a_k+ib_k)(\cos(-kx)+i\sin(-kx)) \\
			&= \f {a_0}2 + \sum_{k=1}^m a_k\cos(kx) + b_k \sin(kx)
		\end{align*}
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				Die reelle Fourier-Summe ergibt also die komplexe Fouriersumme mit entsprechend gewählten $c_k$.

				Das funktioniert auch umgekehrt für eine gegebene Fouriersumme $\{c_k\}_{k=-1}^m\subset \C$ mit $c_k=\_{c_{-k}}$, $c_0\in \R$.
				Die Wahl
				\[
					a_k := c_k +c_{-k}, \qquad b_k := i(c_k-c_{-k}) \qquad k=0,\dotsc,m
				\]
				erfüllt alle Bedingungen aus Lemma \ref{1.22}.
		\end{itemize}
	\end{note}
\end{lem}

\begin{df}[Trigonometrische Polynome] \label{1.23}
	Wir definieren
	\[
		T_n := \left\{ q: \C \to\C : q(z) = \sum_{k=0}^n c_k e^{ikz}\right\}
	\]
	als den \emph{Raum der trigonometrischen Polynome von Grad $n$}.
	\begin{note}
		Wir bezeichnen den trigonometrischen Teil folgendermaßen
		\[
			\omega(z) := e^{iz}
		\]
		Die Polynomform ist jetzt besser zu erkennen:
		\[
			q(z) = \sum_{k=0}^n c_k\cdot \big(\omega(z)\big)^k
		\]
	\end{note}
\end{df}

\begin{st}[Trigonometrische Interpolation in $\C$]
	\label{1.24}
	Zu $\{f_k\}_{k=0}^n \subset \C$ existiert genau ein $q\in T_n$ mit $q(x_k) = f_k$, $k=0,\dotsc, n$.

	Die Koeffizienten sind gegeben durch
	\[
		c_k = \f 1{n+1}\sum_{j=0}^n f_j \omega_k^{-j}
	\]
	mit $\omega_k := e^{ix_k}$.
	\begin{proof}
		Satz \ref{1.1} gilt auch im Komplexen, also existiert genau ein Polynom $p(z)\in \P_n$ mit $p(z)=\sum_{k=0}^n c_kz^k$, $c_k\in \C$ und $p(\omega_k)=f_k$.

		Wir setzen $q(z) := \sum_{k=0}^n c_ke^{ikz} \in T_n$.
		$q(z)$ erfüllt die Interpolationsbedingungen:
		\[
			q(x_l) = \sum_{k=0}^n c_k e^{ikx_l} = \sum_{k=0}^n c_k \omega_l^k = p(\omega_l) = f_l \qquad l=0,\dotsc,n
		\]
		Es gilt
		\begin{enumerate}[i)]
			\item
				$\displaystyle \omega_k^{-j} = e^{-ij\f {2\pi}{n+1}k} = \omega_j^{-k}$
			\item
				$\displaystyle \sum_{j=0}^n (\omega_{l-k})^j = (n+1)\delta_{lk}$
				\begin{proof}
					\[
						\sum_{j=0}^n (\omega_{l-k})^j = \begin{cases}
							n+1 & l=k\\
							\f{(\omega_{l-k})^{n+1}-1}{\omega_{l-k}-1} & l\neq k
						\end{cases}
					\]
					Da jedoch 
					\[
						(\omega_{l-k})^{n+1} = e^{i\f {2\pi}{n+1}(l-k)(n+1)} = e^{i2\pi(l-k)} = 1
					\]
					folgt
					\[
						\sum_{j=0}^n (\omega_{l-k})^j = \begin{cases}
							n+1 & l=k\\
							0 & l\neq k
						\end{cases}
					\]
				\end{proof}
		\end{enumerate}
		Daher gilt für Koeffizienten von $q(z)$
		\begin{align*}
			\sum_{j=0}^n f_j \omega_k^{-j} &= \sum_{j=0}^n p(\omega_j) \omega_k^{-j}\\
			&= \sum_{j=0}^n\left( \sum_{l=0}^n c_l \omega_j^l\right) \omega_k^{-j}\\
			&= \sum_{j,l=0}^n c_l \omega_j^{l-k} \\
			&= \sum_{l=0}^n c_l \sum_{j=0}^n \omega_{l-k}^j\\
			&= \sum_{l=0}^n c_l (n+1) \delta_{lk} = c_k (n+1)
		\end{align*}
		Dividieren durch $n+1$ liefert die Behauptung.
	\end{proof}
\end{st}

\begin{st}[Trigonometrische Interpolation in $\R$]
	\label{1.25}
	Für $n\in \N$ setze 
	\[
		m:= \begin{cases} \f n2 & \text{falls $n$ gerade} \\
			\f {n-1}2 & \text{falls $n$ ungerade}\end{cases} 
		\qquad 
		\theta := \begin{cases} 0 & \text{falls $n$ gerade} \\ 1 & \text{falls $n$ ungerade}\end{cases}
	\]
	Zu $x_k:=\f {2\pi}{n+1}k$ und Daten $\{f_k\}_{k=0}^n\subset \R$ existiert dann genau eine Funktion
	\[
		t(x) = \f {a_0}2 + \sum_{k=1}^m \Big(a_k \cos(kx) + b_k \sin(kx)\Big) + \f{\theta}2 a_{m+1}\cos\big((m+1)x\big)
	\]
	mit $t(x_k) = f_k$, $k=0,\dotsc,n$.

	Für die Koeffizienten gilt
	\begin{align*}
		a_k &:= \f 2{n+1} \sum_{j=0}^n f_j(\cos(jx_k))\\
		b_k &:= \f 2{n+1} \sum_{j=0}^n f_j(\sin(jx_k))
	\end{align*}
	\begin{proof}
		Nach \ref{1.24} existiert ein eindeutiges $q\in T_n$ mit $q(x_k)=f_k$,
		\[
			q(z) = \sum_{k=0}^nn c_k e^{ikz}, \qquad c_k = \f 1{n+1}\sum_{j=0}^n f_j \omega_k^{-j}
		\]
		Setze $c_{-k} := c_{n+1-k}$ für $k=1,\dotsc,m$.
		Wegen
		\[
			\_{w_k} = e^{-i \f {2\pi}{n+1}k} = e^{i \f{2\pi}{n+1}(n+1-k)} = \omega_{n+1-k}
		\]
		gilt
		\[
			c_{-k} = c_{n+1-k} = \f 1{n+1}\sum_{j=0}^n f_j \omega_{n+1-k}^{-j} = \f 1{n+1}\sum_{j=0}^n f_j\_{w_k}^{-j} = \_{c_k}
		\]
		Also ist Lemma \ref{1.22} anwendbar und mit der Wahl $a_k := c_k +c_{-k}$, $b_k := i(c_k-c_{-k})$, $k=0,\dotsc, m$ gilt:
		\[
			\f{a_0}2 + \sum_{k=1}^m \Big(a_k \cos(kx) + b_k \sin(kx) \Big) = \sum_{k=-m}^m c_k e^{ikx}
		\]
		Es gilt außerdem
		\[
			f_l = \sum_{k=0}^n c_k \omega_l^k = \sum_{k=0}^m c_k \omega_l^k + \sum_{k=1}^m c_{n+1-k}\omega_l^{-k} + \theta c_{m+1} \omega_l^{m+1}
		\]
		Für $n$ ungerade ist $m+1 = \f {n+1}2$ und 
		\[
			\omega_{l}^{m+1} = e^{i(m+1)\f{2\pi}{n+1}l} = e^{i\pi l} = \cos(\pi l) = \cos\left( \f {2\pi}{n+1} l \cdot \f{n+1}2\right) = \cos((m+1)x_l)
		\]
		Zusammen mit obigem ergibt sich mit der Wahl $a_{m+1}:= 2c_{m+1}$.
		\begin{align*}
			f_l = \f {a_0}2 + \sum_{k=1}^m \Big(a_k \cos(kx_l) + b_k \sin(kx_l)\Big) + \theta \f {a_{m+1}}2 \cos((m+1)l) 
			=  t(x_l)
		\end{align*}

		Für die Koeffizienten gilt 
		\begin{align*}
			a_k:= c_k + c_{-k} &= \f 1{n+1}\sum_{j=0}^n f_j (\omega_k^{-j}+\omega_k^j) \\
			&= \f 1{n+1} \sum_{j=0}^n f_j(e^{ijx_k} + e^{ijx_k})\\
			&= \f 2{n+1} \sum_{j=0}^n f_j \cos (jx_k)
		\end{align*}
		für $b_k$ analog mit $b_k := i(c_k -c_{-k})$.
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				Es gilt zwar $t(x_l)=q(x_l)$, aber im Allgemeinen ist $t(x)\neq q(x)$ und auch $t(x)\neq \Re(q(x))$ für $x\neq x_l$.
		\end{itemize}		
	\end{note}
\end{st}

\begin{ex*}
	Sei $n=2$ und
	\begin{table}[h]
		\centering
		\begin{tabular}{c|c|c|c}
			$x_j$ & $x_0=0$ & $x_1=\f 23 \pi$ & $x_2=\f 43 \pi$\\\hline
			$f_j$ & $0$ & $\f 32$ & $\f 32$
		\end{tabular}
	\end{table}
	Es ist $\cos(\pm x_1) = \cos(\pm x_2) = -\f 12$, $\sin(x_1) = -\sin(x_2) = i3$
	\begin{align*}
		c_0 &= \f 13 (f_0e^{-i0} + f_1 e^{-i0} + f_2e^{-i0}) = 1\\
		c_1 &= \f 13 (f_0e^{-i0} + f_1 e^{-i\f 23 \pi} + f_2 e^{-i \f 43 \pi}) = \dotsb = -\f 12\\
		c_2 &= \f 13 (f_0e^{-i0} + f_1 e^{-i \f 43 \pi} + f_2 e^{-i \f 83 \pi}) = \dotsb = -\f 12
	\end{align*}
	Das komplexe trigonometrische Interpolationspolynom ergibt sich also durch
	\[
		q(z) = 1 - \f 12 e^{iz} - \f 12 e^{i2z}
	\]
	Für den Realteil gilt
	\[
		\Re(q(z)) = 1 - \f 12 \cos(z) - \f 12 \cos(2z)
	\]
	Für die reelle Interpolierende gilt
	\[
		m = \f n2 = 1,\quad \theta =0
	\]
	Die Koeffizienten ergeben sich durch
	\begin{align*}
		a_0 &= c_0 + c_{-0} = 2c_0 = 2\\
		a_1 &= c_1 + c_{-1} = c_1 + c_2 = -1\\
		b_1 &= i(c_1-c_{-1}) = i(c_1-c_2) = 0
	\end{align*}
	Das reelle trigonometirsche Interpolationspolynom ergibt sich dann durch
	\[
		t(x) = 1 - 1 \cos(x)
	\]
	Offensichtlich ist $q(x)\neq t(x) \neq \Re(q(x))$, obwohl $q(x_k) = t(x_k) = f_k$ erfüllt ist.
\end{ex*}

\subsubsection{Diskrete Fourier Transformation}

\begin{df}[Diskrete Fourier Transformation] \label{1.26}
	Sei $N\in \N$ und $f\in \C^N$ ein Vektor mit komplexen Funktionswerten.
	Wir nennen $\scr F_N: \C^N \to \C^N$, definiert durch
	\[
		\scr F_N(f) = \f 1N \cdot W_N \cdot f
	\]
	die \emph{diskrete Fourier-Transformation (DFT)}, wobei
	\[
		W_N = \big(w_{k}^{-j}\big)_{k,j=0}^{N-1} \in \C^{N\times N} \qquad w_{k}^j = e^{ijk \f {2\pi}N}
	\]
	\begin{note}
		\begin{itemize}
			\item
				Man findet in der Literatur verschiedene Skalierungsfaktoren, hier $\f 1N$.
			\item
				Wir indizieren Vektoren/Matrizen bei $0$ beginnend.		
			\item
				Die Beziehung zur trigonometrischen Interpolation in Satz \ref{1.24} besteht in
				\begin{align*}
					f &= (f_j)_{j=0}^n, \qquad N:= n+1
				\end{align*}
				und
				\begin{align*}
					(\scr F_n)(f))_k = \left( \f 1N W_n f\right)_k &= \f 1N \sum_{j=0}^{N-1}w_k^{-j} f_j \\
					&= \f 1{n+1} \sum_{j=0}^{N-1}w_k^{-j} f_j = c_k
				\end{align*}
				also werden die Koeffizienten $c_k$ durch die DFT erhalten:
				\[
					(c_k)_{k=0}^n = \scr F_N(f)
				\]
		\end{itemize}
	\end{note}
\end{df}

\begin{lem}[Eigenschaften] \label{1.27}
	\begin{enumerate}[i)]
		\item
			$\displaystyle W_N$ ist regulär
		\item
			$\displaystyle W_N = W_N^T$, also $W_N$ symmetrisch
		\item
			$\displaystyle W_NW_N^{*} = N\cdot I$
		\item
			$\displaystyle \scr F_N^{-1}(c) = W^*_N\cdot c = N \cdot \_{\scr F_N(\_c)}$
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item
				$W_N$ ist eine komplexe Vandermonde-Matrix zu den paarweise verschiedenen Stellen
				\[
					x_k = e^{-ik\f {2\pi}N}
				\]
				Also ist die Matrix regulär.
			\item
				$\displaystyle W_N = (w_k^{-j}) \stackrel{\ref{1.24} i)}=  (w_j^{-k}) = W_N^T$
			\item
				$\displaystyle (W_NW_N^*)_{k,l} = \sum_{m=0}^{N-1} w_k^{-m}w_l^m = \sum_{m=0}^{N-1} w_{l-k}^m \stackrel{\ref{1.24} ii)}= N\cdot \delta_{kl} = (N\cdot I)_{k,l}$
			\item
				Wegen
				\[
					\scr F_N (W_N^* c) = \f 1N W_N W_N^* c \stackrel{iii)}= \f N N \cdot c = c
				\]
				also $\scr F_N^{-1} (c) = W_N^* c$ gilt
				\[
					N \_{\scr F_N(\_c)}
					= N \_{\left(\f {1}{N} W_N \_c\right)} 
					\stackrel{ii)}= \f{N}{N} \_{W_N^T \_c} 
					= W_N^* c 
					= \scr F_N^{-1}(c)
				\]
		\end{enumerate}
	\end{proof}
\end{lem}


\subsubsection{Schnelle Fourier-Transformation}

Der Berechnungsaufwand der DFT ist $\mathcal O(N^2)$ (für die Matrixvektormultiplikation).

Für $N=2^Q$, $Q\in \N$ lässt sich die Berechnung auf $\mathcal O(N\log N)$ beschleunigen.
Die Idee ist “Divide and Conquer”: Zerlege das Problem der Größe $N$ rekursiv in 2 Teilprobleme der Größe $\f N2$ und führe diese Lösungen zur Gesamtlösung zusammen.

Sei im Folgenden immer $N=2^Q$, $Q\in \N$ und $x=(x_i)_{i=0}^{N-1} \in \C^N$.
\newcommand{\xeven}{\ensuremath{x_{\text{even}}}}
\newcommand{\xodd}{\ensuremath{x_{\text{odd}}}}
Außerdem sei $P_N \in \C^{\N\times \N}$ eine Permutationsmatrix und $\xeven, \xodd \in \C^{\f N2}$ Teilvektoren von $x$, die jeweils nur die geraden (bzw. ungeraden) Einträge besitzen.
Sei $P_N$ so gewählt, dass folgender Zusammenhang besteht:
\[
	P_N \begin{pmatrix}x_0\\ \vdots \\ x_{N-1}\end{pmatrix}
	= \begin{pmatrix} x_0 \\ x_2 \\ \vdots \\ x_{N-2} \\ x_1 \\ x_3 \\\vdots \\ x_{N-1}\end{pmatrix}
	= \begin{pmatrix} \xeven \\ \xodd \end{pmatrix}
\]
Mit anderen Worten: $P_N$ vertauscht die Zeilen des Vektors $x$ so, dass in der ersten Hälfte erst die geraden Einträge und dann in der zweiten Hälfte die geraden Einträge auftreten.

\begin{lem}[Rekursion für $W_N$]
	\label{1.28}
	Seien $W_N$, $W_{\f N2}$ Matrizen der DFT aus , setze 
	\[
		\omega := e^{i\f {2\pi}N}, \quad D_{\f N2} := \diag(\omega^0, \dotsc, \omega^{-\l(\f N2 -1\r)}) \in \C^{\f N2 \times \f N2}
	\]
	Dann ist
	\[
		W_N = \begin{pmatrix}W_{\f N2} & D_{\f N2}W_{\f N2} \\
			W_{\f N2} & - D_{\f N2} W_{\f N2} \end{pmatrix}
		\cdot P_N
	\]
	\begin{proof}
		Da $P_NP_N^T = I$ (da orthogonal), reicht es zu zeigen, dass
		\[
			W_NP_N^T = \begin{pmatrix}W_{\f N2} & D_{\f N2}W_{\f N2} \\
			W_{\f N2} & - D_{\f N2} W_{\f N2} \end{pmatrix}
		\]
		Wobei die Multiplikation mit $P_N^T$ ein Spaltentausch durchführt.
		Sei 
		\[
			W_NP_N^T = \begin{pmatrix}
				A_{11} & A_{12}\\
				A_{21} & A_{22}
			\end{pmatrix}
		\]
		Wir zeigen die Übereinstimmung für die einzelnen Blöcke.
		\begin{align*}
			(A_{11})_{k,j} &= (W_NP_N^T)_{k,j} \\
				&= (W_N)_{k,2j} = \omega_k^{-2j} = e^{i\f {2\pi}{\f N2}jk} = (W_{\f N2})_{k,j}\\
			(A_{21})_{k,j} &= (W_NP_N^T)_{k+\f{N}{2},j} \\
				&= (W_N)_{k+\f N2,2j} = e^{-i \f {2\pi}N (k+\f N2) 2j} = e^{-i \f {2\pi}{\f N2}kj} \cdot e^{-2\pi i j} = (W_{\f N2})_{k,j}\\
			(A_{12})_{k,j} &= (W_NP_N^T)_{k,j+\f N2} \\
				&= (W_N)_{k,2j+1} = \omega_{k}^{-(2j+1)} = \omega^{-k(2j+1)} = \omega^{-2kj} \cdot \omega^{-k} = (D_{\f N2} \cdot W_{\f N2})_{k,j}\\
			(A_{22})_{k,j} &= (W_NP_N^T)_{k+\f N2, j+\f N2} \\
				&= (W_N)_{k+\f N2, 2j +1} = \omega^{-(k+\f N2)(2j+1)} = \omega^{-2kj-k-Nj - \f N2} = \omega^{-2kj}\cdot \omega^{-jN}\cdot \omega^{-k} \omega^{-\f N2} = (-D_{\f N2}W_{\f N2})_{k,j}
		\end{align*}
	\end{proof}
\end{lem}

\begin{st}[Fast-Fourier-Transform] \label{1.29}
	Die DFT lässt sich für $N=2^Q$ rekursiv berechnen über
	\[
		F_N(f) = \f 12 
		\begin{pmatrix}I & I \\ I & -I\end{pmatrix} 
		\begin{pmatrix} F_{\f N2}(f_{\text{even}}) \\ D_{\f N2} F_{\f N2}(f_{\text{odd}})\end{pmatrix}
	\]
	Dies nennt man \emph{schnelle Fouriertransformation} (Fast Fourier Transform).
	\begin{proof}
		\begin{align*}
			\scr F_N(f) &= \f 1N W_N f 
			\stackrel{\ref{1.28}}= \f 1N \begin{pmatrix}W_{\f N2} & D_{\f N2}W_{\f N2}\\ W_{\f N2} & - D_{\f N2}W_{\f N2} \end{pmatrix} P_n f\\
			&= \f 1N \begin{pmatrix}I & I \\ I & -I\end{pmatrix}\begin{pmatrix}W_{\f N2} & 0 \\ 0 & D_{\f N2}W_{\f N2} \end{pmatrix} \begin{pmatrix} f_{\text{even}} \\ f_{\text{odd}} \end{pmatrix}\\
			&= \f 1N \begin{pmatrix}
				I & I\\ I & -I
			\end{pmatrix}
			\begin{pmatrix}
				W_{\f N2} f_{\text{even}} \\
				D_{\f N2}W_{\f N2} f_{\text{odd}}
			\end{pmatrix}\\
			&= \f 1N \begin{pmatrix}
				I & I \\ I & -I
			\end{pmatrix}
			\f N2
			\begin{pmatrix}
				\f 1{\f N2} W_{\f N2} f_{\text{even}}\\
				D_{\f N2} \f 1{\f N2} W_{\f N2} f_{\text{odd}}
			\end{pmatrix}
			= \f 12 \begin{pmatrix}
				I & I \\ I & -I
			\end{pmatrix}
			\begin{pmatrix}
				\scr F_{\f N2}(f_{\text{even}})\\
				D_{\f N2} \scr F_{\f N2}( f_{\text{odd}})
			\end{pmatrix}
		\end{align*}
	\end{proof}
\end{st}

\begin{nt*}[Aufwandsbetrachtung]
		\begin{itemize}
			\item
				Sei der Berechnungsaufwand für $\scr F_n(f)$ gegeben durch $C(N)$ und $C(1)=0$ (denn für $N=1$ ist nichts zu tun).
				Für den Aufwand gilt dann in etwa:
				\[
					C(N) = \underbrace{2C\left(\f N2\right)}_{\text{Teilprobleme der Größe $\f N2$}} + \underbrace{\f N2}_{\text{Multiplikation mit $D_{\f N2}$}} + \underbrace{\f N2}_{\text{Addition}} + \underbrace{\f N2}_{\text{Substraktion}} \le 2 C\left(\f N2\right) + K\cdot N
				\]
				für ein $K\in \R$.
				Für $N=2^Q$ folgt
				\begin{align*}
					C(N) &= 2C\l(\f N2\r) + KN = 2\l(2C\l(\f N4\r) + K\f N2\r) + KN \\
					&= 4C\l(\f N4\r) + KN + KN \\
					&= NC(1) + QKN = N + K(\log_2 N)N 
				\end{align*}
				also $\mathcal O(N\log N)$.
			\item
				Dies ist eine wesentliche Beschleunigung der DFT.
			\item
				Damit sind wesentlich größere Probleme behandelbar denn $W_N$ muss nicht aufgestellt werden.
		\end{itemize}
\end{nt*}

\begin{nt*}
	\begin{itemize}
		\item
			Bei der Interpolation von Punkten im $\R^2$ mit geschlossener Kurve ergibt sich eine $2\pi$-periodische Funktion $r(\phi)$, die den Rand eines sternförmigen Gebiets beschreibt.
		\item
			Entrauschen von Signalen (eindimensional) und Bildern (zweidimensional).
			Dazu führt man eine FFT durch und setzt $c_k= 0$ für $k$, die zu hohen Frequenzen gehören, inverse FFT.
	\end{itemize}
\end{nt*}


\subsection{Spline-Interpolation}


Sei ein kompaktes Intervall $I=[a,b]$, $a=x_0<x_1 < \dotsb < x_n = b$ und Zielwerte $\{f_j\}_{j=0}^n$.
Die normale Polynominterpolation führt häufig zu Oszillationen.
Wir suchen deshalb eine interpolierende Funktion, welche auf Teilintervallen aus unterschiedlichen Polynomen besteht, aber trotzdem eine gewissen globale Differenzierbarkeit aufweist.

\begin{df}[Spline-Räume]
	\label{1.30}
	Zu Stützstellen $a=x_0 < x_1 < \dotsb < x_n = b$, $I=[a,b]$ und $m\in \N$ definieren wir den Spline-Raum der Ordnung $n$ als
	\[
		S_m := \big\{ f\in C^{m-1}(I) : f\big|_{[x_{i-1},x_i]}\in \P_m\big\}
	\]
	Zu einem Spline $s\in S_m$ bezeichnen wir die Teilpolynome als
	\[
		p_i(x) = s(x)\big|_{[x_{i-1},x_i]} \in \P_m \qquad i=1,\dotsc,n
	\]
\end{df}

\begin{nt*}
	\begin{itemize}
		\item
			Die Anzahl der Freiheitsgrade beträgt $n(m+1)$ für $n$ unabhängige Teilpolynome von Grad $m$.

			Für $n-1$ innere Punkte gibt es jeweils $m$ Bedingungen, nämlich
			\[
				p_j^{(k)}(x_j) = p_{j+1}^{(k)}(x_j) \qquad j=1,\dotsc,n-1
			\]
			Damit bleiben $n(m+1)-(n-1)m = n+m$ Freiheitsgrade für Interpolationspolynome aus $S_m$.
			Es gilt
			\[
				\dim (S_m) = n+m
			\]
		\item
			Bei der Interpolation sind $n+1$ Bedingungen gegeben:
			\[
				s(x_j) = f_j \qquad j=0,\dotsc,n
			\]
			Es fehlen also noch $\dim (S_m) - (n+1) = m-1$ Bedingungen.
		\item
			Üblicherweise wählt man deshalb noch zusätzliche Bedingungen für die Ableitungen an den Endpunkten.
		\item
			Bei linearen Splines (aus $S_1$, $m=1$) sind die Interpolationsbedingungen ausreichend (siehe \ref{1.31}).
	\end{itemize}
\end{nt*}

\begin{st}[Existenz und Eindeutigkeit von linearen Splines]
	\label{1.31}
	Für $m=1$ existiert genau ein $s\in S_1$ mit $s(x_j) = f_j$ für $j=0,\dotsc,n$.
	Es hat die explizite Gestalt
	\[
		s(x) \big|_{[x_{j-1},x_j]} = p_j(x) = f_j \f {x-x_{j-1}}{x_j - x_{j-1}} + f_{j-1}\f {x-x_j}{x_{j-1}-x_j}
		\qquad j=1,\dotsc,n
	\]
	\begin{proof}
		Auf jedem Teilintervall ist die lineare Interpolation eindeutig durch die obige Lagrange-Form festgelegt.
		Also auch für das gesamte Spline.
	\end{proof}
\end{st}

\subsubsection{Kubische Splines}

Bei kubischen Splines ($m=3$) fehlen 2 Bedingungen.
In diesem Fall gibt es folgende Möglichkeiten
\begin{enumerate}[{(R}1{)}]
	\item
		Natürliche Splines
		\[
			p_1''(a) = p_n''(b)=0
		\]
		Mit anderen Worten: keine Krümmung an den Randpunkten
	\item
		Hermite Randbedingungen
		\[
			p_1''(a) = \alpha, \quad p_n'(b) = \beta \qquad \alpha,\beta\in \R
		\]
	\item
		Periodische Splines
		\[
			p_1'(a) = p_n'(b), \quad p_1''(a) = p_n''(b)
		\]
		Dieser Begriff ist nur sinnvoll, falls $p_1(a) = p_n(b)$, die Kurve also geschlossen ist.
\end{enumerate}

\begin{st}[Existenz und Eindeutigkeit kubischer Splines]
	\label{1.32}
	Für jede Wahl von obigen Randbedingungen (R1) bis (R3) existiert genau ein $s\in S_3$, welches die Randbedingungen (R1),(R2) bzw. (R3) erfüllt und gilt $s(x_i)=f_i$ für $i=0,\dotsc,n$.
	\begin{proof}
		Für (R1) sei die Darstellung des Splines gegeben durch
		\[
			s(x)\big|_{[x_{j-1},x_j]} = p_j(x) = a_j(x-x_{j-1})^3 + b_j(x-x_{j-1})^2 + c_j(x-x_{j-1}) + d_j
		\]
		Die Bedingungen sind
		\begin{alignat*}{5}
			p_j(x_{j-1}) &= f_{j-1}& \quad p_j(x_j) &= f_j & \qquad j&=1,\dotsc,n & &\qquad \text{(zusammenhängend)} &\\
			p_j'(x_j) &= p_{j+1}'(x_j) & \quad p_j''(x_j) &= p_{j+1}''(x_j) &  \qquad j&=1,\dotsc,n-1 & &\qquad \text{(glatt)} &\\
			p_1''(x_0)&=0& \quad p_n''(x_n) &= 0 & & & &\qquad \text{(Randbedingung)} &
		\end{alignat*}
		Sei $h_j := x_j - x_{j-1}$.
		Verwende nun $M_j = s''(x_j)$ für $j=0,\dotsc,n$ als neue Variablen.
		Es gilt
		\begin{align*}
			p_j'(x) &= 3a_j(x-x_{j-1})^2 + 2b_j(x-x_{j-1})+c_j \\
			p_j''(x) &= 6a_j(x-x_{j-1}) + 2b_j
		\end{align*}
		$b_j$ ist eindeutig durch $M_j$ bestimmt:
		\begin{align*}
			p_j''(x_{j-1}) 
				&= 2b_j 
				= M_{j-1}  \\
				&\implies b_j
					=\f {M_{j-1}}2 \\
			p_j''(x_j) 
				&= 6a_j(x_j-x_{j-1}) + 2b_j 
				= M_j \\
				&\implies a_j 
					= \f {M_j - 2b_j}{6h_j} 
					= \f {M_j - M_{j-1}}{6h_j} \\
			p_j(x_{j-1}) 
				&= d_j 
				= f_{j-1} \\
			p_j(x_j) 
				&= a_j h_j^3 + b_jh_j^2 + c_jh_j + d_j 
				= f_j \\
				&\implies c_j 
					= \f{f_j-d_j}{h_j} - a_jh_j^2 - b_jh_j 
					= \f {f_j-f_{j-1}}{h_j} - h_j\left(\f {M_j}6 + \f{M_{j-1}}3 \right)
		\end{align*}
		Es reicht also die Eindeutigkeit der $M_j$ zu zeigen, da $a_j,b_j,c_j,d_j$ durch $M_j$ eindeutig bestimmt sind.

		Durch (R1) ist $M_0$ festgelegt:
		\[
			M_0 = M_n = 0
		\]
		Es gilt weiter
		\begin{align*}
			p_j'(x_j) &= 3a_jh_j^2 + 2b_jh_j + c_j \\
			&= \f {M_j-M_{j-1}}2 h_j + h_j M_{j-1} + \f {f_j - f_{j-1}}h_j - h_j \l( \f {M_j}6 + \f{M_{j-1}}3\r) \\
			&= \f {f_j-f_{j-1}}h_j + h_j \l( \f{M_j}3 - \f{M_{j-1}}6\r) \qquad j=1,\dotsc,n \\
			p_{j+1}'(x_j) &= c_{j+1} = \f {f_{j+1}-f_j}{h_{j+1}} - h_{j+1}\left( \f{M_{j+1}}6 + \f {M_j}33\right) \stackrel != p_j'(x_j) \qquad j=1,\dotsc,n-1
		\end{align*}
		Wir erhalten also ein lineares Gleichungssystem mit $n-1$ für $n-1$ Unbekannte $M_1,\dotsc,M_{n-1}$:
		\[
			\f {h_j}6 M_{j-1} + \f {h_j + h_{j+1}}3 M_j + \f{h_{j+1}}6 M_{j+1} = \f {f_{j-1}-f_j}{h_{j+1}} - \f{f_j-f_{j-1}}{h_j}
		\]
		Multipliziert mit $6$ und mit $M_0=M_n=0$, ergibt sich
		\[
			\begin{pmatrix}
				2(h_1+h_2) & h_2 &  \cdots & 0 \\
				h_2 & 2(h_2+h_3) & \cdots  & 0 \\
				\vdots & \ddots & \ddots & \vdots\\
				0 & \cdots & h_{n-1} & 2(h_{n-1}+h_n)
			\end{pmatrix}
			\begin{pmatrix}
				M_1 \\ \vdots \\ M_{n-1}
			\end{pmatrix}
			=
			\begin{pmatrix}
				\f 6{h_2}(f_2-f_1) - \f 6{h_1}(f_1-f_0)\\
				\vdots \\
				\f 6{h_n}(f_n-f_{n-1})- \f 6{h_{n-1}}(f_{n-1}-f_{n-2})
			\end{pmatrix}
		\]
		Die Matrix ist sogar streng diagonaldominant, positiv definit und symmetrisch also regulär.
		Damit ist das LGS eindeutig Lösbar.
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				Der Beweis war konstruktiv.
			\item
				Das Umschreiben des Systems durch zusätzliche Variablen $M_j$ als Tridiagonalsystem ist eine große Vereinfachung.
				Damit müssen nur $n-1$ Gleichungen mit jeweils nur $3$ gekoppelten Unbekannten lösen (vorher waren das $4n$ Unbekannte und bis zu $5$ gekoppelte Variablen pro Gleichung).
			\item
				Die Lösung eines Tridiagonalsystems ist sehr effizient ($\mathcal O(n)$) lösbar durch eine LR-Zerlegung mit tridiagonalen unteren (bzw. oberen) Dreiecksmatrizen $L$ und $R$ und anschließender Vorwärts und Rückwärtssubstitution.
		\end{itemize}
	\end{note}
\end{st}

\begin{st}[Optimalität]
	\label{1.33}
	Sei $s\in S_3$ eine interpolierender Spline zu den Stützstellen $x_i$ und den Funktionswerten $f_i$ ($i=0,\dotsc,n$)
	und eine der Randbedingungen (R1)-(R3) erfüllt.

	Dann gilt für alle $f\in C^2([a,b])$ mit denselben Bedingungen (d.h. $f(x_i)=f_i$ und eine der Randbedingungen (R1,R2,R3)):
	\[
		\int_a^b |f''(x)|^2 dx \ge \int_a^b |s''(x)|^2  dx
	\]
	\begin{note}
		Dies hat eine Physikalische Interpretation.
		Spline bedeutet soviel wie „biegsamer Stab“ und wird mit Nägeln sozusagen in den Punkten $(x_j,f_j)$ befestigt.
		Der Verlauf $f(x)$ des Splines minimiert die Biegeenergie
		\[
			E = \alpha \int_{x_0}^{x_n}\f {(f''(x))^2}{(1+f'(x)^2)^3} dx \qquad \alpha \text{ Materialkonstante mit Einheiten}
		\]
		Falls $f'(x)$ klein ist, kann dies durch
		\[
			\int_{x_0}^{x_n} (f''(x))^2 dx
		\]
		approximiert werden.
		Damit approximieren kubische Splines den Verlauf eines eingespannten Stabes.
	\end{note}
	\begin{proof}
		Sei $f\in C^2([a,b])$ eine zu interpolierende Funktion.
		Dann gilt
		\begin{align*}
			\int_a^b (f-s)''(x) s''(x) dx &=  \sum_{j=1}^n \int_{x_{j-1}}^{x_j} (f-s)''(x)s''(x)) dx \\
			&= \sum_{j=1}^n \Big( \big[(f-s)'(x)s''(x)\big]_{x_{j-1}}^{x_j} - \int_{x_{j-1}}^{x_j} (f-s)'(x))s'''(x) dx \Big)\\
			&= \underbrace{\big( f'(b) - s'(b)\big) s''(b) - \big( f'(a)-s'(a)\big) s''(a)}_{=0 \text{ wegen } \begin{cases}
				s''(a) = s''(b) = 0 & \text{falls (R1)} \\
				f'(b) = s'(b) = \beta, f'(a) = s'(a) = \alpha  &\text{falls (R2)} \\
				f'(a) = f'(b), s'(a)=s'(b), s''(a)=s''(b) & \text{falls (R3)}
				\end{cases}} + ??
			\\
		\end{align*}
		\fixme[Was sollen die ??]\\
		Es folgt also
		\begin{align*}
			\int_a^b (f-s)''(x) s''(x) dx = \sum_{j=1}^n \underbrace{\big(-[(f-s)(x)p_j'''(x)]\big)_{x_{j-1}}^{x_j}}_{=0 (f(x_j)=s(x_j)} + \underbrace{\int_{x_{j-1}}^{x_j} (f-s)(x)s^{(k)}(x)}_{=0}
		\end{align*}
		Insegsamt gilt dann
		\begin{align*}
			\int_a^b |s''(x)|^2 dx &= \int_a^b (s''(x) + \underbrace{(f-s)''(x)) s''(x)}_{=0} dx \\
			&= \int_a^b f''(x) s''(x) \\
			\stackrel{Cauchy-Schwarz}\le \sqrt{\int_a^b (f''(x))^2 dx} \sqrt{\int_a^b (s''(x))^2 dx} \\
		\end{align*}
		Division ergibt dann
		\begin{align*}
			\sqrt{\int_a^b (s''(x))^2 dx} \le \sqrt{\int_a^b (f''(x))^2 dx}
		\end{align*}
		Quadrieren ergibt die Behauptung.
	\end{proof}
\end{st}

\begin{st}[Fehleraussage und Konvergenz]
	\label{1.34}
	Sei $f\in C^4([a,b])$, $f_i=f(x_i)$, $\{x_i\}_{i=0}^n$ aquidistant mit $\f {b-a}n$.
	Sei $s\in S_3$ mit (R2), d.h. $s'(a)=f'(a)$ und $s'(b)=f'(b)$.
	Dann gilt für $l=0,1,2,3$ 
	\[
		\|f^{(l)} - s^{(l)}\|_\infty \le 2 \|f^{(4)}\|_\infty \cdot h^{4-l}
	\]
	also insbesondere
	\[
		\|f-s\|_\infty \le 2\|f^{(n)}\|_\infty \cdot h^4
	\]
	\begin{proof}
		Siehe Stoer-Bulirsch Thm. 2.5.3.3.
	\end{proof}
	\begin{note}
		Es gilt Konvergenz für jedes $f\in C^4$ im Gegensatz zur Polynominterpolation.
	\end{note}
\end{st}

Kann man auf einfache Weise eine Basis für $S_m$ angeben?

\begin{df}[B-Splines]
	\label{1.35}
	Sei $\{x_i\}_{i\in \Z}$ eine streng monoton wachsende Folge in $\R$ mit $\lim_{i\to \pm \infty} x_i = \pm \infty$.
	Dann sind B-Splines $B_{i,k}:\R\to \R$ von Grad $k\in \N_0$ rekursiv definiert durch
	\[
		B_{i,0} := \begin{cases}
			1 & x_i < x \le x_{i+1} \\
			0 & \text{sonst}
		\end{cases}
	\]
	und
	\[
		B_{i,k}(x) := \omega_{i,k}(x) B_{i,k-1}(x) + \big(1-\omega_{i+1,k}(x)\big)B_{i+1,k-1}(x)
	\]
	mit
	\[
		\omega_{i,k}(x) := \f {x-x_i}{x_{i+k}- x_i}
	\]
\end{df}

\begin{ex*}
	\begin{align*}
		B_{0,0}(x) &= 1\big|_{(x_0,x_1]} \\
		B_{1,0}(x) &= 1\big|_{(x_0,x_2]} \\
		B_{0,1}(x) &= \begin{cases}
			\f {x-x_0}{x_1-x_0} & \text{auf $(x_0,x_1]$} \\
			1 - \f {x-x_1}{x_2-x_1} & \text{auf $(x_1,x_2]$} \\
			0 & \text{sonst}
		\end{cases}
	\end{align*}
\end{ex*}

\begin{st}[Eigenschaften]
	\label{1.36}
	\begin{enumerate}[i)]
		\item
			$\displaystyle B_{i,k}\big|_{(x_j,x_{j+1})} \in \P_k \qquad \forall i,j\in \Z, k\in \N_0$
		\item
			$\displaystyle B_{i,k} \in C^{k-1}(\R)$
		\item
			$\displaystyle \supp (B_{i,k}) \subset [x_i,x_{i+k+1}]$
			Dabei ist der „Träger“ $\supp (f)$ definiert als
			\[
				\supp(f) := \_{\{x\in A : f(x) \neq 0\}}
			\]
		\item
			$\displaystyle B_{i,k} \ge 0$
		\item
			$\displaystyle \sum_{i\in \Z} B_{i,k} = 1 \qquad$ (Zerlegung der Eins)
		\item
			$\displaystyle \{B_{i,m}\big|_{[x_0,x_n]}\}_{i=-m}^{n-1}$ bilden eine Basis von $S_m$.
	\end{enumerate}
	\begin{proof}
		Leichte Übung
	\end{proof}
\end{st}


\subsection{Über- und Unterbestimmte Systeme}


\begin{itemize}
	\item
		Was ist eine gute Funktionsapproximation, falls die Anzahl der Bedingungen ungleich der Anzahl der Freiheitsgrade ist?
	\item
		Äquivalent dazu: Wie löst man unterbestimmte oder überbestimmte lineare Gleichungssysteme
		\[
			Ax=b \qquad A\in \R^{m\times n}, x\in \R^n, b\in \R^m
		\]
	\item
		Aus der linearen Algebra gibt es drei bekannte Fälle: eindeutige Lösung, keine Lösung, undendlich viele Lösungen.
		Gibt es in all diesen Fällen eine eindeutige, ausgezeichnete „Lösung“?
\end{itemize}

Die “Minimum Norm Least Squares”-Approximation liefert eine Antwort auf diese Fragen.

\begin{st}[Singulärwertzerlegung (SVD)]
	\label{1.37}
	Sei $A\in \C^{m\times n}$, dann existieren unitäre Matrizen $U\in \C^{m\times m}$, $V\in \C^{n\times n}$ und ein reelle Diagonalmatrix
	\[
		\Sigma := \diag( \sigma_1, \dotsc, \sigma_r)  \in \R^{m\times n}
	\]
	mit $\sigma_1\ge \sigma_2 \ge \dotsb \sigma_r \ge 0$ und $r:= \min(m,n)$ so, dass
	\[
		A = U \Sigma V^*
	\]
	Wir nennen dies Singulärwertzerlegung (Singular Value Decomposition) und $\sigma_i$ Singulärwerte.
	\begin{note}
		\begin{itemize}
			\item
				Falls $A$ reell ist, sind auch $U,V$ reell, also orthogonal und es gilt
				\[
					A=U\Sigma V^T
				\]
			\item
				Erinnerung aus der linearen Algebra:
				Falls $A \in \R^{n\times n}$ symmetrisch und positiv semidefinit, dann existiert $V$ orthogonal, $D$ diagonal mit $A=VDV^T$.

				Also ist die Diagonalisierung in Eigenwerte eine SVD.
			\item
				Die SVD ist sogar allgemeiner als die Eigenwertzerlegung:
				für eine Jordanmatrixmatrix
				\[
					A = \begin{pmatrix}
						-1 & 1 \\
						0 & -1
					\end{pmatrix}
				\]
				existiert nämlich keine Eigenwertzerlegung, aber durchaus eine Singulärwertzerlegung, nämlich:
				\[
					U = \f 1{\sqrt{1+\phi^2}} \begin{pmatrix}
						\phi & 1 \\
						-1 & \phi
					\end{pmatrix}, \qquad
					\Sigma = \begin{pmatrix}
						\phi & 0 \\
						0 & \phi -1
					\end{pmatrix}, \qquad 
					V = \f 1{\sqrt{1+\phi^2}} \begin{pmatrix}
						-1 & -\phi \\
						\phi & - 1
					\end{pmatrix}
				\]
				für $\qquad \phi := \f {1+\sqrt 5}2$.
			\item
				Die Matrix $\Sigma$ ist eindeutig. 
				$U$ und $V$ sind es jedoch nicht (wechsle simultan das Vorzeichen in den Zeilen/Spalten von $U,V$).
			\item
				Falls $k := \rg(A) < \min(n,m)$, dann sind genau die ersten $k$ Singulärwerte ungleich Null, die restlichen verschwinden.
				Wichtig sind dann nur der linke obere Kasten von $\Sigma$, der linke Block von $U$ und der obere Block von $V$.
				Mann kann dann $A$ mit
				\[
					\tilde \Sigma = \diag(\sigma_1,\dotsc,\sigma_k) \in \R^{k\times k}
					, \qquad 
					\tilde U \in \C^{m\times k}
					, \qquad 
					\tilde V \in \C^{n\times k}
				\]
				schreiben als
				\[
					A = \tilde U \tilde \Sigma \tilde V^*
				\]
				Man nennt diese Darstellung \emph{verkürzte Singulärwertzerlegung} mit wesentlich kleineren Matrizen, also deutlich weniger Speicheraufwand.
				Die Matrix-Vektor-Multiplikation ist mit $\mathcal O(nk+mk)$ statt $\mathcal O(n^2 + m^2)$ möglich (\fixme[Ist das so richtig?]).
		\end{itemize}
	\end{note}
	\begin{proof}
		Die Matrix $A^*A \in \C^{n\times n}$ ist hermitesch (also $(A^*A)^*=A^*A$).
		Also existiert eine Eigenwert-Zerlegung (mit reellen Eigenwerten) von $A^*A$:
		\[
			A^*A = V\Lambda V^*
		\]
		mit $V=(v_1,\dotsc, v_n)\in \C^{n\times n}$ unitär, $\Lambda = \diag(\lambda_1,\dotsc,\lambda_n) \in \R^{n\times n}$, $\lambda_1\ge \dotsc \ge \lambda_n$.
		Die $v_i$ sind damit Eigenvektoren von $A^*A$ zu den Eigenwerten $\lambda_i$.

		Zeige, dass alle $\lambda_i \ge 0$:
		\[
			\lambda_i = \lambda_i \|v_i\|^2 = \lambda_iv_i^*v_i = v_i^* \lambda_i v_i = v_i^* A^*A v_i = (A v_i)^*(A v_i) =  \|Av_i\|^2 \ge 0
		\]
		Sei $k$ die Anzahl an Eigenwerten ungleich Null (also $\lambda_i \neq 0$ für $i=1,\dotsc,k$ und $\lambda_i=0$ für $i=k+1,\dotsc,n$).
		Setze 
		\[
			\sigma_i := \begin{cases}
				\sqrt{\lambda_i} & \text{für $i=1,\dotsc,k$} \\
				0 & \text{für $i=k+1,\dotsc, r$}
			\end{cases}
			\qquad i = 1,\dotsc,r
		\]
		und $\Sigma := \diag(\sigma_1,\dotsc,\sigma_r) \in \R^{m\times n}$.
		Setze 
		\[
			u_i := \f 1{\sqrt{\lambda_i}}Av_i
			\qquad i = 1,\dotsc, k
		\]
		Offensichtlich ist $u_i\in \im(A)$.
		Außerdem sind die $u_i$ sind orthonormal, denn:
		\[
			\<u_i,u_j\> 
			= u_i^*u_j 
			= \f 1{\sqrt{\lambda_i \lambda_j}} v_i^* A^*Av_j
			= \f 1{\sqrt{\lambda_i\lambda_j}}v_i^* \lambda_j v_j
			= \f {\lambda_j} {\sqrt{\lambda_i \lambda_j}}\cdot \delta_{ij} 
			= \delta_{ij}
		\]
		also auch linear unabhängig.

		Zeige $\ker A = \ker (A^*A)$.
		„$\subset$“ ist klar.
		Für die Umkehrung sei $x\in \ker (A^* A)$.
		\[
			A^*A x = 0 \implies x^*A^*Ax = 0 \implies \|Ax\|^2 = 0 \implies Ax=0
		\]
		Daher
		\begin{align*}
			\dim(\im A) 
			&= n - \dim (\ker A)) 
			= n-\dim (\ker(A^*A))  \\
			&\quad = n- (n- \dim(\im(A^*A))) 
			= n-(n-k) = k
		\end{align*}
		Also bilden die $\{u_i\}_{i=1}^k$ eine Orthonormalbasis von $\im A$.

		Wähle $\{u_{k+1},\dotsc, u_n\}$ als Orthonormalbasis von $\im( A)^\orth \subset \C^m$.
		Definiere 
		\[
			U := (u_1,\dotsc, u_m)
		\]
		$U$ ist offensichtlich unitär.

		Außerdem gilt mit dieser Wahl $A=U\Sigma V^*$:
		\[
			U\Sigma V^* v_i = U \Sigma e_i = U \sigma_i e_i = \begin{cases}
				\sigma_i u_i & i=1,\dotsc,k \\
				0 & i =k+1,\dotsc,n
			\end{cases}
		\]
		Für $i=1,\dotsc,k$ gilt
		\[
			\sigma_i u_i = \sqrt{\lambda_i} \cdot \f 1{\sqrt{\lambda_i}}Av_i = Av_i
		\]
		Für $i=k+1,\dotsc, n$ gilt wegen $v_i \in \ker (A^*A) = \ker (A)$
		\[
			0 = Av_i
		\]
		Damit ist
		\[
			U\Sigma V^* x = Ax \qquad \forall x\in \C^n
		\]
		Also $A= U\Sigma V^*$.
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				Der Beweis ist konstruktiv und liefert damit ein Verfahren zur erzeugung einer SVD.
			\item
				$\displaystyle \im A = \<u_1,\dotsc,u_k\>$
			\item
				$\displaystyle \im A^* = \<v_1,\dotsc, v_k\>$
			\item
				$\displaystyle \ker A = \<v_{k+1},\dotsc, v_n\>$
			\item
				$\displaystyle \ker A^* = \<u_k,\dotsc, u_{k+1}\>$
		\end{itemize}
	\end{note}
\end{st}

\begin{st}[Niedrigrang-Matrixbestapproximation]
	\label{1.38}
	Sei $A=U\Sigma V^* \in \C^{m\times n}$ eine SVD von $A$ mit $\Sigma = \diag(\sigma_1,\dotsc, \sigma_r) \in \C^{m\times n}$ und $r:=\min\{m,n\}$.

	Definiere für $k=1,\dotsc,r-1$:
	\[
		\hat\Sigma_k := \diag\{\sigma_1,\dotsc,\sigma_k,0,\dotsc,0\} \in \R^{m\times n}, \qquad \hat A_k := U \hat \Sigma_k V^*
	\]
	Dann gilt
	\[
		\min_{\substack{B\in \C^{m\times n} \\ \rg(B) \le k}} \|A-B\| = \|A- \hat A_k\| = \sigma_{k+1}
	\]
	Mit anderen Worten: $\hat A_k$ ist diejenige Matrix mit Rang $\le k$, die $A$ am besten approximiert.
	\begin{proof}
		Sei $V=(v_1,\dotsc, v_n)$, $k\in \{1,\dotsc,r-1\}$, $B\in \C^{m\times n}$, $\rg(B) =  k' \le k$.
		Dann gilt
		\[
			\dim(\ker B) = n - k'
		\]
		Dann ist $Z:= \ker(B) \cap \<v_1,\dotsc,v_{k'+1}\> \neq \{0\}$.
		Wähle $0\neq z \in Z$ mit $\|z\|=1$, dann lässt sich $z$ schreiben als
		\[
			z = \sum_{i=1}^{k'+1}a_i v_i
		\]
		Da $v_1,\dotsc,v_{k'+1}$ orthonormal ist folgt außerdem $\sum_{i=1}^{k'+1}|a_i|^2 = 1$.
		Setze $a:=(a_1,\dotsc, a_{k'+1},0,\dotsc,0)^T \in \C^n$.
		Dann gilt
		\begin{align*}
			\|A-B\|^2 &= \sup_{x\neq 0} \f {\|(A-B)x\|^2}{\|x\|^2}
			\ge \f {\|(Az-\overbrace{Bz}^{=0}\|^2}{\underbrace{\|z\|^2}_{=1}}
			= \|Az\|^2 
			= \l\|U\Sigma V^*\sum_{i=1}^n a_i v_i \r\|^2 \\
			&= \|U\Sigma V^* Va \|^2
			= \|U\Sigma a\|^2 
			\stackrel{\text{Rotationsinvarianz}}= \|\Sigma a\|^2  
			= \l\|\l(\begin{smallmatrix}\sigma_1 a_1 \\ \vdots \\ \sigma_{k'+1}a_{k'+1} \\ 0 \\ \vdots \\ 0\end{smallmatrix}\r)\r\| \\
			&= \sum_{i=1}^{k'+1} \sigma_i^2 \cdot |a_i|^2 
			\ge  \sum_{i=1}^{k'+1}\sigma_{k'+1}^2 |a_i|^2 
			= \sigma_{k'+1}^2 \sum_{i=1}^{k'+1} |a_i|^2 
			= \sigma_{k'+1}^2 
			\ge \sigma_{k+1}^2
		\end{align*}
		Andererseits wird dieser Fehler auch realisiert durch $\hat A_k$:
		\begin{align*}
			\|A-\hat A_k\| &= \|U\Sigma V^* - U\hat \Sigma_k V^*\| = \|U(\Sigma - \hat \Sigma_k) V^*\| \stackrel{\text{Rotationsinvarianz}}= \|\Sigma - \hat \Sigma_k\|\\
			&= \|\diag\{0,\dotsc, 0, \sigma_{k+1},\dotsc, \sigma_r\}\|
			= \sigma_{k+1}
		\end{align*}
	\end{proof}
\end{st}

\begin{df}[Pseudoinverse]
	\label{1.39}	
	Zu $A\in \C^{m\times n}$ mit SVD $A=U\Sigma V^*$ definieren wir die \emph{Pseudoinverse}
	\[
		A^+ := V \Sigma^+ U^* \in \C^{n\times m}
	\]
	mit
	\[
		\Sigma^+ := \diag\{\sigma_1^+,\dotsc, \sigma_r^+\} \in \R^{n\times m}
		,\qquad \sigma_i^+ := \begin{cases}
			\f 1{\sigma_i}	& \sigma_i \neq 0 \\
			0 & \sigma_i = 0
		\end{cases}
	\]
\end{df}

\begin{st}[Penrose-Bedingungen]
	\label{1.40}
	Eine Matrix $B\in \C^{n\times m}$ ist die Pseudoinverse von $A$ genau dann wenn gilt:
	\begin{enumerate}[i)]
		\item
			$ABA = A$
		\item
			$BAB = B$
		\item
			$(AB)^* = AB$
		\item
			$(BA)^* = BA$
	\end{enumerate}
	\begin{proof}
		\begin{seg}{$A^+$ erfüllt die Bedingungen}
			Sei $A=U\Sigma V^*$, $A^+ := V\Sigma^+U^*$.
			\begin{enumerate}[i)]
				\item
					Es gilt
					\[
						AA^+A = U\Sigma V^* V \Sigma^+ U^* U \Sigma V^* = U\Sigma V^* = A
					\]
					denn
					\[
						\Sigma \Sigma^+ \Sigma = \Sigma
					\]
					(betrachte die Diagonalkomponenten: $\sigma_i \sigma_i^+ \sigma_i = \sigma_i$)
				\item
					Analog zu i).
				\item
					Es gilt
					\begin{align*}
						AA^+ &= U \Sigma V^* V \Sigma^+ U^* = U \diag\{\underbrace{1,\dotsc,1}_{k\text{-mal}},0,\dotsc, 0\} U^*\\ &= \big(U \diag\{1,\dotsc,1,0,\dotsc,0\} U^*\big)^* = (AA^+)^*
					\end{align*}
				\item
					Analog zu iii).
			\end{enumerate}
		\end{seg}
		\begin{seg}{$B$ erfüllt die Bedingungen $\implies$ $B=A^+$}
			Zeige zunächst $BA=A^+A$
			\begin{align*}
				BA &\stackrel{\text{iv)}}= (BA)^* = A^*B^* = (AA^+A)^*B^* = A^*(A^+)^*A^*B^*\\ &= (A^+A)^*(BA) \stackrel{\text{iii) + iv)}}= A^+ABA \stackrel{\text{i)}} = A^+A
			\end{align*}
			$AB=AA^+$ verläuft analog.
			Daraus folgt
			\[
				A^+ \stackrel{\text{ii)}}= A^+AA^+ = A^+AB = BAB \stackrel{\text{ii)}}= B 
			\]
		\end{seg}
	\end{proof}
\end{st}

\begin{st}[Eigenschaften]
	\label{1.41}
	\begin{enumerate}[i)]
		\item
			Ist $A$ quadratisch, dann gilt:
			\[
				\det A \neq 0 \implies A^+ = A^{-1}
			\]
		\item
			$(A^+)^+ = A$
		\item
			$(A^*)^+ = (A^+)^*$
		\item
			Im Allgemeinen gilt \emph{nicht} $(AB)^+ = B^+A^+$:
			\[
				(AB)^+ \neq B^+A^+
			\]
		\item
			$A^*(AA^*)^+ = A^+ = (A^*A)^+ A^*$
		\item
			Falls $A$ vollen Spaltenrang hat, dann ist
			\[
				A^+ = (A^*A)^{-1}A^*
			\]
		\item
			Falls $A$ vollen Zeilenrang hat, dann ist
			\[
				A^+ = A^*(AA^*)^{-1}
			\]
	\end{enumerate}
	\begin{proof}
		Übung
	\end{proof}
\end{st}

\begin{nt*}[Orthogonale Projektion]
	\begin{itemize}
		\item
			Wir nennen lineares $P: \C^m \to V$ \emph{orthogonale Projektion} auf den Unterraum $V\subset \C^m$, falls
			\[
				\<y- Py,z\> = 0 \qquad \forall z\in V, \forall y\in \C^m
			\]
			(„Orthogonalität des Projektionsfehlers“)
		\item
			Man kann zeigen (Übung)
			\begin{enumerate}[i)]
				\item
					$P$ ist idempotent:
					\[
						P = P^2
					\]
				\item
					$\Id - P$ ist eine orthogonale Projektion auf $V^\orth$.
				\item
					Pythogoras:
					\[
						\|y-z\|^2 = \|y-Py\|^2 + \|Py-z\|^2 
						\qquad \forall y\in \C^m,\forall z\in V
					\]
				\item
					$P_y$ ist eine Bestapproximation von $y$ auf $V$:
					\[
						\|y-Py\|^2 = \min_{z\in V}\|y-z\|^2
					\]
			\end{enumerate}
	\end{itemize}
\end{nt*}

\begin{lem}
	\label{1.42}
	Die Multiplikation mit
	\begin{enumerate}[i)]
		\item
			$AA^+$ ist eine orthogonale Projektion auf $\im A$.
		\item
			$I-AA^+$ ist eine orthogonale Projektion auf $\im (A)^\orth = \ker A^*$.
		\item
			$A^+A$ ist eine orthogonale Projektion auf $\im A^*$.
		\item
			$I-A^+A$ ist eine orthogonale Projektion auf $\im (A^*)^\orth = \ker A$.
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item
				Zeige $\<y-AA^+y, z\>$ für $z\in \im A$ und $y\in \C^m$.
				Sei also $z=Ax$ mit $x\in \C^n$, dann gilt
				\[
					\<y-AA^+y,Ax\> = \<A^*(y-AA^+y),x\> = \<A^*y - A^*AA^+y,x\> = 0
				\]
				da $A^*AA^+ = A^*(AA^+)^* = A^*(A^+)^* A^* = (AA^+A)^* = A^*$
			\item
				Nach obiger Bemerkung ii)
			\item
				Nach i) ist $A^*(A^*))^+$ orthogonale Projektion auf $\im (A^*)$ und
				\[
					A^*(A^*)^+ = A^*(A^+)^* = (A^+A)^* = A^+A
				\]
			\item
				Nach obiger Bemerkung ii)
		\end{enumerate}
	\end{proof}
\end{lem}

\begin{st}[Minimum-Norm-Least-Squares-Approximation]
	\label{1.43}
	Sei $A\in \R^{m\times n}, b\in \R^m$.
	Setze
	\[
		M := \{ \tilde x\in \R^n : \|Ax-B\|^2 = \min_{x\in \R^n}\|Ax-b\|^2\}
	\]
	als Menge der \emph{Least-Squares-Approximation} zum linearen Gleichungssystem $Ax=b$.
	Dann ist $x^* := A^+b \in M$ und für alle $\tilde x\in M, \tilde x \neq x^*$ gilt
	\[
		\|x^*\| < \| \tilde x \|
	\]
	Also ist $x^*$ die eindeutige \emph{Minimum-Norm-Least-Squares-Approximation (MNLS)} zum LGS $Ax=b$.
	\begin{proof}
		Sei $P: \R^m \to \im A$ eine orthogonale Projektion auf dem Bild von $A$, siehe \ref{1.42}.
		Setze $y:= P_b$, dann ist
		\[
			\|y-b\| = \min_{z\in \im A}\|z-b\|o = \min_{x\in \R^n}\|Ax-b\|
		\]
		Dann lässt sich $M$ darstellen als
		\[
			M := \{ \tilde x \in \R^m : A\tilde x = y\}
		\]
		also ein lineare Unterraum von $\R^m$.
		Weiter ist $x^* \in M$
		\[
			Ax^* = AA^+ b = Pb = y
		\]
		und
		\[
			M = \{x^*\} + \ker A
		\]
		Nach \ref{1.42} ist $A^+A$ eine orthogonale Projektion auf $\im A^*$ und 
		\[
			A^+Ax^* = A^+AA^+ b = A^+b = x^*
		\]
		also ist $x^* \in \im A^* = \ker( A)^\orth$.

		Sei $\tilde x \in M$, $\tilde x \neq x^*$, dann ist
		\[
			\|\tilde x\|^2 = \|x^* + (\tilde x - x^*)\|^2 = \|x^*\| + 2\< x^*,\underbrace{\tilde x - x^*}_{\in \ker A}\> + \underbrace{\|\tilde x - x^*\|^2}_{>0} > \|x^*\|^2
		\]
	\end{proof}
\end{st}

\subsubsection{Anwendung: Ausgleichsrechnung}

Such $p\in \P_{n-1}$ mit $p(x_i)=f_i$, $i=1,\dotsc,m$.
Es ergibt sich das lineare Gleichungssystem $Aa = f$.
Definiere $a^* := A^+f$.

\begin{seg}{$m=n$ (Polynominterpolation)}
	Es gilt in diesem Fall
	\[
		\|Aa^*-f\| = 0, \qquad \ker A = \{0\}, \qquad M = \{a^*\}, \qquad a^* = A^{-1}f
	\]
\end{seg}
\begin{seg}{$m>n$}
	Es gilt
	\[
		\|Aa^*-f\| > 0, \qquad \ker A = \{0\}, \qquad M = \{a^*\}
	\]
\end{seg}
\begin{seg}{$m<n$}
	Es gilt
	\[
		\|Aa^*-f\| = 0, \qquad \ker A \supsetneq \{0\}, \qquad \_a \in M = a^* + \ker A \implies \|a^*\| \le \|\_a\|
	\]
	Wir erhalten eine mehrdeutige Least-Squares-Lösung, aber eine eindeutige Minimum-Norm-Least-Squares-Lösung.
\end{seg}
\begin{nt*}
	Falls $m\ge n$ und $A$ vollen Spaltenrang hat, nennt man
	\[
		A^TAx = A^Tb
	\]
	\emph{Gauß'sche Normalengleichungen}
	Die MNLS-Approximation $x:=A^+b$ erfüllt genau diese Normalengleichungen.
	\begin{proof}
		Mit \ref{1.41} vi) ist $x=A^+b = (A^TA)^{-1}A^Tb$, also
		\[
			A^TAx = A^TA(A^TA)^{-1}A^T b = A^Tb
		\]
	\end{proof}
\end{nt*}



\section{Numerische Integration}	



Seien Stützstellen $\{x_i\}_{i=0}^n \subset [a,b]$ und Werte $\{f(x_i)\}_{i=0}^n$ einer Funktion $f:[a,b]\to \R$ gegeben.

Gesucht ist eine Approximation für das Integral
\[
	I(f) = \int_a^b f(x) \;dx
\]
Anwendungsgebiete sind beispielsweise:
\begin{itemize}
	\item
		Integration von Funktionen, deren Stammfunktionen nicht analytisch vorliegt, z.B.
		\[
			\int_a^b e^{-x^2} dx
		\]
	\item
		Integration von Funktionen, die nicht vollständig bekannt sind: Messungen oder Computer-Routinen.
	\item
		Numerisches Lösungen von gewöhnlichen oder partiellen Differentialgleichungen (Numerik 2 und Folgende)
\end{itemize}

\begin{df}[Quadratur]
	\label{2.1}
	Ein Funktional $I_n : C([a,b]) \to \R$ der Form
	\[
		I_n(f) = \sum_{j=0}^n \omega_j f(x_j)
	\]
	heißt \emph{Quadraturformel} mit Stützstellen $\{x_j\}_{j=0}^n$ und Gewichten $\{\omega_j\}_{j=0}^n$.
\end{df}

\begin{ex*}
	\begin{seg}{Mittelpunktsregel ($n=0$)}
		\[
			x_0 := \f {a+b}2, \qquad \omega_0 := b-a
		\]				
	\end{seg}
	\begin{seg}{Trapezregel ($n=1$)}
		\[
			x_0 := a, x_1 := b \qquad \omega_0 := \omega_1 := \f {b-a}2
		\]				
	\end{seg}
	\begin{seg}{Simpsonregel ($n=2$)}
		\[
			I_2(f) = \f {b-a}6 \l( f(a) + 4f\l(\f {a+b}2\r) + f(b) \r)
		\]
	\end{seg}
\end{ex*}

\begin{df}[Exaktheit]
	\label{2.2}
	Eine Quadratur heiß \emph{exakt} auf $\P_k$, falls
	\[
		I_n(p) = I(p) \qquad \forall p\in \P_k
	\]
\end{df}

\begin{nt*}
	\begin{itemize}
		\item
			Eine Quadratur ist linear, d.h. für alle $f,g$ und $\alpha,\beta \in \R$, gilt
			\[
				I_n ( \alpha f + \beta g) = \alpha I_n(f) + \beta I_n(g)
			\]
		\item
			Zum Nachweis der Exaktheit, reicht es
			\[
				I_n(x^k) = I(x^k) \qquad k\in \N
			\]
			nachzuweisen.
		\item
			\begin{itemize}
				\item
					Die Mittelpunktsregel ist exakt auf $\P_1$.
				\item
					Die Trapezregel ist exakt auf $\P_1$.
				\item
					Die Simpsonregel ist exakt auf $\P_3$.
				\item
					$I_n$ ist exakt auf $\P_0$ genau dann, wenn
					\[
						\sum_{i=0}^n \omega_i = b-a
					\]
			\end{itemize}
	\end{itemize}
\end{nt*}


\subsection{Interpolatorische Quadraturen}

Integriere das Interpolationspolynom $p$ zu den Daten $(x_i,f(x_i))$ für $i=0,\dotsc,n$.

Mit Hilfe der Lagrange-Darstellung von $p$:
\[
	p(x) = \sum_{j=0}^n f(x_j) \cdot L_j^n (x) \qquad 
	\text{mit} 
	\qquad L_j^n(x) := \prod_{\substack{k=0\\k\neq j}}^n \f{x-x_k}{x_j-x_k}
\]
Wir integrieren:
\[
	\int_a^b p(x) dx = \sum_{j=0}^n f(x_j) \cdot \underbrace{\int_a^b L_j^n (x) dx}_{=:\omega_j} =: I_n(f)
\]
Also $I_n(f) = I(p)$.

\begin{df}[Interpolatorische Quadratur]
	\label{2.3}
	Eine Quadratur $I_n$ ist eine \emph{Interpolatorische Quadratur}, falls die Gewichte durch die Lagrange-Polynome gegeben sind:
	\[
		\omega_j := \int_a^b L_j^n \;dx
		\qquad \text{mit} 
		\qquad L_j^n(x) := \prod_{\substack{k=0\\k\neq j}}^n \f{x-x_k}{x_j-x_k}
	\]
\end{df}

\begin{ex*}
	Sei $[a,b] = [0,1]$, $n=2$, $x_0=0, x_1=\f 12, x_2 = 1$.
	Dann ist
	\begin{align*}
		\omega_0 &= \int_0^1 \f {(x-\f 12) (x-1)}{(0-\f 12)(0-1)} \; dx = \int_0^1 2x^2 - 3x+1 \; dx = \f 23 - \f 32 + 1 = \f 16 \\
		\omega_1 &= \int_0^1 \f {(x-0)(x-1)}{(\f 12 - 0)(\f 12 -1)} \; dx = -4 \int_0^1x^2 - x \; dx = -4 \l( \f 13 - \f 12 \r) = \f 23 \\
		\omega_2 &= \int_0^1 \f {(x-0)(x-\f 12)}{(1-0)(1-\f 12)} \; dx = \int_0^1 2x^2 -x \; dx = \f 23 - \f 12 = \f 16
	\end{align*}
	Damit lautet die Quadraturformel
	\[
		I_2(f) = \f 16 \l( f(0) + 4f\l(\f 12\r) + f(1)\r)
	\]
\end{ex*}

\begin{st}[Exaktheit von Interpolatorischen Quadraturen]
	\label{2.4}
	Eine Interpolatorische Quadratur $I_n$ ist exakt auf $\P_n$.
	\begin{proof}
		Sei $p\in \P_n$, dann ist
		\begin{align*}
			I(p) = \int_a^b p(x) dx &= \int_a^b \sum_{j=0}^n p(x_j) L_j^n(x) dx \\
			&= \sum_{j=0}^n p(x_j) \int_a^b L_j^n(x) dx = I_n(p)
		\end{align*}
	\end{proof}
\end{st}

\begin{st}
	\label{2.5}
	Zu Stützstellen $\{x_j\}_{j=0}^n \subset [a,b]$ paarweise verschieden, existiert genau eine Quadratur, welche exakt auf $\P_n$ ist.
	\begin{proof}
		Existenz ist nach \ref{2.4} klar: die Interpolatorische Quadratur tut's.
		Zeige jetzt die Eindeutigkeit:

		Sei $I_n(f) = \sum_{j=0}^n \omega_j f(x_j)$ exakt auf $\P_n$.
		Für Lagrange-Polynome gilt $L_j^n \in \P_n$, also
		\[
			\int_{a}^b \prod_{\substack{k=0\\k\neq j}}^n \f {x-x_k}{x_j-x_k} \; dx \\\
			= \int_a^b L_j^n (x) dx = I(L_j^n) = I_n(L_j^n) = \sum_{i=0}^n \omega_i L_j^n(x_i) = \omega_j
		\]
	\end{proof}
	also ist $I_n$ schon die interpolatorische Quadratur.
\end{st}

\begin{st}
	\label{2.6}
	Sei $I_n(f) = \sum_{j=0}^n \omega_j f(x_j)$ eine Interpolatorische Quadratur mit
	\[
		x_j - a = b - x_{n-j} \qquad j=0,\dotsc,n
	\]
	Dann gilt
	\begin{enumerate}[i)]
		\item
			$\omega_j = \omega_{n-j}$ , d.h. die Interpolatorische Quadratur ist symmetrisch.
		\item
			falls $n$ gerade, so ist $I_n$ exakt auf $\P_{n+1}$.
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item
				Setze $\_{I_n} := \sum_{j=0}^n \omega_{n-j}f(x_j)$.
				Es reicht zu zeigen, dass $\_{I_n}$ exakt ist auf $\P_n$, denn wegen der Eindeutigkeit in \ref{2.5} gilt $\_{I_n}=I_n$ also $\omega_j=\omega_{n-j}$.
				Sei $p\in \P_n$ und setze $\_p(x) := p(a+b-x)$.
				Dann ist $p(x) \in \P_n$ und $I(p) = \_I(\_p)$ wegen „Spiegelung‘.

				Damit folgt
				\begin{align*}
					\_I_n (p) &= \sum_{j=0}^n \omega_{n-j}p(x_j) \\
					&=\sum_{j=0}^n \omega_{n-j} \_p(\underbrace{a+b-x_j}_{=x_{n-j}}) \\
					&=\sum_{j=0}^n \omega_{n-j}\_p(x_{n-j})
					\stackrel{i=n-j} = \sum_{i=0}^n \omega_i \_p(x_i)
					= I_n (\_p) = I(\_p) = I(p)
				\end{align*}
			\item
				Sei $n=2m$ ,d.h. $x_m = \f {a+b}2$ wegen der Symmetrie.
				Wir überlegen uns zunächst
				\begin{align*}
					N(x) &:= \prod_{l=0}^n (x-x_l) \in \P_{n+1} \\
					&= \l( \prod_{l=0}^{m-1}(x-x_l)\r) \cdot \l(x-\f {a+b}2\r) \prod_{l=0}^{m-1}(x-(a+b-x_l))
				\end{align*}
				wegen $((a+b-x)-x_l) = -(x-(a+b-x_l))$ folgt
				\[
					N(a+b-x) = (-1)^{n+1}N(x) = - N(x)
				\]
				Also eine „Punktsymmetrie“ um $x = \f {a+b}2$.
				Damit ist
				\[
					I(N) = 0
				\]
				Zeige nun die Exaktheit von $I_n$ auf $\P_{n+1}$.

				Sei $p\in \P_{n+1}$, dann existiert $q\in \P_n, c\in \R$ mit $p=q+c\cdot N$.
				Sei $p_n$ Interpolationspolynom zu Daten $(x_i,p(x_i))$, $i=0,\dotsc,n$.
				Es gilt
				\[
					p(x_l) = q(x_l) + c\cdot \underbrace{N(x_l)}_{=0} = q(x_l) \qquad l=0,\dotsc,N
				\]
				Also ist $q=p_1$ wegen der Eindeutigkeit der Interpolation.
				\[
					I(p) = I(q+cN) = I(q) + c\underbrace{I(N)}_{=0} = I(q) = I(p_n) = I_n(p)
				\]
		\end{enumerate}
	\end{proof}
\end{st}

\begin{st}[Fehlerabschätzung für Interpolatorische Quadraturen]
	\label{2.7}
	Sei $I_n$ eine interpolatorische Quadratur.
	Dann gelten für das Fehlerfunktional
	\[
		R_n = I(f) - I_n(f)
	\]
	folgende Schranken:
	\begin{enumerate}[i)]
		\item
			$\displaystyle |R_n(f)| \le \f {(b-a)^{n+2}}{(n+1)!}\|f^{(n+1)}\| \qquad f\in C^{n+1}([a,b])$
		\item
			falls $I_n$ symmetrisch und $n$ gerade, dann gilt
			\[
				|R_n(f)| \le \f {(b-a)^{n+3}}{(n+2)!} \|f^{(n+2)}\|_\infty \qquad f\in C^{n+2}([a,b])
			\]
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item
				Sei $p_n\in \P_n$ ein Interpolationspolynom zu den Daten $\{(x_i,f(x_i)\}_{i=0}^n$.
				Dann ist
				\begin{align*}
					|R_n(f)| 
					&= |I(f) - I_n(f)| \\
					&= |I(f) - I(p_n)| \\
					&= \l|\int_a^b f(x) - p_n(x) \r| \\
					&\le \int_a^b |f(x)-p_n(x)| \\
					&\stackrel{\ref{1.10}}= \int_a^b \l| \f 1{(n+1)!}f^{(n+1)}(\xi(x)) \prod_{i=0}^n (x-x_i)\r| \\
					&\le \f {\|f^{(n+1)}\|_\infty}{(n+1)!} \int_a^b (b-a)^{n+1} \\
					&= \f {(b-a)^{n+2}}{(n+1)!}\|f^{(n+1)}\|_\infty
				\end{align*}
			\item
				Sei $q_{n+1}\in \P_{n+1}$ eine Hermite-Interpolation zu den Daten $\{(x_i,f(x_i)\}_{i=0}^n$ und $(x_l,f'(x_l))$ beliebig.
				Sei $p_n\in \P_n$ ein standard Interpolationspolynom zu den Daten $\{(x_i,f(x_i)\}_{i=0}^n$.

				Wie im Beweis von \ref{2.6}	ist für geeignete $q_n\in \P_n, c\in \R$ 
				\[
					q_{n+1} = q_n + c\cdot N(x)
				\]
				wegen $f(x_i) = q_{n+1}(x_i) = q_n(x_i) + c\cdot N(x_i) = q_n(x_i)$ ist $q_n = p_n$ das Interpolationspolynom, also
				\begin{align*}
					I_n(f)
					= I(p_n)
					= I(q_n)
					= I(q_n) + c\cdot I(N)
					= I(q_{n+1}
				\end{align*}
				Damit gilt
				\[
					|R_n(f)| 
					= |I(f) - I_n(f)| 
					= |I(f) - I(q_{n+1})|
					= |I(f-q_{n+1})|
					= \l| \int_a^b f(x) - q_{n+1}(x) \r|
				\]
				Die Behauptung folgt wie in i) mittels der Hermite-Interpolationsdarstellung aus Satz \ref{1.17}.
		\end{enumerate}
	\end{proof}
	\begin{note}
		Die Abschätung $\displaystyle \l| \prod_{i=0}^n (x-x_i) \r| \le (b-a)^{n+1}$ ist sehr grob und lässt sich für konkrete Stützstellenwahl leicht wesentlich verbessern.
	\end{note}
\end{st}

\begin{st}[Koordinatentransformation]
	\label{2.8}
	Sei $\hat I_n(\hat f) = \sum_{j=0}^n \hat \omega_j \hat f(\hat x_j)$ eine interpolatorische Quadratur auf $[\hat a, \hat b]$.
	\begin{enumerate}[i)]
		\item
			Dann ist
			\[
				I_n(f) := \sum_{j=0}^n \omega_j f(x_j), \qquad \omega_j = \f{b-a}{\hat b - \hat a}\hat \omega_j, \qquad x_j =a + \f{b-a}{\hat b -a}(\hat b - \hat a)
			\]
			eine Interpolatorische Quadratur auf $[a,b]$.
		\item
			Gilt für ein geignetes $K\in \R$ und $m$
			\[
				|\hat I(\hat f) - \hat I_n(\hat f) | \le K \cdot \|\hat f^{(m)}\|_\infty \cdot (\hat b - \hat a)^{m+1}
			\]
			Dann gilt für $I_n$ die Abschätzung
			\[
				|I(f) - I_n(f)| \le K \cdot \|f^{(m)}\|_\infty \cdot (b-a)^{m+1}
			\]
	\end{enumerate}
	\begin{note}
		Also reicht es interpolatorische Quadraturen auf einfachen Intervallen zu konstruieren.
		Diese sind nach Skalierung auf jedes beliebige Intervall anwendbar.
	\end{note}
	\begin{proof}
		\begin{enumerate}[i)]
			\item
				Zeige, dass $I_n(f)$ exakt auf $\P_n$ ist.
				Sei $p\in \P_n$, setze $\hat p(\hat x) := p(a- \f {b-a}{\hat b- \hat a}(\hat x- \hat x)) \in \P_n$.
				\begin{align*}
					I_n(p) 
					&= \sum_{j=0}^{n} \f {b-a}{\hat b - \hat a} \hat \omega_j p\l(a + \f {b-a}{\hat b - \hat a}(\hat x_j - \hat a)\r) \\
					&=\f {b-a}{\hat b - \hat a}\sum_{j=0}^n \hat \omega_j \hat p(\hat x_j) \\
					&= \f {b-a}{\hat b - \hat a}\hat I_n(\hat p) \\
					&= \f {b-a}{\hat b - \hat a}\int_{\hat a}^{\hat b}\hat p(\hat x) d\hat x \\
					&\stackrel{x:= a+ \f{b-a}{\hat b - \hat a}(\hat x - \hat a)} = \f {b-a}{\hat b - \hat a} \int_a^b p(x) \f {\hat b - \hat a}{b-a} dx \\
					&= I(p)
				\end{align*}
			\item
				Sei $f\in C^m([a,b]), \hat f(\hat x) := f(a + \f {b-a}{\hat b - \hat a}(\hat x - \hat a) \in C^m([\hat a,\hat b])$
				Dann ist
				\begin{align*}
					\hat f'(\hat x) &= \f {b-a}{\hat b - \hat a}f'(x) \\
					\vdots \quad &= \quad \vdots \\
					\hat f^{(m)}(\hat x) &= \l( \f {b-a}{\hat b-\hat a} \r)^m f^{(m)}(x)
				\end{align*}
				Außerdem
				\begin{align*}
					|I(f) - I_n(f)| 
					&= \l| \int_a^b f(x) dx - \sum_{j=0}^n \omega_j f(x_j) \r| \\
					&= \l| \f {b-a}{\hat b- \hat a} \int_a^b \hat f(\hat x) d\hat x - \sum_{j=0}^n \f {b-a}{\hat b - \hat a} \hat \omega_j \hat f(\hat x_j) \r| \\
					&= \l| \f {b-a}{\hat b - \hat a} (\hat I(\hat f) - \hat I_n(\hat f)) \r| \\
					&\stackrel{\text{Ann}}\le \f {b-a}{\hat b - \hat a}K\cdot \|\hat f^{(m)}\|_\infty (\hat b - \hat a)^{m+1} \\
					&\stackrel{\text{nach obigem}}\le \f {b-a}{\hat b - \hat a} K \cdot \f {(b-a)^m}{(\hat b - \hat a)^m} \|f^{(m)}\|_\infty \cdot (\hat b - \hat a)^{m+1} \\
					&= (b-a)^{m+1}\|f^{(m)}\|_\infty \cdot K
				\end{align*}
		\end{enumerate}
	\end{proof}
\end{st}


\subsection{Die Newton-Cotes Formeln}


\begin{df}
	\label{2.9}
	\emph{Newton-Cotes Quadraturen} (N.-C.) sind Interpolatorische Quadraturen zu äquidistanten Stützstellen.
	Wir unterscheiden
	\begin{itemize}
		\item
			geschlosse Newton-Cotes-Formeln:
			\[
				h:= \f {b-a}n, \qquad x_k := a + kh, \quad (k=0,\dotsc,n)
			\]
		\item
			offene Newton-Cotes-Formeln:
			\[
				h:= \f {b-a}{n+2}, \qquad x_k = a+ (k+1)h, \quad (k=0,\dotsc,n)r
			\]
			(d.h. die Randpunkte sind keine Stützstellen)
	\end{itemize}
\end{df}

\begin{table}[h]
	\centering
	\caption{Koeffiziententabelle geschlossener Newton-Cotes Formeln auf $[0,1]$}
	\begin{tabular}{l|cl}
		n & $(\omega_0, \dotsc, \omega_n)$ \\ \hline
		1 & $(\f 12, \f 12)$ & „Trapezregel“ \\ 
		2 & $\f 16 (1,4,1)$ & „Simpsonregel“, Kepplersche Fassregel \\
		3 & $\f 18 (1,3,3,1)$ & „$\f 38$-Regel“ \\
		4 & $\f 1{90} (7,32,12,32,7)$ & „Milne-Regel“ \\
		5 & $\f 1{288} (19,75,50,50,75,19)$ & \\
		6 & $\f 1{840} (41,216,27,272,27,216,41)$ &
	\end{tabular}
\end{table}

\begin{nt*}
	\begin{itemize}
		\item
			Nach \ref{2.6} i) sind die $\{\omega_i\}$ symmetrisch, da die Stützstellen $\{x_i\}$ symmetrisch sind.
		\item
			Falls $[a,b] \neq [0,1]$ erhält man die Gewichte durch die Skalierung aus \ref{2.8}, z.B.
			\[
				I_3(f) = \f {b-a}8 \l( f(a) + 3 f\l(a+\f {b-a}3\r) + 3 f\l(a+\f {b-a}3 2\r) + f(b) \r)
			\]
		\item
			Falls $a,b\in \Q$, dann auch $\omega_i \in \Q$.
	\end{itemize}
\end{nt*}

\begin{table}[h]
	\centering
	\caption{Koeffiziententabelle offener Newton-Cotes Formeln auf $[0,1]$}
	\begin{tabular}{l|cl}
		n & $(\omega_0, \dotsc, \omega_n)$ \\ \hline
		0 & 1 & „Mittelpunktsregel“ \\ 
		1 & $(\f 12, \f 12)$ & \\
		2 & $(\f 23, -\f 13, \f 23)$ &  \\
		3 & $\f 1{24} (11,1,1,11)$ &
	\end{tabular}
\end{table}

\begin{nt*}
	\begin{itemize}
		\item
			Ab $n=2$ treten negative Gewichte auf (bei geschlossenen Newton-Cotes Formeln ab $n=7$), was zu numerischen Auslöschungseffekten führen kann.
		\item
			Die Gewichte
			\[
				\omega_j := \int_0^1 L_j^n(x) dx
			\]
			lassen sich kompakt als Lösung eines LGS schreiben mit Hilfe einer transponierten Vandermondematrix darstellen (siehe Satz \ref{2.10}).
	\end{itemize}
\end{nt*}

\begin{st}
	\label{2.10}
	Für das Intervall $[0,1]$ sind die Gewichte $\{\omega_j\}_{j=0}^n$ der geschlossenen Newton-Cotes Formel Lösung des linearen Gleichungsystems
	\[
		\begin{pmatrix}
			1 & 1 & 1 & \dotsc & 1 \\
			0 & \f 1n & \f 2n & \dotsc & \f nn \\
			0 & (\f 1n)^2 & (\f 2n)^2 & \dotsc & (\f nn)^2 \\
			\vdots & \vdots & \vdots & \ddots & \vdots \\
			0 & (\f 1n)^n & (\f 2n)^n & \dotsc & (\f nn)^n \\
		\end{pmatrix}
		\begin{pmatrix}
			\omega_0 \\
			\vdots \\
			\omega_n
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 \\ \f 12 \\ \vdots \\ \f 1{n+1}
		\end{pmatrix}
	\]
	\begin{proof}
		$I_n$ ist eine Interpolatorische Qudratur, also exakt auf $\P_n$.
		Wähle $p(x) = x^i$, dann gilt
		\[
			\sum_{j=0}^n \omega_j \l( \f jn \r)^i = I_n(p) = I(p) = I(x^i) = \int_0^1 x^i dx = \f 1{i+1}
		\]
		Dies ist gerade die $i$-te Zeile im System.
	\end{proof}
\end{st}

\begin{kor}[Exaktheit für Newton-Cotes Quadraturen]
	\label{2.11}
	Eine Newton-Cotes Quadratur der Ordnung $n\in \N$ ist exakt auf $\P_n$.
	Falls $n$ gerade, dann sogar auf $\P_{n+1}$.
	\begin{proof}
		Die Newton-Cotes Formeln sind symmetrisch und Interpolationsquadraturen.
		Die Behauptung folgt damit aus \ref{2.4} und \ref{2.6} ii).
	\end{proof}
\end{kor}

\begin{st}
	\label{2.12}
	\begin{enumerate}[i)]
		\item
			Für die Trapezregel ($n=1$) gilt
			\[
				\big|I(f) - I_n(f) \big| \le \f {(b-a)^3}{12} \|f''\|_\infty
			\]
		\item
			Für die Simpsonregel ($n=2$) gilt
			\[
				\big| I(f) - I_n(f) \big| \le \f{(b-a)^5}{2880}\|f^{(4)}\|_\infty
			\]
	\end{enumerate}
	\begin{note}
		Aus \ref{2.7} folgt lediglich
		\[
			|I(f) - I_1(f)| \le \f {(b-a)^3}2 \|f''\|_\infty
		\]
		und
		\[
			|I(f) - I_2(f)| \le \f {(b-a)^5}{24} \|f^{(4)}\|_\infty
		\]
		Der Satz \ref{2.12} ist also eine Verbesserung um Faktor 6 in i) und 120 in ii) wegen der speziellen Wahl der Stützstellen.
	\end{note}
	\begin{proof}
		In Beweis von \ref{2.7} wurde verwendet.
		\[
			\int_a^b \l| \prod_{i=0}^n (x-x_i)\r| dx \le (b-a)^{n+1}
		\]
		\begin{enumerate}[i)]
			\item
				Dies wird um Faktor 6 verbessert durch:
				\[
					\int_a^b |x-a||x-b| = \int_a^b (x-a)(b-x) dx = \dotsb = \f {(b-a)^3}6
				\]
			\item
				analog
		\end{enumerate}
	\end{proof}
\end{st}

\begin{nt*}
	\begin{itemize}
		\item
			Ein Erhöhen von $n$ führt im Allgemeinen nicht zur Konvergenz, d.h. es existieren $f$, so dass
			\[
				\lim_{n\to \infty}I_n(f) \neq I(f)
			\]
		\item
			Für „schöne“ Funktionen kann man Konvergenz zeigen, z.B. bei Polynomen oder bei Funktionen gemäß Satz \ref{1.12}.
			(Konvergenz genau dann, wenn die Polynominterpolation gleichmäßig konvergiert)
		\item
			Gegenbeispiel für Konvergenz: 	
\begin{table}[h]
	\centering
	\caption{$[a,b]=[-1,1], f(x) = \sqrt{|x|}$.}
	\begin{tabular}{l|r}
		n & $I_n(f)$ \\ \hline
		1 & 0.6667 \\
		2 & 0.6667 \\
		3 & 0.0327 \\
		4 & 0.0166 \\
		5 & 0.0480 \\
		6 & 0.2240 
	\end{tabular}
\end{table}
		\item
			Eine Idee zur Verbesserung wäre das Zerlegen des Intervalls: „zusammengesetzten Quadraturen“
	\end{itemize}
\end{nt*}


\begin{df}[Zusammengesetzte Quadratur]
	\label{2.13}
	Sei $\hat n \in \N$, $\hat I_{\hat n}(\hat f) = \sum_{i=0}^{\hat n}\hat \omega_k \hat f(\hat x_k)$ eine Interpolations-Quadratur auf $[0,1]$.

	Sei $[a,b]\subset \R$, $N\in \N$, $H:= \f {b-a}N$, dann ist
	\[
		I_h (f) := H \cdot \sum_{i=1}^N \sum_{k=0}^{\hat n}\hat \omega_k f\l(a+(l-1)H + \hat x_k H\r)
	\]
	eine \emph{zusammengesetzte Quadratur}.
\end{df}

\begin{nt*}
	\begin{itemize}
		\item
			$I_h$ ist im Allgemeinen nicht exakt auf $\P_n$ mit $n=(\hat n + 1) N$, sondern nur auf $\P_{\hat n}$.
		\item
			Die zusammengesetzte Mittelpunktsregel ($\hat n = 0, \hat \omega_0 = 1, \hat x_0 = \f 12$) ergibt eine Riemann-Summe:
			\[
				I_h(t) := H \cdot \sum_{l=1}^N f (x_l) \qquad x_l = a + \l(l- \f 12\r) H
			\]
		\item
			Die zusammengesetzte Trapezregel ergibt ($\hat n = 1, \hat \omega_0 = \hat \omega_1 = \f 12, \hat x_0 = 0, \hat x_1 = 1, h := H = \f {b-a}N$):
			\[
				I_h(f) = \f h2 \l(f(a) + 2 \sum_{l=1}^{N-1}f(x_l) + f(b)\r) \qquad x_l := a+ lh \qquad l=0,\dotsc,N
			\]
		\item
			Zusammengesetzte Simpsonregel mit
			\[
				\hat n = 2, \quad h := \f H2, \qquad x_k := a+kh, \quad k=0,\dotsc, 2N
			\]
			ergibt sich
			\[
				I_h(f) = \f h3 \l( f(a) + 2 \sum_{l=1}^{N-1}f(x_{2l}) + 4 \sum_{l=1}^{N}f(x_{2l-1}) + f(b)\r)
			\]
		\item
			Verbesserung der Quadratur durch Erhöhen der Intervall-Anzahl, z.B. $f(x)= \sqrt{|x|}$, $[a,b]=[-1,1]$ mit der zusammengesetzten Simpson-Regel.
			\begin{table}[h]
				\centering
				\caption{Es ergibt sich eine konvergente für gerade $N$}
				\begin{tabular}{c|c}
					N & $|I(f)-I_h(f)|$ \\ \hline
					1 & 0.6667 \\
					2 & 0.0572 \\
					3 & 0.1287 \\
					4 & 0.0203 \\
					5 & 0.0598
				\end{tabular}
			\end{table}
	\end{itemize}
\end{nt*}

\begin{st}[Fehlerschranke für zusammengesetzte Quadraturen]
	\label{2.14}
	Sei $\hat I_n$ auf $[0,1]$ derart, dass für $m\in \N$, $K\in \R$
	\[
		|\hat I(\hat f) - \hat I_{\hat n}(\hat f)| \le K \cdot \|\hat f^{(m)}\|_\infty
		\qquad \hat f \in C^m([0,1])		
	\]
	Dann erfüllt die zusammengesetzte Quadratur $I_h$ die Fehlerschranke
	\[
		|I(f) - I_{n}(f) \le K \cdot \|\hat f^{(m)}\|_\infty (b-a) H^m
		\qquad \hat f \in C^m([a,b])	
	\]
	\begin{proof}
		Transformiere $\hat I_{\hat n}$ auf $[a+(l-1)H, a+lH]$, $l=1,\dotsc,N$
		\[
			I_{\hat n}^{(l)}(f) := \sum_{k=0}^{\hat n} \hat \omega_k H f(a+(l-1)H + \hat x_k H)
		\]
		Gemäß \ref{2.8} ii) gilt
		\[
			\l| \int_{a+(l-1)H}^{a+lH} f(x) dx - I_{\hat n}^{(l)}\r| \le K \sup_{x\in [a+(l-1)H,a+lH]} |f^{(m)}(x)| H^{m+1}
		\]
		Für Gesamtfehler also
		\begin{align*}
			|I(f) - I_h(f)| 
			&= \l| \sum_{l=1}^N \int_{a+(l-1)H}^{a+lH} f(x) dx - I_{\hat n}^{(l)}(f)\r| \\
			&\le \sum_{l=1}^N K \cdot \|f^{(m)}\|_\infty H^{m+1} \\
			&= \underbrace{N H}_{=b-a} K \|f^{(m)}\|_\infty H^m
		\end{align*}
	\end{proof}
\end{st}

\begin{kor}
	\label{2.15}
	\begin{enumerate}[i)]
		\item
			Für die zusammengesetzt Trapezregel lautet die Fehlerabschätzung
			\[
				|I(f) - I_h(f)| \le \f {\|f''\|_{\infty}}{12} (b-a) h^2 \qquad \forall f \in C^2 ([a,b])
			\]
		\item
			Für die zusammengesetzte Simpsonregel
			\[
				|I(f) - I_h(f)| \le \f {\|f^{(4)}\|_\infty}{180} (b-a) h^4 \qquad \forall f \in  C^4([a,b])
			\]
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item
				Satz \ref{2.14} mit $m=2$, $K=\f 1{12}$ (Satz \ref{2.12} i) ).
			\item
				Satz \ref{2.14} mit $m=4$, $K=\f 1{2880}$, $h=\f H2$ (Satz \ref{2.12} i) ).
		\end{enumerate}
	\end{proof}
\end{kor}

\begin{kor}[Konvergenz]
	\label{2.16}
	Für zusammengesetzte Quadraturen und $f$ gemäß \ref{2.15} gilt
	\[
		\lim_{h \to 0} I_h(f) = I(f)
	\]
\end{kor}

\subsection{Gauß-Quadraturen}

\begin{itemize}
	\item
		Satz \ref{2.5} besagt, dass zu vorgegebene Stützstellen $\{x_k\}_{k=0}^n$ eine Interpolations-Quadratur existiert, welche exakt auf $\P_n$ ist.
	\item
		Fall $\{x_i\}$ symmetrisch im Intervall und $n$ gerade, dann ist die Interpolations-Quadratur nach \ref{2.6} sogar exakt auf $\P_{n+1}$.
	\item
		Kann durch geeignete Wahl von Stützstellen der Grad der Exaktheit weiter gesteigert werden? Bis zu welchem Grad?
\end{itemize}

\begin{df}[Orthogonale Polynome]
	\label{2.17}	
	Sei $[a,b] = [-1,1]$ und $\<\cdot, \cdot\>$ das $L^2$-Skalarprodukt auf $C([a,b])$, d.h.
	\[
		\<f,g\> = \int_a^b f(x)g(x) dx
	\]
	Wir definieren rekursiv
	\begin{align*}
		p_0(x) &:= 1 \\
		p_{n+1}(x) &:= x^{n+1}- \sum_{i=0}^n \f {\<x^{n+1},p_i\>}{\<p_i,p_i\>} p_i(x)
	\end{align*}
	\begin{note}
		\begin{itemize}
			\item
				Ein Vektorraum $V$ mit Skalarprodukt wird ein normierter Raum mit $\|f\| := \sqrt{\<f,f\>}$.
				Falls $V$ vollständig bezüglich dieser Norm, nennt man $V$ \emph{Hilbertraum}, sonst \emph{Prä-Hilbertraum}.

				$(C([a,b]),\<\cdot,\cdot\>)$ ist nicht vollständig bezüglich der $L^2$-Norm, daher ein Prä-Hilbertraum.
			\item
				Die orthonormalen Polynome aus \ref{2.17} werden durch das Gram-Schmidt'sche Orthonormalisierungsverfahren konstruiert, angewandt auf Monome $\{x^i\}_{i\in \N_0}$.

		\end{itemize}
	\end{note}
\end{df}

\begin{ex*}
	\begin{align*}
		p_0(x) &= 1 \\
		p_1(x) &= x - \f {\<x,p_0\>}{\<p_0,p\>}p_0(x) = x - \f 02 1 = x \\
		p_2(x) &= x^2 - \f {\<x^2,1\>}{\<1,1\>} - \f{\<x^2,x\>}{\<x,x\>}x = x^2 - \f {\f 23}2 - \f 0{\f 23} = x^2 - \f 13
	\end{align*}
\end{ex*}

\begin{lem}
	\label{2.18}
	\begin{enumerate}[i)]
		\item
			$\{p_0,\dotsc,p_n\}$ sind wohldefiniert und bilden eine Basis für $\P_n$.
		\item
			Orthogonalität: $p_n \orth \P_{n-1}$, d.h. $\<p_n,q\> = 0$ für alle $q\in \P_{n-1}$.
		\item
			$p_n$ hat $n$ \emph{verschiedene, reelle} Nullstellen $x_1,\dotsc, x_n \in (-1,1)$.
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item
				Zeige per Induktion nach $n$.
				Für $n=0$ ergibt sich $p_0(x)=1$ und $\<p_0\> = \P_0$.

				Seien $\{p_0,\dotsc,p_n\}$ wohldefiniert und Basis für $\P_n$.
				Dann ist $p_i\neq 0$, für $i=0,\dotsc,n$, denn sonst wäre $\{p_0,\dotsc,p_n\}$ linear abhängig.
				Also sind alle $\<p_i,p_i\> \neq 0$ und damit $p_{n+1}$ wohldefiniert.

				Es ist $V_{n+1} := \<p_0,\dotsc,p_{n+1}\> \in \P_{n+1}$ nach Konstruktion.
				Wegen
				\[
					x^{n+1} = p_{n+1} + \sum_{i=0}^n \f {\<x^{n+1},p_i}{\<p_i,p_i\>} p_i(x)
				\]
				ist $x^{n+1} \in V_{n+1}$.
				Nach Induktionsvoraussetzung ist auch $\P_n \subset V_{n+1}$ also $\P_{n+1}\subset V_{n+1}$ also $\P_{n+1} = V_{n+1}$.
				Also ist $\{p_i\}_{i=0}^{n+1}$ Erzeugenden-System für $\P_{n+1}$ aus $n+2$ Elementen.
				Wegen $\dim \P_{n+1} = n+2$ ist $\{p_i\}_{i=0}^{n+1}$ linear unabhängig und damit eine Basis
			\item
				Nach i) ist $\P_{n+1} = \<p_0,\dotsc, p_{n-1}\>$ also reicht es zu zeigen, dass $\<p_n,p_i\>=0$ für $i=0,\dotsc, n-1$.
				Zeige per Induktion.
				Für $n=1$ ist $\<p_1,p_0\> = \int_{-1}^1 x = 0$.
				Sei $j\in \{0,\dotsc,n\}$.
				\begin{align*}
					\<p_{n+1},p_j\> 
					&= \l\< x^{n+1}- \sum_{i=0}^n \f {\<x^{n+1},p_i\>}{\<p_i,p_i\>} p_i(x) , p_j\r\> \\
					&= \<x^{n+1},p_j\> - \sum_{i=0}^n \f{\<x^{n+1},p_i\>}{\<p_i,p_i\>} \underbrace{\<p_i(x),p_j\> }_{= \delta_{ij}} \\
					&= \<x^{n+1},p_j\> - \f {\<x^{n+1},p_j\>}{\<p_j,p_j\>} \<p_j,p_j\> = 0
				\end{align*}
			\item
				Angenommen $z\in \C\setminus \R$ ist Nullstelle von $p_n(x) = \sum_{i=0}^n a_i x^i$ mit $a_i\in \R$.
				Dann ist $\_z$ auch Nullstelle von $p$, denn
				\[
					0 = \_{p(z)} = \_{\sum_{i=0}^n a_i z^i} = \sum_{i=0}^n a_i\_{z}^i = p(\_z)
				\]
				Definiere
				\[
					q(x) := \f {p_n(x)}{(x-z)(x-\_z)} \in \P_{n-2}
				\]
				Dann kann man schreiben
				\[
					0 = \<q,p_n\> = \int_{-1}^1 \f {p_n(x)^2}{|x-2|^2} dx \implies p_n(x) = 0
				\]
				was ein Widerspruch ist.

				Angenommen $z\in (-1,1)$ ist eine mehrfache Nullstelle von $p_n$, definiere
				\[
					q(x) := \f {p_n(x)}{(x-z)^2} \in \P_{n-2}
				\]
				dann ist
				\[
					0= \<q,p_n\> = \int_{-1}^1 \f {p_n(x)^2}{(x-z)^2} dx  \implies p_n = 0
				\]
				was ein Widerspruch ist.

				Angenommen $z\in \R\setminus (-1,1)$ ist eine Nullstelle, definiere
				\[
					q(x) := \f {p_n(x)}{x-z} \in \P_{n-1}
				\]
				dann ist
				\[
					0 = \<q,p_n\> = \int_{-1}^1 \f{p_n(x)^2}{x-z} \implies p_n = 0
				\]
				($x-z$ ist entweder $\ge 0$ oder $\le 0$ für alle $x\in [-1,1]$)
				was ein Widerspruch ist.


		\end{enumerate}
	\end{proof}
\end{lem}

\begin{df}[Gauß'sche Quadraturen]
	\label{2.19}
	Sei $n\in \N$, wäle $\{x_i\}_{i=0}^n$ als Nullstellen von $p_{n+1} \in \P_{n+1}$.
	Dann nennen wir die zugehörigen Interpolations-Quadratur \emph{Gauß-Quadratur} (G.Q.).
\end{df}

\begin{ex*}
	\begin{itemize}
		\item
			Sei $n=1$, $p_2(x) = x^2-\f 13$ mit den Nullstellen $x_0=-\sqrt{\f 13}, x_1 = \sqrt{\f 13}$.
			Gewichte: $\omega_0=\omega_1 = 1$, da Interpolations-Quadratur.

			\begin{table}[h]
				\centering
				\caption{Gauß-Quadraturen auf $[-1,1]$}				
				\begin{tabular}{c|c|c}
					n & $(x_0, \dotsc, x_n)$ & ($\omega_0,\dotsc,\omega_n$) \\ \hline
					0 & 0 & 2 \\
					1 & $\l(-\sqrt{\f 13}, -\sqrt{\f 13}\r)$ & (1,1) \\
					2 & $\l(-\sqrt{\f 35}, 0, \sqrt{\f 35}\r)$ & $\l(\f 59, \f 89, \f 59\r)$ \\
					3 & $\l(- \sqrt{\f 37 + \f 27 \sqrt{\f 65}}, - \sqrt{\f 37 - \f 27\sqrt{\f 65}}, \sqrt{\f 37 - \f 27\sqrt{\f 65}}, \sqrt{\f 37 + \f 27\sqrt{\f 65}} \r)$
					& $\l(\f {18-\sqrt{30}}{36}, \f {18+\sqrt{30}}{36}, \f {18+\sqrt{30}}{36},\f {18-\sqrt{30}}{36}\r)$
				\end{tabular}
			\end{table}
	\end{itemize}
\end{ex*}

\begin{nt*}
	Die sogenannten \emph{Legendre-Polynome} sind definiert durch
	\[
		P_n(x) := \f 1{2^n n!} \f {d^n}{dx^n} (x^2 - 1)^n
	\]
	und erfüllen eine sogenannte „3-Term-Rekursion“:
	\[
		n P_n(x) = (2n-1)xP_{n-1}(x) - (n-1)P_{n-2}(x) \qquad n\ge 2
	\]
	Die orthogonalen Polynome aus Definition \ref{2.17} sind also skalierte Legendre-Polynome
	\[
		p_n(x) = \f {n!}{(2n)!} \f {d^n}{dx^n} (x^2-1)^n
	\]
	Gauß-Quadraturen auf $[-1,1]$ werden auf Gauß-Legendre-Quadraturen genannt.
\end{nt*}

\begin{st}[Symmetrie]
	\label{2.20}
	Gauß-Quadraturen sind symmetrisch.
	\begin{proof}
		Zeige zunächst induktiv: 
		\[
			n \text{ gerade (ungerade) } \implies p_n \text{ gerade (ungerade) }
		\]
		$p_0 = 1$ ist offensichtlich gerade.
		Die Behauptung gelte für $p_i$ mit $i=0,\dotsc,n$.
		\[
			p_{n+1}(x) = x^{n+1} - \sum_{i=0}^n \f{\<x^{n+1},p_i\>}{\<p_i,p_i\>} p_i(x)
		\]
		Sei $n$ ungerade, dann ist $x^{n+1}$ gerade.
		Für $i$ ungerade ist
		\[
			\<x^{n+1},p_i\> = \int_{-1}^1 \underbrace{x^{n+1}}_{\text{gerade}} \underbrace{p_i(x)}_{\text{ungerade}} dx = 0
		\]
		Für $i$ gerade ist $p_i(x)$ gerade ist $p_{n+1}$ eine Linearkombination von geraden Funktionen, also $p_{n+1}$ gerade.
		Analog für $n$ gerade.

		Falls $x$ Nullstelle von $p_n$, dann ist auch $-p_n$ Nullstelle von $p_n$.
		Damit sind die Stützstellen der Gauß-Quadratur symmetrisch und somit auch die Gewichte (da Interpolations-Quadratur).
	\end{proof}
\end{st}

\begin{st}[Exaktheit]
	\label{2.21}
	Die Gauß-Quadratur $I_n$ ist exakt auf $\P_{2n+1}$.
	\begin{proof}
		Sei $p\in \P_{2n+1}$.
		Polynomdivision mit $q\in \P_n$ liefert
		\[
			p= p_{n+1} \cdot q + r \qquad  q,r\in \P_n
		\]
		Es folgt
		\begin{align*}
			I(p) 
			&= \int_{-1}^1 p(x) dx 
			= \underbrace{\int_{-1}^1 p_{n+1}q(x) dx}_{=0 \text{ da $p_{n+1}\orth \P_n$}} + \int_{-1}^1 r(x) dx
			= \int_{-1}^1 r(x) dx 
		\end{align*}
		Außerdem
		\begin{align*}
			I_n(p) 
			= \sum_{i=0}^n \omega_i p(x_i) \\
			= \sum_{i=0}^n \omega_i \underbrace{p_{n+1}(x_i)}_{=0} q(x_i) + \sum_{i=0}^n \omega_i r(x_i) \\
			= I_n(r) = I(r)
		\end{align*}
		da $I_n$ exakt auf $\P_n$.
		Also $I_n(p) = I(p)$.
	\end{proof}
\end{st}

\begin{st}[Grenze der Exaktheit]
	\label{2.22}
	Es existiert keine Interpolations-Quadratur $I_n$, welche exakt auf $\P_{2n+2}$ (oder höher) ist.
	\begin{proof}
		Seien $\{x_i\}_{i=0}^n$ die Stützstellen von $I_n$.
		Betrachte 
		\[
			p(x) = \prod_{i=0}^n (x-x_i)^2 \in \P_{2n+2}
		\]
		Es gilt
		\[
			I_n(p) = \sum_{i=0}^n \omega_i \underbrace{p(x_i)}_{=0} = 0
		\]
		aber
		\[
			I(p) = \int_{-1}^1 \underbrace{\prod_{i=0}^n (x-x_i)^2 }_{\ge 0} dx > 0
		\]
		also $I_n(p) \neq I(p)$ also nicht exakt auf $\P_{2n+2}$.
	\end{proof}
\end{st}

\begin{st}[Eindeutigket der Gauß-Quadratur]
	\label{2.23}
	Es existiert genau eine Quadratur $I_n$, welche exakt auf $\P_{2n+1}$ ist.
	\begin{proof}
		Die Existenz ist klar: Gauß-Quadratur \ref{2.21}.

		Sei $I_n$ die Gauß-Quadratur und $\_{I_n}$ eine Quadratur, welche exakt auf $\P_{2n+1}$ ist.
		Es reicht zu zeigen, dass die Stützstellen $\{x_i\}_{i=0}^n$ von $\_{I_n}$ Nullstellen vom orthogonalen Polynom $p_{n+1}$, denn dann sind auch die Gewichte identisch zu $I_n$, da beide Interpolations-Quadraturen und exakt auf $\P_n$ sind.
	
		Sei $q\in \P_n$, $\_p_{n+1} := \prod_{i=0}^n (x-x_i) \in \P_{n+1}$.
		\[
			\int_{-1}^1 \_p_{n+1}(x) q(x) 
			= I(\underbrace{\_p_{n+1} q}_{\in \P_{2n+1}})
			\stackrel{\text{Exaktheit}}= \_I_n (\_p_{n+1}q)
			\stackrel{\_p_{n+1}(x_i)=0}= 0
		\]
		Also ist $\_p_{n+1} \orth \P_n$.

		Es gilt $\dim(\P_{n+1}) = \dim( \P_n^\orth \cap \P_{n+1}) + \dim (\P_n)$, also
		\[
			\dim( \P_n^\orth \cap \P_{n+1}) = \dim(\P_{n+1}) - \dim(\P_n) = n+2 - (n+1) = 1
		\]
		Damit ist 
		\[
			 \<p_{n+1}\> = \P_n^\orth \cap \P_{n+1} = \<\_p_{n+1}\>
		\]
		Also sind die $p_{n+1}$ und $\_p_{n+1}$ linear abhängig: $\_p_{n+1} = \lambda p_{n+1}$.
		Damit sind die Nullstellen identisch, also auch die Stützstellen von $I_n$ und $\_I_n$.
	\end{proof}
\end{st}

\begin{ex*}[Vergleich von Newton-Cotes und Gauß-Quadratur auf $f(x)=\cos(x\f \pi 2)$ auf]
	\begin{table}[h]
		\centering
		\caption{Fehlervergleich auf $[-1,1]$}
		\begin{tabular}{c|r|r}
			n & Gauß-Quadratur & Newton-Cotes-Quadratur \\ \hline
			0 & 0.7268 & 0.7268 \\
			1 & 0.0401 & 1.2732 \\
			2 & 0.0009 & 0.0601 \\
			3 & 0.0000 & 0.0258 \\
			4 & 0.0000 & 0.0009
		\end{tabular}
	\end{table}
\end{ex*}

\begin{st}[Fehlerschranke]
	\label{2.24}
	Für $f\in C^{2n+2} ([-1,1])$, dann gilt für Fehler der Gauß-Quadratur
	\[
		|I(f) - I_n(f) \le K \cdot 2^{2n+3} \|f^{2n+2}\|_\infty
	\]
	mit
	\[
		K = \f {((n+1)!)^4}{(2n+3)\big((2n+2)!\big)^3}
	\]
	\begin{proof}
		Übung, oder auf Mathematik-Online: Höllig 2004
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				$K$ wird sehr schnell klein:
				\begin{table}
					\centering
					\caption{}
					\begin{tabular}{c|c}
					n & K \\ \hline
					0 & $\f 1{24}$ \\
					1 & $\f 1{4320}$ \\
					2 & $\f 1{2016000}$ \\
					3 & $\f 1{1778112000}$ 
					\end{tabular}
				\end{table}
			\item
				Die Sätze \ref{2.24}, \ref{2.27} i), ii) und \ref{2.12} i), ii) haben alle die gleiche Struktur.

				Sei $d$ maximal, so dass $I_n$ exakt auf $\P_d([a,b])$ (also $d=n$ bei Newton-Cotes für $n$ ungerade, $d=n+1$ bei Newton-Cotes für $n$ gerade und $d=2n+1$ bei Gauß-Quadratur)

				Dann ist
				\[
					|I(f) - I_n(f) | \le K \cdot (b-a)^{d+2} \|f^{d-1}\|_\infty
				\]
				für geeignetes $K$.
		\end{itemize}
	\end{note}
\end{st}

\begin{st}[Positivität]
	\label{2.25}	
	Alle Gewichte $\{\omega_j\}_{j=0}^n$ der Gauß-Quadratur sind positiv: $\omega_j > 0$.
	\begin{proof}
		$I_n$ ist exakt auf $\P_{2n+1}$ also werden quadrierte Lagrange-Polynome exakt integriert:
		\[
			\omega_j = \sum_{i=0}^n \omega_i (\underbrace{L_j^n(x_i)}_{\delta_{ij}})^2 = I_n((L_j^n)^2) = I((L_j^n)^2) =\int_{-1}^1 \underbrace{(L_j^n(x))^2}_{\ge 0} dx > 0
		\]
	\end{proof}
\end{st}

\begin{st}[Konvergenz]
	\label{2.26}
	Sei $(I_n)_{n\in \N}$ eine Sequenz von Gauß-Quadraturen.
	Dann gilt für $f\in C^0([a,b])$:
	\[
		\lim_{n\to \infty} I_n(f) = I(f)
	\]
	\begin{proof}
		Sei $\eps > 0$.
		Nach dem Weierstrass-Approximationssatz existiert $p\in \P_{n_0}$ für genügend großen $n_0\in \N$, so dass
		\[
			|f(x) - p(x)| \le \f {\eps}{2(b-a)} \qquad x\in [a,b]
		\]
		Dann gilt für alle $n\ge n_0$:
		\begin{align*}
			|I(f) - I_n(f)|
			\le \underbrace{|I(f) - I(p)|}_{\le \int_a^b |f(x) - p(x)| dx \le (b-a)\f \eps{2(b-a)}= \f \eps 2} 
			+ \underbrace{|I(p) - I_n(p)|}_{=0 \text{ da $I_n$ exakt auf $\P_{n_0}$}} 
			+ \underbrace{|I_n(p)-I_n(f)|}_{\le \sum_{i=0}^n |\omega_i||p(x_i)-f(x_i)| \le (b-a) \f {\eps}{2(b-a)} } \\
			= \f \eps 2 + 0 + \f \eps 2 = \eps
		\end{align*}
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				Trotz eventuelle Fehlender gleichmäßiger Konvergenz der Interpolations-Polynome, d.h.
				\[
					\|p_n - f\|_\infty \not \to 0
				\]
				konvergieren die Integrale $I_n(f) \to I(f)$.
			\item
				Die Aussage von \ref{2.6} ist nicht auf Newton-Cotes übertragbar ($\sum_{i=0}^n |\omega_i| \to \infty$ der Beweis nutzt $\sum_{i=0}^n |\omega_i| = b-a$).
		\end{itemize}
	\end{note}
\end{st}

\begin{nt*}[Allgemeinere Integrale]
	Sei $\omega \in C^0(I)$ eine „Gewichtsfunktion“ auf dem Intervall $I\subset \R$ mit $\int_I \omega(x) dx < \infty$ und $\omega(x) \ge 0$ für alle $x\in I$.

	Dann definiert 
	\[
		\<f,g\>_\omega := \int_I f(x) g(x) \omega(x) dx
	\]
	ein Skalarprodukt auf $C^0(I)$.

	Definiere analog zu \ref{2.17} $\{p_i\}_{i=0}^{n+1}$ als Orthogonalsystem bezüglich $\<\cdot,\cdot\>_\omega$ von $\P_{n+1}$.
	Wähle die Quadraturpunkte $\{x_i\}_{i=0}^n$ als Nullstellen von $p_{n+1}$ und setze $\{\omega_j\}_{j=0}^n$ als gewichtete Integrale der Lagrange-Polynome:
	\[
		\omega_i := \int_I L_i^n(x) \omega(x) dx
	\]
	Dann ist $I_n(f) = \sum_{i=0}^n\omega_i (x_i)$ eine Approximation für $I(f) := \int_I f(x) \omega(x) dx$.
	Es existieren einige spezielle Versionen
	\begin{itemize}
		\item
			$I=[-1,1], \omega(x) =1$, dann sind $\{x_i\}$ die Nullstellen der Legendre Polynome („Gauß-Legendre-Quadraturen“).
		\item
			$I=(-1,1), \omega(x) = \f 1{\sqrt{1-x^2}}$, dann sind die Tschebyscheff-Polynome ein Orthonormalsystem bezüglich $\<\cdot,\cdot\>_\omega$, $p_n= \hat T_n$.
			\[
				x_i := \cos \l(\pi \f {2j+1}{2(n+1)}\r) \qquad j= 0,\dotsc, n
			\]
			nach Satz \ref{1.14}. Und $\omega_0 = \dotsb = \omega_n = \f {\pi}{n+1}$.
		\item
			$I=[0,\infty), \omega(x) = e^{-x}$, dann bilden die Laguerre-Polynome ein Orthogonalsystem:
			\[
				L_0(x) := 1, \qquad L_1(x) := 1-x, \qquad L_{n+1}(x) = (1+2n-x)L_n(x) - n^2 L_{n-1}(x)
			\]
			mit $p_n = (-1)^n L_n$ („\emph{Gauß-Laguerre-Quadratur}“).
		\item
			$I=(-\infty,\infty), \omega(x) = e^{-x^2}$, dann bilden die \emph{Hermite-Polynome} ein Orthogonalsystem:
			\[
				H_0(x) := 1, \qquad H_1(x) := 2x, \qquad H_{n+1}(x) := 2xH_n(x) - 2nH_{n-1}(x)
			\]
			mit $p_n(x)=2^n H_n(x)$.
		\item
			$\alpha, \beta > -1$, $\omega(x) = (1-x)^\alpha (1+x)^\beta$, $I=(-1,1)$.
			Dann bilden \emph{Jacobi-Polynome} ein Orthogonalsystem:
			\[
				J_n(x;\alpha,\beta) = \f 1{2^n n! \omega(x)}\f {d^n}{dx^n}\Big( (x^2-1)^n \omega(x)\Big)
			\]
			Es ergibt sich die „Gauß-Jacobi-Quadratur“.
			Für $\alpha=\beta=0$ ergibt sich die Gauß-Legendre-Quadratur.
			Für $\alpha=\beta= -\f 12$ ergibt sich die Gauß-Tschebyscheff-Quadratur.
	\end{itemize}
	Diese Formeln sind vorteilhaft bei der Integration von Funktionen mit Singularitäten an Endpunkten von $I$.
\end{nt*}

\begin{ex*}
	Sei $f(x) = \f {x^2}{\sqrt{1-x^2}}$ und $\int_{-1}^1 f(x) dx = \int_{-1}^1 x^2 \underbrace{\f 1{\sqrt{1-x^2}}}_{\omega(x)} dx$.
	Wähle die Gauß-Tschebyscheff-Quadratur mit $n=2$ (für $p(x) = x^2$ exakt).
	\begin{align*}
		x_0 := \cos\l(\pi \f 16 \r) = \sqrt{\f 34}, \qquad x_1 &= \cos\l(\pi \f 36\r) = 0, \qquad x_2 = \cos \l(\pi\f 56\r) = -\sqrt{\f 34}
	\end{align*}
	mit $\omega_0 = \omega_1 = \omega_2 = \f {\pi}3$
	\[
		\int_{-1}^1 f(x) dx = \sum_{i=0}^2 \omega_i x_i^2 = \f {\pi}3 \cdot \f 64 = \f \pi2
	\]
	das stimmt mit der analytischen Lösung überein:
	\[
		\int_{-1}^1 \f {x^2}{\sqrt{1-x^2}} dx = \f 12 \sin^{-1}(x) - \f 12 x\sqrt{1-x^2} \; \Big|_{-1}^1 = \f 12 \l( \f \pi 2 - \l(-\f \pi 2\r) \r) = \f \pi2
	\]
\end{ex*}

\begin{nt*}[Zusammengesetzte Gauß-Quadraturen]
	\begin{itemize}
		\item
			Berechnung der Nullstellen von $p_n$ in hoher Präzision kann für gorße $n$ schwierig sein.
		\item
			Daher sind auch zusammengesetzt Gauß-Quadraturen praktikable Alternativen.
			\begin{ex*}
				Zusammengesetzte 2-Punkt-Gauß-Quadratur auf $[a,b]$, $N\in \N$, $h=\f {b-a}N$, $n:= N\cdot 2 - 1$, Mittelpunkte $m_l := a+ (l-\f 12)h$ , $l= 1,\dotsc, N$.
				\[
					I_h (f) = \f h2 \sum_{l=1}^N \Big( f\l(m_l- \f 1{2\sqrt 3} h\r) + f\l( m_l + \f 1{2\sqrt 3} h\r) \Big)
				\]
			\end{ex*}
		\item
			Die Fehlerschranke \ref{2.24} überträgt sich mittels \ref{2.8} ii) auf zusammengesetzte Gauß-Quadraturen.
	\end{itemize}
\end{nt*}


\subsection{Fehlerdarstellung nach Peano}

Wir wollen einen abstrakten Rahmen für Fehlerfunktionale vorstellen, welche insbesondere einige der vorhergehenden Fehleraussagen impliziert.

\begin{df}[Zulässige Funktionale]
	\label{2.27}
	Eine lineares Funktional $R : C^k([a,b]) \to \R$ heißt zulässig, wenn
	es eine Punktauswertung 
	\[
		f^{(m)}(x_0), \qquad x_0\in [a,b], \qquad m\in \{0,\dotsc,k\}
	\]
	oder ein gewichtetes Integral
	\[
		\int_a^b f^{(m)}(x) \omega(x) dx
	\]
	oder eine endliche Linearkombination von diesen.
\end{df}

\begin{ex*}
	\begin{itemize}
		\item
			Fehlerfunktionale für Quadraturen sind zulässig
			\[
				R(f) = \int_a^b f(x) dx - \sum_{i=0}^n \omega_i f(x_i)
			\]
		\item
			Fehlerfunktionale für Finite-Differenzen-Approximation sind zulässig
			\[
				R(f) = \f {f(b)-f(a)}{b-a} - f'(x_0) \qquad x_0\in [a,b]
			\]
	\end{itemize}
\end{ex*}

\begin{lem}[Vertauschungsregel]
	\label{2.28}	
	Für zulässiges Funktional $R$, $u\in C^0([a,b])$, $C\in C^k([a,b]^2)$ gilt
	\[
		R\l( \int_a^b K(t,\cdot) u(t) dt \r) 
		= \int_a^b R\Big(K(t,\cdot)\Big) u(t) dt
	\]
	\begin{proof}
		Setze
		\begin{align*}
			y(\cdot) &= \int_a^b K(t,\cdot) u(t) dt \\
			v(t) &= R(K(t,\cdot))
		\end{align*}
		Zu zeigen ist $R(y) = \int_a^b v(t) u(t) dt$.
		Für $R(f) = f^{(m)}(x_0)$:
		\begin{align*}
			R(y) = \f {d^m}{dx^m} \l( \int_a^b K(t,x) u(t) dx \r) \Big|_{x_0}
			= \int_a^b \l( \f {d^m}{dx^m} K(t,x) \r) u(t) dt \Big|_{x_0}
			= \int_a^b v(t) u(t) dt
		\end{align*}
		Für $R(t) = \int_a^b f^{(m)}(x) \omega(x) dx$ analog, für Linearkombinationen folgt die Behauptung aus der Linearität.
	\end{proof}
\end{lem}

\begin{lem}
	\label{2.29}
	Sei 
	$
		h_+^l := \begin{cases}h^l & h\ge 0 \\ 0 & \text{sonst}\end{cases}
	$
	Es gilt für $f\in C^{k+1}([a,b])$
	\[
		f(x) = (P_kf)(x) + \int_a^b K_k(t,x) f^{(k+1)}(t) dt
	\]
	mit
	\[
		P_k(t) (x) := \sum_{j=0}^k f^{(j)}(x) \f {(x-a)^j}{j!} \in \P_k
	\]
	und
	\[
		K_k(t,x) := \f 1{k!}(x-t)^k_+ \in C^{k-1}([a,b]^2)
	\]
	\begin{proof}
		Nutze Taylor mit Integralrestglied:
		\begin{align*}
			f(x) &= \sum_{j=0}^k f^{(j)}(a) \f {(x-a)^j}{j!} + \int_a^x \f {(x-t)^k}{k!} f^{(k+1)}(t) dt
			&= P_k f + \int_a^b \f {(x-t)^k_+}{k!} f^{(k+1)} (t) dt
		\end{align*}
	\end{proof}
\end{lem}

\begin{st}[Peano Fehlerdarstellung]
	\label{2.30}
	Sei $R$ ein zulässiges Funktional auf $C^k([a,b])$ welches auf $\P_n$, $n>k$ verschwindet, d.h. $R(p)=0$ für alle $p\in \P_n$.
	Dann gilt für alle $f\in C^{n+1}([a,b])$:
	\[
		R(f) = \int_a^b K(t) f^{(n+1)}(t) dt
	\]
	mit Peano-Kern $K(t) := \f 1{n!}R((\cdot - t)^n_+)$ unabhängig von $f$.
	\begin{proof}
		Da $f\in C^{n+1}([a,b])$, ist $f,P_nf,f-P_nf \in C^k([a,b])$
		\begin{align*}
			R(f) &\stackrel{\ref{2.29}}= R(P_n f) + R\l( \int_a^b \underbrace{K_n(t,\cdot)}_{\in C^k([a,b])} f^{(n+1)}(t) dt \r)
			&\stackrel{\ref{2.28}} =  \int_a^b R(K_n(t,\cdot)) f^{(n+1)}(t) dt \\
			&= \int_a^b R\l( \f 1{n!}(\cdot - t)^n_+ \r) f^{(n+1)}(t) dt
		\end{align*}
	\end{proof}
\end{st}

\begin{lem}[Verallgemeinerter Mittelwertsatz]
	Seien $f,g \in C^0([a,b])$ mit $g\ge 0$.
	Dann existiert $\zeta \in [a,b]$ so dass
	\[
		\int_a^b f(x) g(x) dx = f(\zeta) \int_a^b g(x) dx
	\]
	\begin{proof}
		Setze $r(t) := \int_a^b f(x)g(x) dx - f(t) \int_a^b g(x) dx \in C^0([a,b])$.
		Seien $x_{\text{min}}, x_{\text{max}} \in [a,b]$ mit
		\[
			f(x_{\text{min}}) = \min_{x\in[a,b]} f(x) , \qquad f(x_{\text{max}}) = \max_{x\in[a,b]}f(x)
		\]
		Daraus folgt $r(x_{\text{min}}) = \int_a^b (f(x)-f(x_{\text{min}})g(x) dx \ge 0$.

		Analog für $r(x_{\text{max}}) \le 0$.
		Der Zwischenwertsatz folgt, da $\xi \in[x_{\text{min}}, x_{\text{max}}]$ existiert.
	\end{proof}
\end{lem}

\begin{kor}[Fehlerdarstellung durch Monoauswertung]
	Falls Peano-Kern $K(t)$ auf $[a,b]$ keinen Vorzeichenwechsel hat, so gilt für alle $f\in C^{n+1}([a,b])$ existiert $\xi\in [a,b]$ mit
	\[
		R(f) = f^{(n+1)}(\xi) \f 1{(n+1)!} R(x^{n+1})
	\]
	\begin{proof}
		\[
			R(f) \stackrel{\ref{2.30}}= \int_a^b K(t) f^{(n+1)}(t) dt 
			\stackrel{\ref{2.31}}= f^{(n+1)}(\xi)\int_a^b K(t) dt
		\]
		Anwendung auf $x^{n-1}$:
		\[
			R(x^{n+1}) = \underbrace{(n+1)!}_{=\f {d^{n+1}}{dx^{n+1}}x^{n+1}} \int_a^b K(t) dt \quad 
			\implies \quad \int_a^b K(t) dt = \f {R(x^{n+1}}{(n+1)!}
		\]
		Dies in die vorige Gleichung eingesetzt ergibt die Behauptung.
	\end{proof}
\end{kor}

\begin{ex*}
	Anwendung auf Simpson-Quadratur auf $[-1,1]$.
	\[
		R(f) = \f 13 (f(-1) + 4f(0) + f(1)) -\int_{-1}^1 f(x) dx
	\]
	$R: C^0 ([a,b]) \to \R$, $k=0$.
	Die Simpson-Quadratur ist exakt auf $n=3$, also mit \ref{2.30} ist
	\[
		R(f) = \int_{-1}^1 K(t) \cdot f^{(4)}(t) dt \qquad f\in C^4([-1,1])
	\]
	mit
	\begin{align*}
		K(t) &= \f 1{3!}R((\cdot - t)_+^3) \\
		&= \f 1{18} ((-1-t)_+^3+ 4(0-t)_+^3 + (1-t)_+^3) - \f 16 \int_{-1}^1 (x-t)_+^3 dx \\
	\end{align*}
	Es ist $(-1-t)_+^3 = 0$ für $t\in [-1,1]$ und
	\begin{align*}
		(-t)_+^3 &= \begin{cases}
			-t^3 & t\in[-1,0] \\
			0 & t\in[0,1]
		\end{cases} \\
		(1-t)_+^3 &= (1-t)^3 \\
		\int_{-1}^1 (x-t)_+^3 dx &= \int_t^1 (x-t)^3 dx = \f 14(1-t)^4
	\end{align*}
	Also für $t\in [0,1]$:
	\begin{align*}
		K(t) &= \f 1{18} (1-t)^3 - \f 1{24} (1-t)^4 \\
			&= \f 1{72}(1-t)^3\big(4-3(1-t)\big) = \f 1{72}(1-t)^3(1+3t) \ge 0
	\end{align*}
	Für $t\in [-1,0]$:
	\begin{align*}
		K(t) &= \f 1{18}\big(-4t^3 + (1-t)^3\big) - \f 1{24}(1-t)^4 \\
		&=  \dotsb \\
		&= K(-t) \ge 0
	\end{align*}
	Da $K(t) \ge 0$ auf $[-1,1]$ folgt mit \ref{2.32} die Darstellung
	\[
		R(f) = f^{(4)}(\xi) \f {R(x^4)}{4!}
	\]
	also
	\begin{align*}
		R(x^4) &= \f 13 (1+4\cdot 0 + 1) - \int_{-1}^1 x^4 dx \\
		&= \f 23 - \f 15 x^5 \Big|_{-1}^1
		= \f {10}{15} - \f {6}{15} = \f 4{15}
	\end{align*}
	Also
	\[
		R(f) = f^{(4)}(\xi) \f 4{15} \cdot \f 1{24} 
		= \f 1{90} f^{(4)}(\xi) 
		= \f 1{2880} (b-a)^5 f^{(4)}(\xi)
	\]
	und damit wie \ref{2.12} ii)
	\[
		|R(f)| \le \f 1{2880} (b-a)^5 \cdot \|f^{(4)}\|_\infty
	\]
	Analog für Finite-Differenzen-Quotienten (Übung)
\end{ex*}


\subsection{Romberg-Verfahren}


\begin{df}[Romberg-Verfahren]
	\label{2.33}
	Sei $I_h$ eine zusammengesetzte Quadratur auf $[a,b]$ mit Diskretisierungsparameter $h\in \R^+$.
	Sei $f\in C^0([a,b])$.
	Setze $b(h) := I_h(f)$.
	Wähle $h_i := h_0 2^{-i}$ für $i=1,\dotsc,n$ mit $n\in \N$ (Romberg-Folge), so dass $b(h_i)$ wohldefiniert ist.
	Führe Richardson-Extrapolation gemäß Definition \ref{1.8} auf $\{h_i\}_{i=0}^n$ und $\{I_{h_i}(f)\}_{i=0}^n$ durch, um eine Approximation für
	\[
		\lim_{h\to 0} b(h) = \lim_{h\to 0} I_h(f) = I(f)
	\]
	\begin{note}
		\begin{itemize}
			\item
				Die Wohldefiniertheit der $b(h_i)$ ist für $\f{b-a}{h_0} \in \N$ gewährleistet.
			\item
				Die Auswertung von $b(h) = I_h(f)$ ist für kleine $h$ teuer ($\mathcal O(\f 1h)$), daher lohnt sich die Richardson-Extrapolation.
			\item
				Wie in Abschnitt 1.1 bemerkt, kann die Richardson-Extrapolation verbessert werden, falls die asymptotische Entwicklung nur $h^{qn}$ Monome enthält für $q>1$.
				Dies ist insbesondere für die zusammengesetzte Trapezregel erfüllt, wie im Folgenden hergeleitet wird.
		\end{itemize}
	\end{note}
\end{df}

\begin{df}[Bernoulli-Polynome]
	\label{2.34}
	Wir definieren rekursiv die \emph{Bernoulli-Polynome} $B_k \in \P_k$ durch
	\[
		B_0(x) := 1 \qquad
		\f d{dx} B_k(x) := k \cdot B_{k-1}(x) \quad \land \quad \forall k\ge 1 : \int_0^1 B_k(x) dx = 0
	\]
	Die \emph{Bernoulli-Zahlen} sind gegeben durch
	\[
		\beta_k := B_k(0)
	\]
	\begin{note}
		\begin{itemize}
			\item
				Die $B_k$ sind wohldefiniert durch
				\[
					\_{B_k}(x) := \int_0^x k B_{k-1}(t) dt 
					\quad \text{und} \quad
					B_k(x) := \_{B_k}(x) - \int_0^1 \_{B_k}(t) dt
				\]
			\item
				Die ersten Bernoulli-Polynome sind
				\begin{align*}
					B_0(x) &= 1  \\
					B_1(x) &= x - \f 12 \\
					B_2(x) &= x^2 - x + \f 16 \\
					B_3(x) &= x^3 - \f 32 x^2 + \f 12 x
				\end{align*}
				Die ersten Bernoullizahlen sind dann
				\[
					\beta_0 = 1, \quad
					\beta_1 = -\f 12, \quad
					\beta_2 = \f 16, \quad
					\beta_3 = 0
				\]
			\item
				Mann kann zeigen
				\begin{align*}
					B_k(x) &= \sum_{i=0}^k \binom ki \beta_i x^{k-i} \\
					\f x{e^x - 1} &= \sum_{k=0}^\infty \beta_k \f {x^k}{k!}
				\end{align*}
		\end{itemize}
	\end{note}
\end{df}

\begin{lem}[Eigenschaften]
	\label{2.35}
	Es gilt
	\begin{enumerate}[i)]
		\item
			$B_k(0) = B_k(1)$ für $k\ge 2$
		\item
			$B_k(t) = (-1)^k B_k(1-t)$ für $k\ge 0$
		\item
			$B_{2k+1}(0) = B_{2k+1}(1) = B_{2k+1}(\frac 12) = 0$ für $k\ge 1$
	\end{enumerate}
	\begin{proof}
		Es gilt
		\[
			B_k \Big(\f 12 + \tau\Big) = B_k\Big(\f 12 - \tau\Big)  + \int_{\f 12 - \tau}^{\f 12 + \tau} k B_{k-1}(t) dt
		\]
		\begin{enumerate}[i)]
			\item
				Für $\tau = \f 12$ folgt mit $\int_{0}^1 B_{k-1}(t) dt = 0$ für $k\ge 2$
				\[
					B_k(1) = B_k(0) + 0
				\]
			\item
				Zeige induktiv: $B_k$ achsensymmetrisch für gerade $k$ und punktsymmetrisch bezüglich $(\f 12,0)$ für ungerade $k$.
				Für $k=0, k=1$ ist die Aussage klar.
				Gelte die Behauptung für $k-1$.

				Falls $k$ gerade, dann ist $B_{k-1}$ punktsymmetrisch zu $(\f 12, 0)$
				\[
					B_k \Big(\f 12 + \tau\Big) = B_k(\f 12 - \tau)  + \underbrace{\int_{\f 12 - \tau}^{\f 12 + \tau} k B_{k-1}(t) dt}_{=0}
				\]
				und damit $B_k(\f 12 + \tau) = B_k(\f 12 - \tau)$

				Falls $k$ ungerade, dann ist $B_{k-1}$ achsensymmetrisch zu $x=\f 12$ und
				\[
					\int_{\f 12 - \tau}^{\f 12} B_{k-1}(t) dt = \int_{\f 12}^{\f 12 + \tau} B_{k-1}(t) dt
				\]
				Setze $c := B_k(\f 12)$
				\begin{align*}
					B_k\Big(\f 12 + \tau\Big) - c 
					&= B_k\Big(\f 12\Big) + \int_{\f 12}^{\f 12 + \tau} k B_{k=1}(t) dt - B_k\Big(\f 12\Big) \\
					&= c - \l( \int_{\f 12}^{\f 12 - \tau} k B_{k-1}(t) dt + B_k\Big( \f 12\Big) \r) \\
					&= c - B_k\Big(\f 12 - \tau\Big)
				\end{align*}
				also ist $B_k$ punktsymmetrisch zu $(\f 12, c)$.
				Außerdem
				\[
					c = \int_{0}^1 B_k(x) dx = 0
				\]
				womit $B_k$ punktsymmetrisch zu $(\f 12,0)$ ist.
			\item
				Für $k'=2k+1$ ist $B_{k'}$ punktsymmetrisc zu $(\f 12,0)$ nach ii), also $B_{k'}(\f 12) = 0$.
				$B_{k'-1}$ ist achsensymmetrisch, also
				\[
					0 = \int_0^1 B_{k'-1}(t) dt = 2 \int_0^{\f 12} B_{k'-1} (t) dt 
				\]
				Es gilt
				\[
					0 = B_{k'}\Big(\f 12\Big) = B_{k'}(0) + \int_{0}^{\f 12} k' B_{k'-1}(t) dt = B_{k'}(0)
				\]
				und $B_{k'}(1) = 0$ wegen der Punktsymmetrie.
		\end{enumerate}
	\end{proof}
\end{lem}

\begin{st}[Euler-MacLaurin'sche Summenformel]
	Sei $f\in C^{2m}([a,b])$, $m\in \N$, $h:= \f {b-a}N$, $N\in \N$ und $I_h$ die zusammengesetzte Trapezregel
	\[
		T_h(f) = \f h2 \big(f(a) + f(b)\big) + h \cdot \sum_{j=1}^{N-1} f(a+jh)
	\]
	dann gilt für den Fehler
	\[
		I_h(f) - I(f) = \sum_{k=1}^{m-1} h^{2k} \f{\beta_{2k}}{(2k)K} \Big( f^{(2k-1)}(b) - f^{(2k-1)}(a) \Big) + o(h^{2m})
	\]
	\begin{proof}
		Sei $\phi \in \C^{2m}([0,1])$, dann gilt wegen $\f{B_k'}k = B_{k-1}$, $B_1(1) = \f 12, B_1(0) = - \f 12$:
		\begin{align*}
			&\int_0^1 B_0(t) \phi(t) dt 
			= B_1(t) \phi(t) \Big|_{0}^1 - \int_0^1 B_1(t) \phi'(t) \\
			&\quad= \f 12 \big( \phi(1) - \phi(0) \big) - \f 12 B_2(t) \phi'(t) \Big|_0^1 + \int_0^1 \f 12 B_2(t) \phi'(t) dt \\
			&\quad= \f 12 \big( \phi(1) - \phi(0) \big) - \f 12 B_2(t) \phi'(t) \Big|_0^1 + \underbrace{\f 1{2\cdot 3} B_3(t) \phi''(t) \Big|_0^1}_{=0 \text{ \ref{2.35} ii)}} - \int_0^1 \f 1{3!} B_3(t) \phi^{(3)}(t) dt \\
			&\quad= \dotsc \\
			&\quad= \f 12 \big( \phi(1) - \phi(0) \big) - \sum_{k=1}^{m-1} \l( \f {B_{2k}(1)}{(2k)!} \phi^{(2k-1)}(1) - \f {B_{2k}(0)}{(2k)!} \phi^{(2k-1)}(0) \r) + \int_0^1 \f 1{(2m)!} B_{2m}(t) \phi^{(2m)}(t) \; dt \\
			&\quad\stackrel{\ref{2.35} i)}=  \f 12 \big(\phi(1) - \phi(0)\big) - \sum_{k=1}^{m-1} \f {\beta_{2k}}{(2k)} \big( \phi^{(2k-1)}(1) - \phi^{(2k-1)}(0) \big) + \int_0^1 \f {B_{2m}(t)}{(2m)!} \phi^{(2m)}(t) \; dt
		\end{align*}
		Wähle  $\phi_j(t) := h \cdot f(x_{j-1} + th)$, $x_j := a+jh$, $j=1,\dotsc, N$ (bzw. $j=0,\dotsc, N$).
		Dann gilt
		\begin{itemize}
			\item
				$\displaystyle
				\int_0^1 \phi_j(t) dt = \int_{x_{j-1}}^{x_j} f(x) dx
				$
			\item
				$\displaystyle
				\phi_j^{(k-1)}(t) = h^k f^{(k-1)}(x_{j-1} + th)
				$
			\item
				$\displaystyle
				\phi_j(1) = h f(x_j) = \phi_{j+1}(0)
				$
			\item
				$\displaystyle
				\phi_j^{2k-1}(1) = \phi_{j+1}^{2k-1}(0)
				$
		\end{itemize}
		\fixme[Vorzeichenfehler irgendwo, Beweis nachvollziehen]
		Damit folgt:
		\begin{align*}
			I(f)&=\int_a^b f(x)\, dx = \sum_{j=1}^N \int_{x_{j-1}}^{x_j} f(x) \, dx = \int_{j=1}^N \int_0^1 \phi_j(t)\, dt\\
			&= \sum_{j=1}^N \frac{1}{2}(\phi_j(0)+ \phi_j(1))- \sum_{j=1}^N \sum_{k=1}^{m-1} \frac{\beta_{2k}}{(2k)!} ( \phi_j^{(2k-1)} (1)-\phi_j^{(2k-1)}(0))+\sum_{j=1}^N \int_0^1 \frac{B_{2m}(t)}{(2m)!} \phi_j^{(2m)}(t)\, dt\\
			&= \sum_{j=1}^N \frac{h}{2} (f(x_j)+f(x_{j-1})) - \sum_{j=1}^N \sum_{k=1}^{m-1} \frac{\beta_{2k}}{(2k)!} h^{2k} (f^{(2k-1)}(x_j)-f^{(2k-1)} (x_{j-1}))+ \sum_{j=1}^N \int_0^1 \frac{B_{2m}(t)}{(2m)!} h^{2m+1} \cdot f^{(2m)}(x_{j-1}+ ht)\, dt\\
			&= I_h(f)- \sum_{k=1}^{m-1} \frac{\beta_{2k}}{(2k)!} h^{2k} (f^{(2k-1)} ( f^{(2k-1)}(b)-f^{(2k-1)}(b)-f^{(2k-1)}(a))+h^{2m}\left [h \sum_{j=1}^N \int_0^1 f^{(2m)} (x_{j-1}+ht)\, dt\right ]
		\end{align*}
		Dies ist die Behauptung falls der Term $[...]$ durch Konstante unabhängig von $h$ bechränkt werden kann. Dies ist der Fall:
		\[
			|h\sum_{j=1}^N \int_0^1 f^{(2m)}(x_{j-1}+t\cdot h) \frac{B_{2m}(t)}{(2m)!} \, dt|\le h \frac{1}{(2m)!} \sum_{j=1}^N ||f^{(2m)}||_\infty \cdot ||B_{2m}||_\infty = \underbrace{h\cdot N}_{b-a}\cdot \frac{1}{(2m)!} ||f^{(2m)}||_\infty ||B_{2m}||_\infty
		\]
		unabhängig von $h$.
	\end{proof}
\end{st}
\begin{note*}
	\begin{itemize}
		\item Die zusammengesetzte Trapezregel ist also sehr gut für periodische Funktionen
			\[
				f^{(2k-1)}(b)=f^{(2k-1)}(a) \implies I_h(f)-I(f)=O(h^{2m})
			\]
		\item Zusammengesetzte Trapezregel erlaubt asymptotische Entwicklung in $h^2$. Dies kann durch Wahl $q=2$ in der Richardson-Extrapolation/(Romberg-Verfahren) ausgenutzt werden.
	\end{itemize}
	(Anmerkung: eine Wahl für $q2$ in Richardson-Extrapolation zu wählen bedeutet, dass wir bezüglich $b(h)=\sum_{n\in \N} h^{qn} c_n$
\end{note*}
\subsection{Weiterführende Techniken}
\begin{seg}{Adaptive Quadraturen}
	Problem: Integration verfeinern, welches Bereiche mit großer Variation und Bereiche mit kleiner Variation besitzt. Es ergeben sich stets zwei grundlegende Probleme:
	\begin{enumerate}[a)]
		\item Zusammengesetzte Quadratur mit zu groben $h$: Bereiche mit hohen Variationen werden nicht aufgelöst. Dies führt zu einem großen Fehler
		\item $I_h$ mit feinem h: gesamtes Intervall gut aufgelöst, aber in glatten Bereichen ergeben sich viel überflüssige Stützstellen. Dies führt zu einer langen Rechenzeit.
	\end{enumerate}
\end{seg}
\begin{seg}{Idee:}
	Dies motiviert folgendes:
	\begin{itemize}
		\item unterschiedliche Teilintervall-Größen
		\item automatische Anpassung der Teilintervall-Größen: Beginne mit grober Intervall-Zerlegung, Berechne Quadratur, berechne einen Fehlerschätzer für alle Teilintervalle markiere alle Teilintervalle, auf denen der Fehler zu groß ist, halbiere die markierten Intervalle und wiederhole diese Schritte bis gewünschte Genauigkeit erreicht ist.
	\end{itemize}
\end{seg}
\begin{seg}{Allgemeines Prinzip:}
	\begin{figure}[ht]
		\fbox{Lösung $\rightarrow$ Schätzung $\rightarrow$ Markierung $\rightarrow$ Verfeinerung}
\end{figure}
und dieser Schritt wiederholt sich schließlich.
\end{seg}
\begin{alg*}
	\begin{algorithmic}
		\Require $f:[a,b]\to \R$
		\Require $\Delta^{(0)}:=\{x_0^{(0)},..., x_{n_0}^{(0)}\}$, Anfangszerlegung $a=x_0^{(0)}<...<x_{n_0}^{(0)}=b$. 
		Wir bezeichnen im Folgenden $K_i^{(0)}:=[x_{i-1}^{(0)},..., x_{i}^{(0)}]$ Teilintervalle von $\Delta^{(0)}$.
		\Require $L_max$: Maximalzahl der Verfeinerungsiterationen
		\Require $\epsilon_{tol}>0$: gewünschte Toleranzgrenze
		\Require $\theta\in (0,1)$: Anteil der in jeder Iteration zu verfeinernden Intervalle.
		\State $l:=0$
		\State fertig$:= 0$ % Alternativ do-while Schleife (weiß nicht wie ich das definiere
		\While {fertig $\neq 1$}
		\State 1. $I_h(f; \Delta^{(l)}):= \sum_{i=1}^{n_l} I_h (f, f_{k}^{(2)})$ als zusamengesetzte Quadratur bezüglich einer einfachen Quadratur $I_h$, zum Beispiel Trapezregel.
		\State 2. Berechne $\eta_i^{(l)}, i=1,..., n_l$ lokale Fehlerschätzer. zum Beispiel $\eta_i^{(l)}=|I_n(f, K_i^{(l)})- I_{\hat n} (f, K_i^{(l)})|$ mit $I_{\hat n}$ "`bessere Quadratur"' zum Beispiel Simson-Regel.
		\State 3. $\eta^{(l)}=\sum_{i=1}^{n_l} \eta_i^{(l)}$ Gesamtfehlerschätzer
		\State 4.
		\If{$\eta^{(l)}\le \epsilon_{tol}$ oder $l\ge l_max$} fertig$:= 1$ \Else fertig$:=0$ \EndIf
		\State 5. \If {fertig$\neq 1$}
		\State $\eta_{max}^{(l)}:= \max \eta_i^{(l)}:= \max \eta_i^{(l)}$ maximaler lokaler Fehlerschätzer
		\State $m_i^{l}:=\begin{cases} 1 &, \text{falls $\eta_i^{(l)}>\theta, \eta_{\max}^{(l)}$ (soll verfeinert werden)}\\ 0 &, \text{sonst}\end{cases}$
		\State $\Delta^{(l+1)}:= \Delta^{(l)} \cup \{ \frac{x_{i-1}^{(l)}+x_{i}^{(l)}}{2}$, für $i$ mit $m_i^{(l)}=1\}$
		\State $l:=l+1$
		\EndIf 
		\EndWhile
		\Return $I_h^{(l)}$ (eventuell $\Delta^{(l)}, l, \eta_i^{(l)}, \eta^{(l)}$, etc. )
	\end{algorithmic}
\end{alg*}
\begin{note*}
	Alternativen:
	\begin{itemize}
		\item andere Markierungsstrategien
			\begin{itemize}
				\item Wähle $k\in \N$, markiere in jeder Iteration die $k$ Intervalle mit $\max \eta_i^{(l)}$
				\item Wähle $\theta$: Anteil der Intervalle zur Verfeinerung
				\item Verwende ein Extrapolationsverfahren oder $\{I_n(f, k_i^{(l)}\}$ um den erwarteten Fehlerabfall vorherzusagen $\rightarrow$ siehe Stoer-Bulirsch.
		\end{itemize}
	\item Art der Verfeinerung ist austauschbar (andere/mehr Zwischenpunkte)
	\item Abbruchkriterium ist modifizierbar
		\begin{itemize}
			\item Wähle $\sigma\in (0,1)$, Abbruch falls Änderung zu gering:
				\[
					|I_h(f,\Delta^{(l)})-I_{h}(f, \Delta^{(l-1)} )|\le \sigma \epsilon_{tol}
				\]
			\item Falls $\eta_i^{(l)}$ durch verbesserte Quadratur $I_{\hat h}$ berechnet werden, kann man auch $\hat I_h(f, \Delta^{(i)}):=\sum_{i=1}^{n^{(l)}} I_{\hat h} (f, K_i^{(n)})$ als bessere QUadratur zurückgeben.
		\end{itemize}
\end{itemize}
\end{note*}
\begin{note*}[Effizienz und Zuverlässigkeit]
	\begin{itemize}
		\item Sei $E_h(f, \Delta^{(l)}):= \sum_{i=1}^{n^{(l)}} |I_h(f,K_i^{(l)} - \int_{K_i^{(l)}} f(x)\, dx|$ Summe der lokalen Integrationsfehler.
		\item Wir nennen einen Fehlerschätzer $\eta^{(l)}$ \emph{zuverlässig}, falls er (bis auf Faktor) obere Schranke für Fehler ist, dass heißt es existiert $\gamma >0$ unabhängig von $l$ und $\Delta^{(l)}$ mit 
			\[
				E_h(f, \Delta^{(l)}\le \gamma \cdot \eta^{(l)}
			\]
		\item Wir nennen $\eta^{(l)}$ \emph{effizient}, falls er den Fehler nicht beliebig überschätzt, dass heißt es existiert $T>0$ unabhängig von $l$ und $\Delta^{(l)}$ mit 
			\[
				\eta^{(l)}\le T \cdot E_n(f, \Delta^{(l)})
			\]
	\end{itemize}
	\fixme[Ab und zu n und h verwechselt]
\end{note*}

\begin{st}[Effizienz und Zuverlässigkeit der $\eta_i^{(l)}$]
	\label{2.38}
	Sei $\eta^{(l)} = \sum_{i=1}^{n^{l}} \eta_v^{(l)}$, $\eta_i^{(l)} = | I_n(f, K_i^{(l)} - I_{\hat n}(f, K_i^{(l)}|$ und
	\[
		R_n (f, K_i^{()}) := I_n(f,K_i^{(l)}) - \int_{K_i^{(l)}} f(x) dx
	\]
	als lokales Fehlerfunktional (analog $R_{\hat n}$ für $I_{\hat n}$).
	Sei außerdem $h_0 := \max |K_i^{(0)}|$.

	Falls $q \in (0,1)$ existiert, so dass
	\[
		| R_{\hat n}(f,K)| \le q |R_n(f,K)|
		\qquad \forall K\subset [a,b], |K| \le h_0
	\]
	So ist $\eta^{(l)}$ zuverlässiger und effizienter Schätzer für $E_h(f,\Delta^{(l)})$.
	\begin{proof}
		Es gilt
		\begin{align*}
			|R_n(f,K_i^{(l)}|
			&\le \Big|I_n(f,K_i^{(l)}) - I_{\hat n}(f,K_i^{(l)} \Big| + \l|I_{\hat n}(f,K_i^{(l)}) - \int_{K_i^{(l)}}f(x) dx \r| \\
			&= \eta_i^{l} + |R_{\hat n}(f,K_i^{(l)}) | \\
			&\le \eta_i^{(l)} + q |R_n(f,K_i^{(l)})|
		\end{align*}
		Also ist
		\[
			\l|R_n(f,K_i^{(l)}\r| \le \f 1{1-q} \eta_i^{(l)}
		\]
		und damit
		\begin{align*}
			E_h(f, A^{(l)} = \sum_{i=1}^{\eta^{(l)}} \l|R_n(f, K_i^{(l)} \r| \le \f 1{1-q} \sum_{i=1}^{\eta^{(l)}} \eta_i^{(l)} = \f 1{1-q} \eta^{(l)}
		\end{align*}
		also zuverlässig mit $\gamma = \f 1{1-q}$.

		Für die Effizienz gilt
		\begin{align*}
			\l|R_n(f, K_i^{(l)}) \r| 
			&\ge \eta_i^{(l)} - q \l| R_n(f, K_i^{(l)}) \r| \\
		\end{align*}
		Also ist
		\[
			\l|R_n(f, K_i^{(l)}\r| \ge \f 1{1+q} \eta_i^{(l)}
		\]
		und
		\[
			\eta^{(l)} = \sum_{i=1}^{\eta^{(l)}}{\eta_i^{(l)}} \le (1+q) \sum_{n=1}^{\eta^{(l)}} | R_n(f, K_i^{(l)}) | = (1+q) E_h
		\]
		also effizient mit $\Gamma = 1+q$.
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				Falls die adaptive Quadratur abbricht mit $\eta^{(l)} \le \eps_{\text{tol}}$ und die Vorraussetzungen von \ref{2.37} erfüllt sind, dann ist der Integrationsfehler garantiert klein
				\[
					\Big| I_h(f,\Delta^{(l)} - \int_a^b f(x) dx \big|
					\le \sum_{i=1}^{\eta^{(l)}} \Big| I_n(f,K_i^{(l)} - \int_{K_i^{(l)}} f(x) dx \Big|
					= E_h(f,\Delta^{(l)})
					\le \f 1{1-q} \eta^{(l)}
					\le \f 1{1-q} \eps_{\text{tol}}
				\]
			\item
				Man nennt die Bedingung an die $R_{\hat n, K}$ auch „Saturierungsbedingung“, welche meistens durch genügend feine Initialzerlegung erreicht werden kann.

				Sei z.B. $I_n$ die Trapez- und $I_{\hat n}$  die Simpsonregel.
				Nach der Peano-Fehlerdarstellung gilt für $f\in C^4([a,b])$.
				\begin{align*}
					|R_{\hat n}(f, K)| &= \f {|K|^5}{2880} |f^{(4)(\hat \xi)}
					|R_{n}(f, K)| &= \f {|K|^3}{12} |f^{(3)(\xi)}
				\end{align*}
				also für $f^{(2)}(\xi) \neq 0$:
				\begin{align*}
					|R_{\hat n}(f,K)| = \f {|K|^2}{240} \cdot \f {|f^{(4)}(\hat \xi)}{|f^{(2)}(\xi)|} |R_n(f,K)|
				\end{align*}
				Sei $m := \min |f''(x)| > 0$, $M := \|f^{(4)}\|_\infty$.
				Wähle $\Delta^{(0)}$ so fein, dass $h_0^2 \le \f {120 m}M$.
				Wegen $|K| \le h_0$ folgt
				\[
					\big| R_{\hat n}(f, K) \big| 
					\le \f {120m}M \cdot \f{M}{240m} |R_n(f,K)|
					= \f 12 |R_n(f,K)|
				\]
				also ist die Saturierungsbedingung mit $q=\f 12$ erfüllt.
		\end{itemize}
	\end{note}
\end{st}

\subsection{Monte-Carlo-Verfahren}

Wir wählen die Stützpunkte für eine Quadratur nicht als Nullstellen von Polynomen, sondern zufällig.
Die Gewichte wählen wir dabei konstant.

\begin{df}[Monte-Carlo-Approximation]
	\label{2.38}	
	Sei $f:[0,1]^d \to \R$ integrierbar und $n\in \N$.
	Wähle $\{x_i\}_{i=1}^n$ zufällig gemäß einer Gleichverteilung.
	Dann ist
	\[
		\f 1n \sum_{i=1}^n f(x_i)
	\]
	eine Monte-Carlo-Approximation für
	\[
		\int_{[0,1]^d} f(x) dx
	\]
\end{df}

\begin{nt*}
	\begin{itemize}
		\item
			In der Praxis werden die $x_i$ durch einen Zufallszahlen-Generator erzeugt.
	\end{itemize}
\end{nt*}

\subsubsection{Motivation aus der Stochastik}

(ohne Begriffserläuterungen und Definitionen, für Details siehe z.B. R.E. Caflisch: “Monte-Carlo and quasi-Monte-Carlo Methods”, Acta Numerica, 7:1-49, 1998)

\begin{itemize}
	\item
		Sei $X$ gleichverteilte Zufallsvariable mit Werten in $[0,1]^d$.
		Dann ist $f(X)$ eine Zufallsvariable und der \emph{Erwartungswert} von $f(X)$ ist
		\[
			E[f(X)] := \int_{[0,1]^d} f(x) dx =: I
		\]

		Der sogenannte \emph{empirische Erwartungswert} ist
		\[
			\f 1n \sum_{i=1}^n f(x_i)
		\]
		zu Realisierungen $\{x_i\}$ von $X$ und stellt eine Näherung für $I$ dar.
	\item
		Die Güte und Konvergenz der Monte-Carlo-Approximation muss ebenfalls mit Mitteln der Stochastik bewertet werden.
	\item
		Seien $X_1, \dotsc, X_n$ unabhängig gleichverteilte Zufallsvariablen mit Werten in $[0,1]$.
		Dann ist
		\[
			I_n := \f 1n \sum_{i=1}^n f(X_i)
		\]
		wieder eine Zufallsvariable mit Erwartungswert
		\[
			E[I_n] := E\l[\f 1n \sum_{i=1}^n f(X_i)\r] = \f 1n \sum_{i=1}^n E[f(X_i)] = \f 1n \sum_{i=1}^n I = I
		\]
	\item
		Man kann auch zeigen, dass für die quadratische Abweichung vom Erwartungswert (auch \emph{Varianz} genannt) gilt:
		\[
			\Var(I_n) := E[(I_n-E[I_n])^2] = \f 1n \Var(f(X)) = \f 1n \int_{[0,1]^d} (f(x) - I)^2 dx = \f 1n \sigma_f^2
		\]
		also gilt für die Standardabweichung (Wurzel aus der Varianz):
		\[
			\sigma_{I_n} = \sqrt{\Var(I_n)} = n^{-\f 12} \sigma_f
		\]
	\item
		Seien ${x_i^{(k)}}_{i=1}^n$ für $k=1,\dotsc, k_{\text{max}}$ Realisierungen der Zufallsvariablen $\{X_i\}_{i=1}^n$ mit $k_{\text{max}}$ groß.
		Dann erhalten wir $k_{\text{max}}$ viele Realisierungen $I_n^{(k)}\in \R$ der Monte-Carlo-Approximation für $I$.

		Der \emph{empirische Erwartungswert} der $I_n^{(k)}$ ist ungefähr $I$
		\[
			\f 1{k_{\text{max}}} \sum_{k=1}^{k_{\text{max}}} I_n^{(k)} \approx I
		\]
		und die empirische Varianz von $I_n$ ist in etwa $\Var(I_n)$
		\[
			\f 1{k_{\text{max}}} \sum_{k=1}^{k_{\text{max}}}(I_n^{(k)} - I)^2 \approx \Var(I_n) = n^{-1} \sigma_f^2
		\]
		Dies ist ein Maß für die Streuung der Werte $I_n^{(k)}$ um $I$.
	\item
		Die Konvergenzrate $\sigma_{I_n} \in O(n^{-\f 12})$ nennt man „algebraische Konvergenzrate“.
		Diese ist recht langsam und ein Nachteil der Monte-Carlo-Approximation.
		
		Beispielsweise erfordert die Verbesserung um Faktor 10 das Erhöhen von $n$ um Faktor 100.
	\item
		Der große Vorteil ist, dass die Konvergenzrate $\sigma_{I_n}$ unabhängig von der Dimension $d$ ist.
\end{itemize}

\begin{nt*}[Anwendungen]
	\begin{itemize}
		\item
			Zur Flächen- und Volumenberechnung:

			Sei $D\subset [0,1]^d$, dann ist das Volumen von $D$ approximiert durch
			\[
				|D| = \int_{[0,1]^d} \1_d(x) dx \approx \f 1n \sum_{i=1}^n \1_D (x_i)
			\]
			also durch die Monte-Carlo-Approximation für das Integral von $f(x) := \1_D(x)$.
		\item
			Hochdimensionale Integration:

			Für Integrale im $\R^d$ können Newton-Cotes-, Gauß- und zusammengesetzte Quadraturen verallgemeinert werden (Koordinatenweise Quadratur).
			Für hohe $d$ ist das jedoch inpraktikabel, da für $n$ Intervalle pro Achse (also $n+1$ Punkte pro Achse) $(n+1)^d$ viele Quadraturpunkte benötig werden.
			Die Anzahl der Stützstellen wächst also exponentiell in $d$ (“Curse of Dimensionality”).

			Im Gegensatz dazu ist die Monte-Carlo-Approximation in $\R^d$ einfach und bildet oft die einzige Möglichkeit, hochdimensionale Integrale hinreichend genau zu berechnen (“Breaking the curse of dimensionality”).
	\end{itemize}		
\end{nt*}

\begin{ex*}[Berechnung von $\pi$]
	Sei $D = K_{\f 12}(\f 12, \f 12)$, dann ist $|D| = \f \pi 4$.
	Wir berechnen das jetzt mit dem Monte-Carlo-Algorithmus.
	\[
		|D| = \int_{[0,1]^2} \1_D = I
	\]
	Für die Varianz gilt
	\[
		\Var(\1_D(X)) = \int_{[0,1]}(f(x) - \tfrac \pi 4)^2 dx = (1-\tfrac \pi 4) \f \pi 4
	\]
	\begin{figure}[h]
		\centering
		\caption{Beispiel für $k_\text{max}=200, n=1,\dotsc, 500$}
		\fixme[Bilder zur Verteilung siehe Programme im Ilias]
	\end{figure}
\end{ex*}

\section{Nichtlineare Gleichungsysteme}

\section{Optimierung}


\end{document}

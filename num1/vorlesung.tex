\documentclass[11pt]{scrbook}
\usepackage{mathe-vorlesung}

\title{Numerische Mathematik 1}
\author{}

\begin{document}

\maketitle

\tableofcontents
\newpage

\chapter{Approximation und Interpolation}

Sei $V= \{\Phi(x;a_0,\dotsc, a_n) \in C(\R) : a_0,\dotsc,a_n\in \R\}$ die Menge der stetigen Funktionen für $x\in \R$, deren Elemente durch Parameter $a_0,\dotsc,a_n$ parametrisiert sind.

Seien weiter Stützstellen $\{x_i\}_{i=0}^n\subset \R$ und Zielwerte $\{f_i\}_{i=0}^n \subset \R$ gegeben (z.B. Messungen oder Funktionwerte einer komplizierten (deren Auswertung kostspielig ist) Funktion $f_i=f(x_i)$, oder das Ergebnis eines Computerprogramms).

Ziel ist es, Parameter $a_0,\dotsc, a_n$ zu finden, so dass
\[
	\Phi(x_i;a_0,\dotsc, a_n) = f_i \qquad \forall i= 0,\dotsc,n \qquad \text{„Interpolation“}
\]
oder allgemeiner so dass
\[
	\sum_{i=0}^n(\Phi(x_i; a_0,\dotsc a_m) -f_i)^2 \qquad \text{minimal}
\]
(„Least-squares-Approximation“)

\begin{ex*}
	\begin{enumerate}
		\item 
			Polynominterpolation:
			\[
				\Phi(x; a_0,\dotsc, a_n) = \sum_{k=0}^na_kx^k \in \P_n
			\]
		\item
			Trigonometrische Interpolation:
			\[
				\Phi ( x;a_0,\dots,a_m,b_1,\dots, b_m ) = \frac{a_{0}}{2} + \sum_{k=1}^m ( a_k \cos kx + b_k \sin kx )
			\]
		\item
			Spline-Interpolation:
			Sei $a=x_0< x_1 < \dotsb < x_n = b$, $q,r\in \N_0$, $q\le r$
			\[
			V := \Big\{ f\in C^q([a,b]) : f_{[x_i,x_{i+1}]}\in \P_r, i=0,\dotsc,n-1 \Big\}
			\]
			(global $q$-mal stetig differenzierbare Fuktionen, stückweise polynomial vom Grad $r$)
		\item
			Exponentielle Interpolation (nichtlinear)
			\[
			\Phi(x;a_0,\dotsc, a_m,\lambda_0,\dotsc, \lambda_m) = \sum_{k=0}^m a_ke^{\lambda_k x}
			\]
		\item
			Rationale Interpolation
			\[
				\Phi(x; a_0,\dotsc, a_m, b_0,\dotsc, b_{\_m}) = \f {a_0 + a_1x + \dotsb + a_m x^m}{b_0 + b_1x + \dotsb + b_{\_m}x^{\_m}}
			\]
	\end{enumerate}
\end{ex*}

Potentielle Fragestellungen:
\begin{itemize}
	\item 
		Ist die Approximations-/Interpolationsaufgabe zu gegebenem $V$ und Daten $(x_i,f_i)$ lösbar?
		Wenn ja, ist die Lösung eindeutig?
	\item
		Wie finden wir algorithmisch die Koeffizienten?
	\item
		Können wir Fehleraussagen treffen?
	\item
		Im Falle, dass die Daten aus Funktionsauswertungen stammen:
		Was ist die optimale Stützstellenwahl?
	\item
		\dots
\end{itemize}


\section{Polynominterpolation}

Sei $\{x_i\}_{i=0}^n$, $\{f_i\}_{i=0}^n$ gegeben mit $x_i\neq x_j$ für $i\neq j$.

\begin{st}[Existenz und Eindeutigkeit]
	\label{1.1}
	Es existiert genau ein Polynom $p\in \P_n$ mit $p(x_i)=f_i$ für $i=0,\dotsc,n$.
	\begin{proof}
		$p(x) = \sum_{k=0}^n a_k x^k$ löst das Interpolationsproblem $p(x_i)=f_i$ genau dann, wenn $a=(a_k)_{k=0}^n \in \R^{n+1}$ löst $a_0 +a_1x_i^1 + \dotsb a_nx_i^n = f_i$ für $i=0,\dotsc,n$.
		Das ist äquivalent zu $Aa=f$ mit
		\[
		A= \begin{pmatrix} 1 & x_0 & x_0^2 & \hdots & x_0^n \\
		\vdots  \\
		1 & x_n & x_n^2 & \hdots & x_n^n\end{pmatrix}, f= \begin{pmatrix}f_0 \\ \vdots \\ f_n\end{pmatrix}
		\]
		Wir zeigen, dass $A$ regulär ist, bzw. $\ker(A) = \{0\}$.
		Sei $\_a \in \R^{n+1}, A\_a=0$, dann erfüllt
		$\_p(x) := \sum_{k=0}^n\_a_k x^k$ die Gleichung $\_p(x_i)=0$.
		$\_p$ ist Polynom von Grad $n$ und hat $n+1$ Nullstellen. 
		Also 
		\[
			\_p=0 \implies \_a_k=0 \implies \_a=0
		\]
		Damit ist $A$ regulär, das LGS $Aa=f$ eindeutig lösbar und $p$ damit eindeutig bestimmt.
	\end{proof}
	\begin{note}
		\begin{enumerate}
			\item 
				Darstellung in Monombasis $p(x)= \sum_{k=0}^n a_kx^k$ heißt \emph{Normalform} des Interpolationsproblems.
			\item
				Die Matrix $A$ aus dem Beweis ist die \emph{Vandermondematrix}, typischerweis schlecht konditioniert und voll besetzt.
				Daher ist das zugehörige LGS $Aa=f$ recht aufwändig zu lösen.
			\item
				Bei Verwendung anderer Basen ist das Interpolationsproblem leichter zu lösen.
			\item
				Allgemeine lineare Interpolation:
				Seien linear unabhängige Polynome $\{\phi_k\}_{k=0}^n \subset \P_n$ gegeben.
				Dann gilt
				\begin{align*}
					p(x) = \sum_{k=0}^n a_k \phi_k(x) &\text{ löst die Interpolationsaufgabe}\\
					\iff \qquad a\in \R^{n+1} &\text{ löst } Aa=f \text{ mit } f=(f_i)_{i=0}^n \text{ und } A= \begin{pmatrix}\mid & \cdots & \mid \\
						\phi_0(x) & \cdots & \phi_n(x)\\
						\mid & \cdots & \mid
					\end{pmatrix}
				\end{align*}
				Dieses Prinzip wenden wir im Folgenden auf die Lagrange-Polynome (\ref{1.2}) und die Newton-Polynome (\ref{1.3}) an.
		\end{enumerate}
		
	\end{note}
\end{st}

\begin{st}[Lagrange-Form]
	\label{1.2}
	Die \emph{Lagrange-Polynome}
	\[
		L_k^n(x) = \prod_{\substack{i=0 \\ i\neq k}}^n \f {x-x_i}{x_k-x_i} \qquad k=0,\dotsc,n
	\]
	erfüllen
	\[
	L_k^n(x_i) = \delta_{ik} \qquad i,k=0,\dotsc, n
	\]
	und bilden eine Basis für $\P_n$ und das Interpolationspolynom hat sogenannte \emph{Lagrange-Form}
	\[
		p(x) = \sum_{k=0}^n f_k L_k^n(x)
	\]
	\begin{proof}
		siehe NLA.		
	\end{proof}
	\begin{note}
		\begin{enumerate}
			\item Wegen $L_k^n(x_i)=\delta_{ik}$ nennt man die $\{L_k\}_{k=0}^n$ eine \emph{nodale Basis}.
			\item Lösen eines LGS entfällt, da für die zugehörige Matrix $A=I$ gilt.
			\item Die $L_k^n(x)$ sind recht aufwändig auszuwerten ($\mathcal O(n^2)$).
			\item Die Basen sind nicht hierarchisch, d.h. $\{L_k^n\} \not\subset \{L_k^{n'}\}$ für $n'> n$.
		\end{enumerate}
	\end{note}
\end{st}

\begin{df}[Newton-Polynome, Newton-Form]
	\label{1.3}
	Wir nennen
	\[
		N_k^n(x) := \prod_{i=0}^{k-1}(x-x_i) \qquad k=0,\dotsc,n
	\]
	\emph{Newton-Polynome} und die Darstellung des Interpolationspolynoms
	\[
		p(x) = \sum_{k=0}^n a_k N_k^n(x)
	\]
	\emph{Newton-Form} der Interpolierenden
\end{df}

\begin{note*}
	Im Gegensatz zu den Lagrange-Polynomen ($\deg(L_k^n) = n$) gilt für die Newton-Polynome:
	\[
		\deg(N_k^n) = k
	\]
\end{note*}

\begin{lem}[Eigenschaften der Newton-Polynome]
	\label{1.4}
	\begin{enumerate}[i)]
		\item $N_k^n(x_j) = 0$ für $0\le j \le k$ 
		\item $\{N_k^n\}_{k=0}^n$ bilden eine Basis für $\P_n$
		\item Die Basen sind hierarchisch, d.h. $\{N_k^n\}\subset \{N_k^{n'}\}$ für $n'> n$
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item klar
			\item
				$N_k^n \in \P_n$ ist klar, zeige lineare Unabhängigkeit.
				Sei $a\in \R^{n+1}$ mit $0=\sum_{k=0}^n a_k N_k^n(x)$.
				Angenommen $a\neq 0$, wähle $k_0 := \max\{k : a_k \neq 0\}$.
				Also
				\[
					0 \neq N_{k_0}^n = - \f 1{a_{k_0}} \sum_{k=0}^{k_0-1}a_k N_k^n(x)
				\]
				Also hat $N_{k_0}$ Grad $k_0$ und ist Summe von Polynomen niedrigeren Grades, ein Widerspruch.
				Damit ist $a=0$ und $\{N_k^n\}$ linear unabhängig.
			\item klar
		\end{enumerate}
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item Die Interpolationsmatrix ist eine untere Dreiecksmatrix
			\item LGS ist durch Vorwärtseinsetzen in $\mathcal O(n^2)$ lösbar
			\item Alternative Berechnung: Dividierte Differenzen (später)
		\end{itemize}
	\end{note}
\end{lem}

\begin{ex*}
	Sei $n=2$ und folgende Datenmenge gegeben.	
	\begin{table}[H]
		\centering
		\begin{tabular}{l|rrr}
			$x_i$ & 1 & 2 & 3  \\ \hline
			$f_i$ & 2 & 3 & 6 
		\end{tabular}
	\end{table}

	Wegen $N_0^2(x) = 1, N_1^2(x) = (x-1), N_2^2(x) = (x-1)(x-2)$ ist das zugehörige LGS von der Form
	\[
	\begin{pmatrix}1&0&0\\1&1&0\\1&2&2\end{pmatrix}\begin{pmatrix}a_0\\a_1\\a_2\end{pmatrix} = \begin{pmatrix}2\\3\\6\end{pmatrix}
	\]
	Es ergibt sich $a_0=2,a_1=1, a_2=1$ und damit das Interpolationspolynom
	\[
		p(x) = 2\cdot 1 + 1\cdot(x-1) + 1\cdot(x-1)(x-2) = x^2 - 2x+3
	\]
\end{ex*}

\begin{st}[Rekursionsformel]
	\label{1.5}
	Sei $p_{i,j}\in \P_j$ das Interpolationspolynom zu den Daten $\{x_l,f_l\}$, $l=i,\dotsc,i+j$.
	Dann gilt für $j\ge 1$
	\[
		p_{i,j}(x) = \f {(x-x_i)p_{i+1,j-1} - (x-x_{i+j})p_{i,j-1}}{x_{i+j}-x_i}
	\]
	\begin{proof}
		Es gilt
		\begin{align*}
			p_{i+1,j-1}(x_l) &= f_l \qquad l=i+1,\dotsc,i+j\\
			p_{i,j-1}(x_l) &= f_l \qquad l=i,\dotsc,i+j-1
		\end{align*}
		Mit
		\[
			q(x) := \f {(x-x_i)p_{i+1,j-1} - (x-x_{i+j})p_{i,j-1}}{x_{i+j}-x_i}
		\]
		gilt $q\in \P_j$ und die Randpunkte werden interpoliert:
		\begin{align*}
			q(x_i) &= p_{i,j-1}(x_i) = f_i\\
			q(x_{i+j}) &= p_{i+1,j-1}(x_{i+j}) = f_{i+j}
		\end{align*}
		für die Zwischenstellen $l=i+1,\dotsc,i+j-1$ gilt
		\begin{align*}
			q(x_l) &= \f {(x_l-x_i)p_{i+1,j-1}(x_l) - (x_l-x_{i+j})p_{i,j-1}(x_l)}{x_{i+j}-x_i}\\
				   &= \f {(x_l-x_i)f_l - (x_l-x_{i+j})f_l}{x_{i+j}-x_i} = f_l
		\end{align*}
		Damit ist $q(x)$ ein Interpolationspolynom zu den Daten $\{x_l,f_l\}_{l=i}^{i+j}$ und wegen der Eindeutigkeit aus \ref{1.1} $p_{i,j}=q$.
	\end{proof}
\end{st}

\begin{df}[Dividierten Differenzen]
	\label{1.6}
	Wir definieren rekursiv
	\begin{align*}
		f[x_i] &:= f_i\\
		f[x_i,\dotsc,x_{i+j}] &:= \f {f[x_{i+1},\dotsc,x_{i+j}] - f[x_i,\dotsc,x_{i+j-1}]}{x_{i+j}-x_i}
	\end{align*}
\end{df}

\begin{st}[Newton-Form über Dividierten Differenzen]
	\label{1.7}
	Das Interpolationspolynom hat Darstellung
	\[
		p(x) = f[x_0]N_0^n(x) + f[x_0,x_1]N_1^n(x) + \dotsb + f[x_0,\dotsc,x_n]N_n^n(x)
	\]
	\begin{proof}
		Sei die Newton-Form
		\[
			p(x) := a_0 + a_1(x-x_0) + a_2(x-x_0)(x-x_1) + \dotsb + a_n\prod_{i=0}^{n-1}(x-x_i)
		\]
		gegeben.
		Dann ist
		\[
			p_{0,k}(x) := a_0 + a_1(x-x_0) + \dotsb + a_k\prod_{i=0}^{k-1}(x-x_i)
		\]
		Interpolierende zu den Daten $(x_j,f_j)$ für $j=0,\dotsc,k$, denn
		\begin{align*}
			f_j = p(x_j) &= \underbrace{\sum_{k'=0}^ka_{k'}N_{k'}^n(x_j)}_{p_{0,k(x_j)}} + \sum_{k'=k+1}^n a_{k'}\underbrace{\prod_{i=0}^{k'-1}(x_j-x_i)}_{=0}\\
			&= p_{0,k}(x_j)
		\end{align*}
		also ist $a_k$ Koeffizient vor dem höchsten Term in $p_{0,k}$.
		Sei $a_{i,j}$ der Koeffizient des höchsten Terms in $p_{i,j}$.

		Es ergibt sich mit \ref{1.5} für den höchsten Koeffizienten:
		\begin{align*}
			a_{i,j} &= \f {a_{i+1,j-1} - a_{i,j-1}}{x_{i+j}-x_i}\\
			a_{i,0} &= f_i
		\end{align*}
		Betrachte \ref{1.6} und identifiziere $a_{i,j}$ mit $f[x_i,\dotsc,x_{i+j}]$, damit ist:
		\[
			a_{i,j} = f[x_i,\dotsc, x_{i+j}]
		\]
		also insbesondere
		\[
			a_k = a_{0,k} = f[x_0,\dotsc, x_k]
		\]
	\end{proof}
\end{st}

\subsection{Neville-Schema}

Die Rekursion aus \ref{1.6} kann man als Schema darstellen:

\begin{table}[H]
	\centering
	\begin{tabular}{cccc}
	$x_0$ & $f[x_0]=f_0$ & $f[x_0,x_1]$ & $f[x_0,x_1,x_2]$ \\
	$x_1$ & $f[x_1]=f_1$ & $f[x_1,x_2]$ &  \\
	\vdots & \vdots & \vdots\\
	$x_n$ & $f[x_n]=f_n$ &\\
	\end{tabular}
\end{table}

\begin{ex*}
	Sei $n=2$ und folgende Daten gegeben
	\begin{table}[H]
		\centering
		\begin{tabular}{l|rrr}
			$x_i$ & 1 & 2 & 3  \\ \hline
			$f_i$ & 2 & 3 & 6 
		\end{tabular}
	\end{table}
	Nutze das Neville-Schema, um die Koeffizienten in der Newton-Basis zu ermitteln:
	\begin{table}[H]
		\centering
		\begin{tabular}{c|ccc}
			$x_i$ & $f_i=a_{i,0}$ & $a_{i,1}$ & $a_{i,2}$ \\ \hline
			1 & \emph{2} & \emph{1} & \emph{1} \\
			2 & 3 & 3 & \\
			3 & 6 & &
		\end{tabular}
	\end{table}
	Es ergibt sich das Interpolationspolynom
	\[
		p(x) = 2+ 1(x-1) + 1(x-1)(x-2) = x^2 -2x+3
	\]
\end{ex*}

\begin{note}
	\begin{itemize}
		\item Falls nach der Interpolation ein neues Datenpaar $(x_{n+1},f_{n+1})$ zur Verfügung steht, kann das alte Neville-Schema wiederverwendet werden.
			Es muss nur unterhalb der Diagonalen jeweils ein neuer Wert berechnet werden.
		\item
			Dividierte Differenzen $f[x_i,\dotsc,x_{i+j}]$ sind symmetrisch bezüglich den Daten, d.h. bei Permutation $\tau: \{i,\dotsc,i+j\} \to \{i,\dotsc,i+j\}$ gilt
			\[
				f[x_{\tau(i)},\dotsc,x_{\tau(i+j)}] = f[x_i,\dotsc,x_{i+j}]
			\]
	\end{itemize}
\end{note}

\subsection{Punkterweiterung der Interpolierenden}

\begin{enumerate}[a)]
	\item 
		Falls die Newton-Form vorliegt, erlaubt das \emph{Horner-Schema} die effiziente Auswertung durch geschickte Klammerung.
		\begin{align*}
			p(x) &= a_0 + a_1(x-x_0) + \dotsb + a_n\prod_{i=0}^{n-1}(x-x_i)\\
				&= \bigg(\dotso \Big(a_n(x-x_{n-1}) + a_{n-1}\Big)(x-x_{n-2}) + \dotso \bigg)(x-x_0) +  a_0
		\end{align*}
		Statt $\mathcal O(n^2)$ Produkte sind nur  $\mathcal O(n)$ Produkte erforderlich.
	\item
		Man kann das Interpolationspolynom auswerten, ohne es vorliegen zu haben.
		Verwende dazu das Neville-Schema für die Rekursion aus Satz \ref{1.5} nur für Stelle $x$.
		\begin{table}[H]
			\centering
			\begin{tabular}{cccc}
				$x_0$ & $f_0=p_{0,0}(x)$ & $p_{0,1}(x)$ & $p_{0,2}(x)$ \\
				$x_1$ & $f_1=p_{1,0}(x)$ & $p_{1,1}(x)$ &  \\
				\vdots & \vdots & \vdots\\
				$x_n$ & $f_n=p_{n,0}(x)$ &\\
			\end{tabular}
		\end{table}
\end{enumerate}

\begin{ex*}
	\begin{enumerate}[a)]
		\item 
			Für die Daten aus dem letzten Beispiel ergibt sich das Horner-Schema für die Newton-Form als
			\[
				p(x) = \Big(1(x-2)+1\Big)(x-1) +2
			\]
			und damit
			\[
				f(4) = 11
			\]
		\item
			Selbiges mit dem Neville-Schema:
			\begin{table}[H]
				\centering
				\begin{tabular}{c|ccc}
					$x_i$ & $f_i$ &  & $f(4)$ \\ \hline
					1 & 2 & 5 & \emph{11}\\
					2 & 3 & 9\\
					3 & 6
				\end{tabular}
			\end{table}
	\end{enumerate}
\end{ex*}

\subsection{Zusammenfassung}

\begin{table}[H]
	\centering
	\begin{tabular}{p{0.23\textwidth}|p{0.22\textwidth}|p{0.24\textwidth}|p{0.23\textwidth}}
		\centering Allgemein & \centering Monombasis & \centering Lagrange-Basis & \centering Newton-Basis \tabularnewline \hline

		\centering $\displaystyle \phi_k(x)$ & 
		\centering $\displaystyle x^k$ & 
		\centering $\displaystyle L_k^n(x) = \prod_{\substack{i=0 \\ i\neq j}}^n \f{x-x_i}{x_k-x_i}$ & 
		\centering $\displaystyle N_k^n(x) = \prod_{i=0}^{k-1} (x-x_i)$ \tabularnewline

		\centering $\begin{pmatrix}
			\phi_0(x_0) & \cdots & \phi_n(x_0) \\
			\vdots & \ddots & \vdots \\
			\phi_0(x_n) & \cdots & \phi_n(x_n)
		\end{pmatrix}$ &
		\centering $\begin{pmatrix}
			1 & x_0 & \cdots & x_0^n \\
			\vdots & \vdots & \ddots & \vdots \\
			1 & x_n & \cdots & x_n^n
		\end{pmatrix}$ &
		\centering $\begin{pmatrix}
			1 & & 0 \\
			 & \ddots &   \\
			0 & & 1
		\end{pmatrix}$ &
		\centering $\begin{pmatrix}
			1 &  & 0 \\
			\vdots & \ddots &  \\
			1 & \cdots & *
		\end{pmatrix}$ \tabularnewline

		&
		\begin{itemize}
			\item Van\-der\-mon\-de\-ma\-trix $A$ ist schlecht konditioniert.
		\end{itemize} &
		\begin{itemize}
			\item Es gibt kein LGS zu lösen ($A=I$).
			\item Punktauswertung ist teuer.
		\end{itemize} &
		\begin{itemize}
			\item $A$ ist Dreiecksmatrix.
			\item Neville-Schema einsetzbar (auch für direkte Punktauswertung)
		\end{itemize} \\

		LGS lösen: & \centering $\mathcal O(n^3)$ & \centering $\mathcal O(1)$ & \centering $\mathcal O(n^2)$ \tabularnewline
		Punktauswertung: & \centering $\mathcal O(n)$ via Horner & \centering $\mathcal O(n^2)$ & \centering $\mathcal O(n)$ via Horner \tabularnewline
	\end{tabular}
\end{table}

\subsection{Anwendung: Richardson-Extrapolation}

Für eine gegebene Funktion $b:(0,\infty)\to \R$ ist $\lim_{h\to 0} b(h)$ gesucht, bzw. eine Approximation.

\begin{seg}[Ansatz:]
Wähle $h_0,\dotsc, h_n \in \R^+$, berechne $b_k := b(h_k)$.
Sei $p(h)$ das Interpolationspolynom zu den Daten $(h_k,b_k)$.

Werte $p(0)$ aus als Approximation von $\lim_{h\to 0} b(h)$.
Nach vorigem Abschnitt b) ist $p(0)$ ohne Interpolationspolynom auswertbar.
Vereinfachung der Rekursion aus \ref{1.5} für $x=0$:
\begin{align*}
	p_{i,j}(0) &= \f {(0-h_i)p_{i+1,j-1}(0) - (0-h_{i+j})p_{i,j-1}(0)}{h_{i+j}-h_i} \\
	&= \f {(h_{i+j}-h_i)p_{i+1,j-1}(0)}{h_{i+j}-h_i} - \f {h_{i+j}\Big(p_{i+1,j-1}(0)-p_{i,j-1}(0)\Big)}{h_{i+j}-h_i}\\
	&= p_{i+1,j-1}(0) + \f {p_{i+1,j-1}(0) -p_{i,j-1}(0)}{\f {h_i}{h_{i+j}} - 1}
\end{align*}
\end{seg}

\begin{df}[Richardson-Extrapolation] \label{1.8}
	Zu $\{h_i\}_{i=0}^n \subset \R^+$ definiere
	\begin{align*}
		b_i := b_{i,0} &:= b(h_i)\\
		b_{i,j} &:= b_{i+1,j-1} + \f {b_{i+1,j-1}-b_{i,j-1}}{\f {h_i}{h_{i+j}}-1}
	\end{align*}
	Dann ist $b_{0,n}$ eine Approximation für $\lim_{h\to 0} b(h)$.
	\begin{note}
		Es ist sinnvoll, für $\{h_i\}$ Folgenglieder aus einer gegen 0 konvergierenden Folge zu nehmen.
	\end{note}
\end{df}

\begin{ex*}
	Berechnung von $e=\lim_{n\to \infty} (1 +\f 1n)^n = \lim_{h\to 0} (1+h)^{\f 1h}$.
	D.h. $b(h)=(1+h)^{\f 1h}$.
	Wähle $h_k=2^{-k} \implies b_{k,0}=b(h_k) = (1+2^{-k})^{2^k}$
	\[
		\implies b_{0,0} = 2, \qquad b_{1,0} = \f 94, \qquad b_{2,0} = \f {625}{256} \approx 2,44
	\]
	Neville-Schema für Rekursion aus 1.8, verwende $\f {h_i}{h_{i+j}} = 2^j$.
	\begin{table}[H]
		\centering
		\begin{tabular}{c|ccc}
			$h_0=1$ & $b_{0,0}=2$  & $b_{0,1}=\f 52$ & $b_{0,2}=\f {257}{96}$\\
			$h_1=\f12$ & $b_{1,0}=\f 94$  & $b_{1,1}=\f {337}{128}$ & \\
			$h_2=\f14$ & $b_{2,0}=\f {625}{256}$  \\
		\end{tabular}
	\end{table}
\end{ex*}

\begin{note}
	Die Richardson-Extrapolation lohnt sich insbesondere für Funktionen, deren Auswertung für kleine $h$ teuer ist, z.B. $\mathcal O(\f 1n)$.
\end{note}

\begin{note}
	Verbesserungsmöglichkeit bei Kenntnis der Form der asymptotischen Entwicklung.
	Falls $b(h) = \sum_{n=0}^\infty a_nh^{qn}$ für $q>1$, dann ist die Interpolation mit Polynomen angebracht, die nur diese Terme $h^{qn}$ enthalten, diese werden anschließend extrapoliert.

	Setze dazu $\_h = h^q$, d.h. $\_{h_{i}}:= h_i^q$ und
	\[
		\_b(\_h) := \sum_{n=0}^\infty a_n\_h^n
	\]
	Dann ist $\_b(\_h_i) = b(h_i)$.
	Führe die Richardson-Extrapolation für $\{\_h_i\}$ und $\{\_b(\_h_i)\}$ durch, dann ist das Ergebnis eine Aproximation für
	\[
		\lim_{\_h\to 0}\_b(\_h) = \lim_{h\to 0}b(h)
	\]
\end{note}

\subsection{Fehleranalysis}

Wir nehmen an, die Daten $\{t_i\}_{i=0}^n$ stammen von Funktionsauswertungen $f_i=f(x_i)$.

Wie gut apporximiert die Interpolierende die Funktion?

\begin{df}
	\label{1.9}
	Sei $I\subset \R$ ein abgeschlossenes Intervall.
	Wir definieren
	\[
		C(I) = C^0(I) := \{f: I\to \R : f \text{ stetig}\}
	\]
	als Raum der stetigen Funktionen und
	\[
		C^m(I) := \Big\{f: I\to \R : f,f',\dotsc,f^{(m)} \text{ existieren und stetig}\Big\}
	\]
	als Raum der $m$-mal stetig differenzierbaren Funktionen ($m\in \N$) und
	\[
		C^\infty(I) := \bigcap_{m\in \N}C^m (I)
	\]
	als Raum der unendlich oft stetig differenzierbaren Funktionen.

	Für $f\in C(I)$, $I$ beschränkt, definieren wir die Supremums-Norm
	\[
		\|f\|_\infty := \|f\|_{C(I)} := \sup_{x\in I}|f(x)|
	\]
\end{df}

\begin{note}
	\begin{itemize}
		\item 
			Es gilt $C^\infty(I) \subsetneq \dotsb \subsetneq C^0(I)$.
		\item
			Für beschränktes $I$ ist $(C(I), \|\cdot\|_\infty)$ ein normierter Raum.
		\item
			$(C(I),\|\cdot\|_\infty)$ ist vollständig, d.h. jede Cauchy-Folge konvergiert in $C(I)$.
		\item
			Eine vollständigen, normierten Vektorraum nennen wir Banachraum.
			Für beschränktes $I$ ist $(C(I),\|\cdot\|_\infty)$ Banachraum.
	\end{itemize}
\end{note}

\begin{st}[Punktweise Interpolationsfehler]
	\label{1.10}
	Sei $f\in C^{n+1}(\R)$ und $p\in \P_n$ Interpolierende.
	\[
		p(x_i) = f_i \qquad i=0,\dotsc,n \;\land\; x_i\neq x_j \text{ für } i\neq j
	\]
	Dann gilt für alle $x\in \R$
	\[
		p(x) - f(x) = \f 1{(n+1)!}f^{(n+1)}(\xi) (x-x_0)(x-x_1)\dotsb(x-x_n)
	\]
	mit geeignetem $\xi = \xi(x)\in I = \xi \in [\min(x,x_0,\dotsc,x_n),\max(x,x_0,\dotsc,x_n)]$.

	\begin{proof}
		Für $x=x_i$ ist 
		\[
			p(x) - f(x) = \f 1{(n+1)!}f^{(n+1)}(\xi) (x-x_0)(x-x_1)\dotsb(x-x_n)
		\]
		erfüllt ($0=0$).
		Für $x\neq x_i$, $i=0,\dotsc,n$.
		Setze
		\[
			w(x) := \prod_{i=0}^n (x-x_i) \qquad \text{„Knotenpolynom“}
		\]
		und definiere 
		\[
			g(t):= f(t)-p(t)-\f {w(t)}{w(x)}(f(x)-p(x))
		\]
		
		Es gilt $g(x_i)=0$, $i=0,\dotsc,n$ (wegen $f(x_i)=p(x_i)$) und $g(x)=0$.
		Also hat $g$ mindestens $n+2$ verschiedene Nullstellen in $I$.
		Nach dem Satz von Rolle hat $g'$ mindestens $n+1$ verschiedene Nullstellen, usw. und schließlich $g^{(n+1)}$ mindestens $1$ Nullstelle $\xi\in I$. 

		Mit $p^{(n+1)} = 0$, $w^{(n+1)}(x) = (n+1)!$ folgt
		\begin{align*}
			0 = g^{(n+1)}(\xi) &= f^{(n+1)}(\xi) - p^{(n+1)}(\xi) - \f {(n+1)!}{w(x)}\Big(f(x)-p(x)\Big)\\
			&= f^{(n+1)}(\xi) - 0 - \f {((n+1)!}{w(x)}\Big(f(x)-p(x)\Big)\\
		\end{align*}
		Also 
		\[
			f(x) -p(x) = \f 1{(n+1)!}f^{(n+1)}(\xi) w(x)
		\]
	\end{proof}
\end{st}

\begin{kor}[Fehlerschranke]
	\label{1.11}
	Für alle beschränkten Intervalle $I\subset \R$, mit $\{x_0,\dotsc,x_n\}\subset I$ gilt
	\[
		\|p-f\|_\infty \le \f 1{(n+1)!}\|w\|_\infty \|f^{(n+1)}\|_\infty
	\]
	\begin{proof}
		klar
	\end{proof}
\end{kor}

\begin{st}[Gleichmäßige Konvergenz]
	\label{1.12}
	Sei $I=[a,b]$, $f\in C^\infty(I)$ mit $\|f^{(n)}\|_\infty \le M$ für alle $n\in \N$.
	Sei $\Delta_n := \{x_0^n,\dotsc, x_n^n\}\subset I$ eine Menge $n$ disjunkter Stützstellen und $p_n\in \P_n$ eine zugehörige Interpolierende.
	Dann gilt
	\[
		\lim_{n\to \infty} \|p_n -f\| = 0
	\]
	\begin{proof}
		Für $w_n(x):=\prod_{i=0}^n(x-x_i^n)$ gilt $|w_n(x)| \le (b-a)^{n+1}$.
		Aus \ref{1.11} folgt
		\[
			\|p_n-f\|_\infty \le \f {\|w_n\|_\infty}{(n+1)!}\|f^{(n+1)}\|_\infty \le \f{(b-a)^{n+1}}{(n+1)!}M \to 0
		\]
	\end{proof}
\end{st}

\begin{ex*}
	Sei $f(x)=e^x$ auf $I=[-1,1]$, $f\in C^\infty(I)$.
	Es gilt offensichtlich
	\[
		\|f^{(n)}\| \le e \qquad n\in \N
	\]
	Selbst wenn man nur Stützstellen auf $[-1,-0.75]$ wählt, konvergiert die Folge gleichmäßig gegen die Zielfunktion.
\end{ex*}

\begin{note}
	Vergleiche Stoer-Bulirsch.
	\begin{itemize}
		\item
			Ohne Einschränkungen an $(\Delta_n)_{n\in \N}$ oder $f\in C^\infty(I)$ kann man \emph{keine} gleichmäßige Konvergenz und nicht einmal punktweise Konvergenz erwarten.
			Das Problem ist, dass $f^{(n+1)}(\xi)$ schneller mit $n$ wachsen kann als $\left(\f {w(x)}{(n+1)!}\right)^{-1}$.	
		\item
			Für jedes $f\in C(I)$ existiert Folge $(\Delta_n)_{n\in \N}$, sodass $p_n\to f$ gleichmäßig.
		\item
			Satz von Faber: Zu jeder Folge $(\Delta_n)_{n\in \N}$ gibt es ein $f\in C([a,b])$ so dass $p_n\not\to f$ gleichmäßig.
		\item
			In der Praxis kommt es häufig zu Oszillationen von $p_n$, vor allem am Rand von\\ $[ \min\{x_0,\dotsc, x_n\},\max\{x_0,\dotsc,x_n\} ]$.
	\end{itemize}
\end{note}

\begin{ex*}[Runge]
	Sei $f(x)= \f 1{1+x^2}$ mit $I=[-5,5]$ und $x_k^n := -5+k h_n$ ($k=0,\dotsc,n$) mit $h_n:= \f {10}n$ äquidistanten Stützstellen.

	Sei $p_n\in \P_n$ mit $p_n(x_k^n) = f(x_k^n)$, dann ergibt sich \fixme[Oszillationen].
	
	Man kann zeigen, dass $\tilde x \approx 3,68$ existiert, so dass in $(-\tilde x,\tilde x)$ punktweise und nicht punktweise Konvergenz für $|x|\ge \tilde x$ vorliegt. Sogar: $\|p_n-f\|_\infty \to \infty$.
\end{ex*}

\subsection{Optimale Wahl von Stützstellen}

Finde Stützstellen $\{x_0,\dotsc,x_n\}\subset I := [-1,1]$, so dass $\|w\|_\infty$ minimal wird.
Dann ist die Schranke in \ref{1.11} am kleinsten.

\begin{df}[Tschebyscheff-Polynome] \label{1.13}
	Wir definieren die \emph{Tschebyscheff-Polynome auf $I$} durch
	\begin{align*}
		T_0(x) &:= 1\\
		T_1(x) &:= x\\
		T_{n+1}(x) &:= 2x\cdot T_n(x) - T_{n-1}(x)\\
	\intertext{und \emph{normierte Tschebyscheff-Polynome} durch}
		\hat T_n(x) &:= \begin{cases} T_n(x) & n \in \{0,1\} \\
			2^{1-n} \cdot T_n(x) & n \ge 2
		\end{cases}
	\end{align*}
\end{df}

\begin{ex*}
	\begin{alignat*}{2}
		T_2(x) &= 2x^2 - 1\qquad&  \hat T_2(x) &= x^2 -\f 12\\
		T_3(x) &= 4x^3 - 3x \qquad& 		\hat T_3(x) &= x^3 -  \f 34 x
	\end{alignat*}
\end{ex*}

\begin{st}
	\label{1.14}
	Für $n\in \N_0$ gilt
	\begin{enumerate}[i)]
		\item 
			$T_n\in \P_n$
		\item
			Die $\hat T_n$ sind normiert.
		\item
			Für $x\in [-1,1]$ gilt
			\[
				T_n(x) = \cos(n\cos^{-1}(x))
			\]
		\item
			$|T_n(x)| \le 1$
		\item
			Extrema für $j=0,\dotsc,n$:
			\[
				T_n\Big(\cos(\tf \pi n \cdot j)\Big) = (-1)^j
			\]
		\item
			Nullstellen für $j=0,\dotsc,n-1$:
			\[
				T_n\Big(\cos(\tf\pi n (j + \tf 12)\Big) = 0
			\]
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item klar
			\item klar
			\item
				Ein Additionstheorem liefert
				\[
					\cos(\alpha+\beta) = \cos(\alpha)\cos(\beta) - \sin(\alpha)\sin(\beta)
				\]
				Also
				\begin{align*}
					\cos((n+1)\theta) &= \cos(n\theta) \cos(\theta) - \sin(n\theta) \sin(\theta)\\
					\cos((n-1)\theta) &= \cos(n\theta) \cos(\theta) + \sin(n\theta) \sin(\theta)
				\end{align*}
				und damit
				\[
					\cos((n+1)\theta) + \cos((n-1)\theta) = 2 \cos(n\theta)\cos(\theta)
				\]
				Setze $\theta:= \cos^{-1}(x)$, dann ergibt sich
				\begin{align*}
					F_{n+1}(x) = \cos((n+1)\cos^{-1}(x)) &= 2\cos(n\cos^{-1}(x)) \cos(\cos^{-1}x) - \cos((n-1)\cos^{-1}(x))\\
					&= 2x \underbrace{\cos(n\cos^{-1}(x))}_{F_n(x)} - \underbrace{\cos((n-1)\cos^{-1}(x))}_{F_{n-1}(x)}
				\end{align*}
				d.h. $F_n(x):= \cos(n\cos^{-1}(x))$ erfüllt die Rekursion der Tschebyscheff Polynome aus Definition \ref{1.13}.
				Außerdem  $F_0(x)=1, F_1(x)=x$, also $F_n(x)=T_n(x)$.
			\item
				klar mit iii).
			\item
				Durch Nachrechnen mit iii).
			\item
				Durch Nachrechnen mit iii).
		\end{enumerate}
	\end{proof}
\end{st}

\begin{st}[Optimalität]
	\label{1.15}
	Sei $I := [-1,1]$ ein Intervall;
	\begin{enumerate}[i)]
		\item 
			Sei $p\in \P_n$ normiert auf $I$, dann gilt
			\[
				\|p\|_\infty \ge 2^{1-n}
			\]
		\item
			Auf $I$ gilt
			\[
				\displaystyle \|\hat T_n \|_\infty = 2^{1-n}
			\]
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item 
				Wir nehmen an, es existiert ein normiertes $p\in \P_n$ mit $|p(x)|<2^{1-n}$ für alle $x\in I$.
				Sei $x_j= \cos(\f {j\pi}{n})$.
				Mit \ref{1.14} und der Definition von $\hat T_n$ folgt
				\[
					(-1)^j p(x_j) \le |p(x_j)| < 2^{1-n} = 2^{1-n}\underbrace{(-1)^j}_{T_n(x_j)}(-1)^j = \hat T(x_j)(-1)^j
				\]
				und
				\[
					(-1)^j(\hat T_n(x_j)-p(x_j)) > 0 \qquad j=0,\dotsc,n
				\]
				also hat $\hat T_n -p$ mindestens $n$ Vorzeichenwechsel und somit auch $n$ verschiedene Nullstellen. 
				Da beide Polynome normiert waren, ist $\hat T_n-p\in \P_{n-1}$, ein Widerspruch.
				Also gilt
				\[
					|p(x)| \ge 2^{1-n} \qquad \forall p\in \P_n \text{ normiert}
				\]
			\item
				aus i) folgt wegen $\hat T_n$ normiert, dass
				\[
					\|\hat T_n\|_\infty \ge 2^{1-n}
				\]
				Mit Satz \ref{1.14} iv) ist $|T_n(x)| \le 1$ also $|\hat T_n(x)| \le 2^{1-n}$ für alle $x\in I$.
				Also 
				\[
					\|\hat T_n\|_\infty = 2^{1-n}
				\]
		\end{enumerate}
	\end{proof}
\end{st}

\begin{kor}[Optimale Stützstellen]
	\label{1.16}
	Setze $\{x_0,\dotsc,x_n\}$ als Nullstellen von den Tschebyscheff-Polynomen $T_{n+1}$:
	\[
		x_i := \cos\Big(\tf \pi{n+1}(i + \tf 12)\Big) \qquad i=0,\dotsc,n
	\]
	Dann ist $\omega(x) := \prod_{i=0}^n (x-x_i) = \hat T_{n+1}$ Knotenpolynom mit minimalem $\|\omega\|_\infty$.
\end{kor}


\subsection{Stabilität der Interpolation}


Wie sensitiv ist die Interpolation bei Störungen in den Zielwerten $\{f_i\}_{i=0}^n$?

\begin{df}[Lebesgue-Konstante] \label{1.17}
	Sei $I\subset \R$ ein kompaktes Intervall und $V\subset C(I)$ ein Unterraum der stetigen Funktionen auf $I$ mit $\dim(V)=n+1$.
	Seien paarweise verschiedene Stützstellen $\{x_0,\dotsc, x_n\}\subset I$ gegeben und $\{L_k^n(x)\}_{k=0}^n$ eine nodale Basis von $V$ (d.h. $L_k^n(x_j) = \delta_{kj}$).
	Dann nennen wir
	\[
		\Lambda_n := \max_{x\in I}\sum_{k=0}^n \big|L_k^n(x)\big|
	\]
	die \emph{Lebesgue-Konstante} der Interpolation.
\end{df}

\begin{note}
	$\Lambda_n$ ist insbesondere auch für Interpolation mit anderen Funktionen statt Polynomen definiert.

	Sei $V=\<\{\phi_k\}_{k=0}^n\> \subset C(I)$ mit Basis $\{\phi_k\}_{k=0}^n$.
	Falls die Interpolationsmatrix regulär ist, so existiert eine nodale Basis von $V$:
	\[
		A=\begin{pmatrix}
			\phi_0(x_0) & \hdots & \phi_n(x_0)\\
			\vdots & \ddots & \vdots\\
			\phi_0(x_n) & \hdots & \phi_n(x_n)
		\end{pmatrix}
		\qquad \text{regulär}
	\]
	Dann sind $B=(b_{ik})_{i,k=0}^n := A^{-1}$ und $L_k^n(x) := \sum_{i=0}^n b_{ik}\phi_i(x)$ wohldefiniert und
	\[
		L_k^n (x_j) = \sum_{i=0}^n b_{ik}\phi_i(x_j) = (A B)_{jk} = I_{jk} = \delta_{jk}
	\]
\end{note}

\begin{st}[Störungs-Aussage]
	\label{1.18}
	Seien $\{f_i\}_{i=0}^n$ und $\{\tilde f_i\}_{i=0}^n$ zwei Mengen von Zielwerten über Stützpunkten $\{x_i\}_{i=0}^n\subset I$ und $p$ und $\tilde p$ zugehörige Interpolationsfunktionen.
	Dann gilt
	\[
		\|p-\tilde p\|_\infty \le \Lambda_n \max_{i=0,\dotsc,n}|f_i-\tilde f_i|
	\]
	Diese Abschätzung ist scharf (d.h. $\Lambda_n$ kann nicht kleiner gewählt werden, wir finden Beispiele $\{f_i\},\{\tilde f_i\}$ für die Gleichheit gilt).
	\begin{proof}
		Zeige zunächst: $\forall \{\_{f_i}\}\subset \R$ und interpolierender Funktion $\_p\in V$, gilt:
		\[
			\|\_p\|\infty \le \Lambda_n \max_{i=0,\dotsc,n}|\_{f_i}| \qquad \text{und ist scharf}
		\]
		Für $t\in I$ gilt
		\begin{align*}
			|\_p(t)| &= \left| \sum_{i=0}^n \_{f_i} L_i^n(t)\right| 
			\le \sum_{i=0}^n |\_{f_i}||L_i^n(t)|\\
			&\le \sum_{i=0}^n \max_{j=0,\dotsc,n}|\_{f_j}||L_i^n(t)|
			= \bigg(\sum_{i=0}^n|L_i^n(t)| \bigg)\max_{j=0,\dotsc,n} |\_{f_j}|
		\end{align*}
		Also ist im Supremum über $t \in I$:
		\[
			\|\_p\|_\infty \le \Lambda_n \max_{j=0,\dotsc,n}|\_{f_j}|
		\]

		Sei $\tau := \argmax\limits_{x\in I} \sum_{i=0}^n|L_i^n(x)|$ und $\_{f_i} := \sgn(L_i^n(\tau))$.
		Dann gilt
		\begin{align*}
			\_p(\tau) &= \left| \sum_{i=0}^n \underbrace{\_{f_i}L_i^n(\tau)}_{\ge 0}\right| 
			= \sum_{i=0}^n |L_i^n(\tau)| \\
			&= \max_{x\in I} \sum_{i=0}^n |L_i^n(x)| 
			= \Lambda_n \cdot \underbrace{1}_{=\max\limits_{i=0,\dotsc,n}|\_{f_i}|}
		\end{align*}
		also ist 
		\[
			\|\_p\|_\infty \le \Lambda_n \max_{j=0,\dotsc,n}|\_{f_j}|
		\]
		scharf.

		Der Satz folgt, da $\_p := p-\tilde p$ das Interpolationspolynom zu den Daten $\_{f_i} := f_i - \tilde {f_i}$ ist.
	\end{proof}
\end{st}

\begin{ex*}[Polynominterpolation, vgl. Deuflhard/Hohmann]~

	\begin{table}[H]
		\centering
		\begin{tabular}{r|r|r}
			$n$ & $\Lambda_n$ für äquidistante $\Delta_n$ & $\Lambda_n$ für Tschebyscheff-Knoten \\ \hline
			5 & 3,106292 & 2,104398 \\
			10 & 29,890695 & 2,489430 \\
			15 & 512,052451 & 2,727778 \\
			20 & 10986,533993 & 2,900825
		\end{tabular}
	\end{table}		
	Also führen die Tschebyscheff-Knoten auch quantitativ zu stabilerer Interpolation.
\end{ex*}


\subsection{Hermite-Interpolation}

Wir wollen jetzt nicht nur Funktionswerte für die Interpolierende vorgeben, sondern auch Werte für ihre Ableitungen.

Seien $\{x_i\}_{i=0}^m \subset \R$ paarweise verschiedene Knoten und $\{f_i^{(j)}\}_{j=0}^{m_i-1}$ Zielwerte für $i=0,\dotsc, m$ (d.h. $m_i$ viele Zielwerte für jedes $i \in \{0,\dotsc, m\}$, macht $n := \sum_{i=0}^m m_i$ viele Zielwerte).

Gesucht ist ein Polynom $p\in \P_n$ mit 
\[
	p^{(j)}(x_i) =f_i^{(j)}
\]
mit $i=0,\dotsc,m$ (Stützstellen) und $j=0,\dotsc,m_i-1$ (Ableitungen an Stützstelle $i$) und 
\[
	n := \sum_{i=0}^m m_i - 1
\]
($\sum_{i=0}^m m_i$ war die Anzahl der vorgegebenen Zielwerte, also $\sum_{i=0}^m m_i -1$ der Grad des Interpolationspolynoms).

\begin{st}[Existenz und Eindeutigkeit]
	\label{1.19}
	Es existiert ein eindeutiges Polynom $p\in \P_n$ als Lösung des Hermite-Interpolationsproblems.
	\begin{proof}
		Wir verfahren ähnlich wie in \ref{1.1}.
		Zeige, dass die Interpolationsmatrix $A$ bezüglich $\{p_i\}_{i=0}^n$ regulär ist.
		\[
			A= \begin{pmatrix}
				\phi_0^{(0)}(x_0) &\hdots &\phi_n^{(0)}(x_0)\\
				\phi_0^{(1)}(x_0) &\hdots &\phi_n^{(1)}(x_0)\\
				\vdots &  & \vdots \\
				\phi_0^{(m_0-1)}(x_0)& \hdots &\phi_n^{(m_0-1)}(x_0)\\
				\phi_0^{(0)}(x_1) &\hdots &\phi_n^{(0)}(x_1)\\
				\vdots& & \vdots \\
				\phi_0^{(m_1-1)}(x_1)& \hdots &\phi_n^{(m_1-1)}(x_1)\\
				\phi_0^{(0)}(x_2) &\hdots &\phi_n^{(m_2)}(x_2)\\
				\vdots& & \vdots \\
				\phi_0^{(m_m-1)}(x_m) &\hdots &\phi_n^{(m_m-1)}(x_m)
			\end{pmatrix}
			\qquad \text{ist quadratisch}
		\]
		Wir zeigen jetzt $\ker(A) = 0$.

		Sei $A\_a = 0$ für $\_a\in \R^{n+1}$, dann erfüllt $\_p(x) := \sum_{i=0}^n \_a_i \phi_i(x)$ die Gleichung $\_p^{(j)}(x_i) = 0$ für $i=0,\dotsc,m$ und $j=0,\dotsc,m_i-1$.
		
		Also ist $(x-x_i)^{m_i}$ Teiler von $\_p$ für jedes $x_i$.
		Insgesamt ist also
		\[
			\omega(x) := \prod_{i=0}^m (x-x_i)^{m_i}
		\]
		ein Teiler von $\_p$ in $\P_n$, d.h.
		\[
			\exists q(x)\in \P_n : \_p(x) = q(x) \omega(x)
		\]
		Es gilt $\deg \_p \le n$ und $\deg \omega = \sum_{i=0}^m m_i = n+1$.
		Also muss $q(x)=0$ und damit $\_p(x) = 0$.

		Also $\_a=0$.
	\end{proof}
\end{st}

\begin{note}
	Falls nicht alle Ableitungen $p^{(j)}(x_i) =f_i^{(j)}$ für $j=0,\dotsc,m_i-1$ vorgeschrieben sind, ist die Interpolations-Aufgabe im Allgemeinen \emph{nicht} lösbar.

	\begin{ex*}
		Sei $m=1$.
		Gesucht ist $p\in \P_1$ mit
		\[
			p'(x_0) = 1 \qquad \land \qquad p'(x_1) = 2
		\]
		Dazu gibt es keine Lösung (keine Existenz).
		Für
		\[
			p'(x_0) = 1 \qquad \land \qquad p'(x_1) = 1
		\]
		sind alle $(x) = x+a$, $a\in \R$ Lösungen (keine Eindeutigkeit).
	\end{ex*}
\end{note}

\begin{st}[Fehlerdarstellung]
	\label{1.20}
	Sei $f\in C^{n+1}(I)$, $\{x_i\}_{i=0}^n\subset I$ und $p\in \P_n$ das Interpolations-Polynom mit
	\[
		p^{(j)}(x_i) = f^{(j)}(x_i) \qquad i=0,\dotsc,m \quad j=0,\dotsc,m_i-1
	\]
	Dann existiert für alle $x\in I$ ein $\xi \in I$ mit
	\[
		f(x) - p(x) = \f 1{(n+1)!} f^{(n+1)} (\xi) \omega(x)
	\]
	für $\omega(x) = \prod_{i=0}^n(x-x_i)^{m_i}$.
	\begin{proof}
		Analog zu Satz \ref{1.10}.
	\end{proof}
\end{st}

\begin{st}[Newton-Form]
	\label{1.21}	
	Das Hermite-Interpolationspolynom ist gegeben durch
	\[
		p(x) = \sum_{k=0}^n a_k \prod_{j=0}^{k-1}(x-z_j)
	\]
	mit Stützstellen
	\[
		z_j := x_i \qquad j=n_{i},\dotsc, n_{i+1}-1
	\]
	wobei $n_i = \sum_{r=0}^{i-1}m_r$.
	Mit anderen Worten: $z_j$ durchläuft die Stützstellen $x_i$, jede davon jedoch genau so oft, wie es Zielwertbedingung für diese Stützstelle hat.

	Die Koeffizienten $a_k := a_{0,k}$ für $k=0,\dotsc,n$ werden erhalten aus der Rekursion
	\begin{align*}
		a_{k,j} := \begin{cases}
			\displaystyle \qquad \f 1{j!}f_i^{(j)} 			&  \begin{aligned}i&=0,\dotsc,m \\ j&=0,\dotsc,m_i-1 \\ k&=n_{i},\dotsc, n_{i+1}-1-j\end{aligned} \\
			\displaystyle \f {a_{k+1,j-1} - a_{k,j-1}}{z_{k+j} - z_k} & \text{sonst}
		\end{cases}
	\end{align*}
	\begin{proof}
		\fixme[Fehlt und sieht nicht so trivial aus]
	\end{proof}
\end{st}

\begin{ex*}
	Gegeben die folgenden Stützstellen und Werte: \\

	\begin{table}[H]
		\centering	
		\begin{tabular}{l|c|c}
			 & $x_0=1$ & $x_1 =2$ \\ \hline
			$f_i^{(0)}$ & -1 & 14 \\
			$f_i^{(1)}$ & 4 & 32 \\
			$f_i^{(2)}$ & 12 & 
		\end{tabular}		
	\end{table}

	Dann ist $m=1$, $m_0=3$, $m_1=2$, und damit $n=3+2-1=4$.
	Es werden die Stützstellen $z_0=z_1=z_2=x_0$, $z_3=z_4=x_1$ gewählt.
	Die Basispolynome sind dann
	\[
		1, (x-1), (x-1)^2, (x-1)^3, (x-1)^3(x-2)
	\]
	Bestimme die Koeffizienten mit Hilfe des Neville-Schemas: \\

	\begin{table}[H]
		\centering	
		\begin{tabular}{l|c|c|c|c|c|c}
			$z_k$  & $a_{k,0}$ & $a_{k,1}$ & $a_{k,2}$ \\ \hline
			$z_0=1$ & $f_0^{(0)} = -1$  & $\f 1{1!}f_0^{(1)} = 4$ & $\f 1{2!} f_0^{(2)} = 6$ & $\f {11-6}{2-1}=5$ & $\f {6-5}{2-1}=1$ \\
			$z_1=1$ & $f_0^{(0)} = -1$ & $4$ 						&	$\f {15-4}{2-1} = 11$ & $\f {17-11}{2-1}=6$ \\
			$z_2=1$ & $f_0^{(0)} = -1$ & $\f {14-(-1)}{2-1} = 15$ & $\f {32-15}{2-1} = 17$ \\
			$z_3=2$ & $f_1^{(0)} = 14$ & $\f 1 {1!} f_1^{(1)} = 32$ &  \\
			$z_4=2$ & $f_1^{(0)} = 14$ & 
		\end{tabular}		
	\end{table}
	Es ergibt sich dann
	\begin{align*}
		p(x)& = -1\cdot 1 + 4\cdot (x-1) + 6\cdot(x-1)^2 + 5\cdot(x-1)^3 + 1\cdot(x-1)^3 (x-2) = x^4-2\\
		p'(x) &= 4x^3\\
		p''(x) &= 12x
	\end{align*}
\end{ex*}


\section{Trigonometrische Interpolation}

Für gegebenes $n\in \N$, $x_k = 2\pi \f {k}{n+1} \in [0,2\pi)$, $k=0,\dotsc,n$ ($n+1$ äquidistante Stützstellen) und $\{f_k\}_{k=0}^n\subset \R$ ist eine $2\pi$-periodische Funktion der Gestalt
\[
	t(x) = \f {a_0}2 + \sum_{k=1}^n \Big(a_k \cos(kx) + b_k \sin(kx) \Big) \qquad a_k,b_k \in \R
\]
mit $t(x_k)=f_k$ gesucht.

\begin{note}
	\begin{itemize}
		\item
			Diese Darstellung ist zunächst nur für gerade $n$ sinnvoll.
			Für ungerade $n$ wird eine leichte Modifikation notwendig sein.
		\item
			Die Bestimmung von $t(x)$ wird über komplexe trigonometrische Polynome erfolgen.
	\end{itemize}
\end{note}

\begin{lem}[Reelle und komplexe Fourier-Summe]
	\label{1.22}
	Seien $\{a_k,b_k\}_{k=0}^m\subset \R$, $\{c_k\}_{k=-m}^m\subset \C$ mit $c_0=\f {a_0}2$, $c_k=\f 12(a_k-ib_k)$ und $c_{-k}=\f 12(a_k+ib_k)$ für $k=1,\dotsc, m$.

	Dann gilt
	\[
		\f {a_0}2 + \sum_{k=1}^m \Big(a_k\cos(kx) + b_k\sin(kx)\Big) = \sum_{k=-m}^m c_k e^{ikx}
	\]
	\begin{proof}
		Einsetzen und die Eulersche Formel $e^{iz}=\cos(z) +i\sin(z)$ liefern
		\begin{align*}
			\sum_{k=-m}^m c_ke^{ikx} &= \f {a_0}2 (\cos(0x) + i\sin(0x)) \\
			& \qquad + \sum_{k=1}^m \f 12 (a_k-ib_k)(\cos(kx) +i\sin(kx)) + \sum_{k=1}^m \f 12 (a_k+ib_k)(\cos(-kx)+i\sin(-kx)) \\
			&= \f {a_0}2 + \sum_{k=1}^m a_k\cos(kx) + b_k \sin(kx)
		\end{align*}
	\end{proof}
	\begin{note}
			Die reelle Fourier-Summe ergibt also die komplexe Fouriersumme mit entsprechend gewählten $c_k$.

			Das funktioniert auch umgekehrt für eine gegebene Fouriersumme $\{c_k\}_{k=-m}^m\subset \C$ mit $c_k=\_{c_{-k}}$, $c_0\in \R$.
			Die Wahl
			\[
				a_k := c_k +c_{-k}, \qquad b_k := i(c_k-c_{-k}) \qquad k=0,\dotsc,m
			\]
			erfüllt alle Bedingungen aus Lemma \ref{1.22}.
	\end{note}
\end{lem}

\begin{df}[Trigonometrische Polynome] \label{1.23}
	Wir definieren
	\[
		T_n := \left\{ q: \C \to\C : q(z) = \sum_{k=0}^n c_k e^{ikz}\right\}
	\]
	als den \emph{Raum der trigonometrischen Polynome von Grad $n$}.
	\begin{note}
		Wir bezeichnen den trigonometrischen Teil folgendermaßen
		\[
			\omega(z) := e^{iz}
		\]
		Die Polynomform ist jetzt besser zu erkennen:
		\[
			q(z) = \sum_{k=0}^n c_k\cdot \big(\omega(z)\big)^k
		\]
	\end{note}
\end{df}

\begin{st}[Trigonometrische Interpolation in $\C$]
	\label{1.24}
	Zu den Daten $\{f_k\}_{k=0}^n \subset \C$ mit Stützstellen $x_k = 2\pi \f k{n+1}$ existiert genau ein $q\in T_n$ mit $q(x_k) = f_k$, $k=0,\dotsc, n$.

	Die Koeffizienten sind gegeben durch
	\[
		c_k = \f 1{n+1}\sum_{j=0}^n f_j \omega_k^{-j}
	\]
	mit $\omega_k := e^{ix_k}$.
	\begin{proof}
		Satz \ref{1.1} gilt auch im Komplexen, also existiert genau ein Polynom $p(z)\in \P_n$ mit $p(z)=\sum_{k=0}^n c_kz^k$, $c_k\in \C$ und $p(\omega_k)=f_k$.

		Wir setzen $q(z) := \sum_{k=0}^n c_ke^{ikz} \in T_n$.
		$q(z)$ erfüllt die Interpolationsbedingungen:
		\[
			q(x_l) = \sum_{k=0}^n c_k e^{ikx_l} = \sum_{k=0}^n c_k \omega_l^k = p(\omega_l) = f_l \qquad l=0,\dotsc,n
		\]
		Es gilt
		\begin{enumerate}[i)]
			\item
				$\displaystyle \omega_k^{-j} = e^{-ij x_k} = e^{-ij 2\pi \f k{n+1}} = \omega_j^{-k}$
			\item
				$\displaystyle \sum_{j=0}^n (\omega_{l-k})^j = (n+1)\delta_{lk}$
				\begin{proof}
					\[
						\sum_{j=0}^n (\omega_{l-k})^j = \begin{cases}
							n+1 & l=k\\
							\f{(\omega_{l-k})^{n+1}-1}{\omega_{l-k}-1} & l\neq k
						\end{cases}
					\]
					Da jedoch 
					\[
						(\omega_{l-k})^{n+1} = e^{i 2\pi\f {l-k}{n+1}(n+1)} = e^{i2\pi(l-k)} = 1
					\]
					folgt
					\[
						\sum_{j=0}^n (\omega_{l-k})^j = \begin{cases}
							n+1 & l=k\\
							0 & l\neq k
						\end{cases}
					\]
				\end{proof}
		\end{enumerate}
		Daher gilt für Koeffizienten von $q(z)$
		\begin{align*}
			\sum_{j=0}^n f_j \omega_k^{-j} &= \sum_{j=0}^n p(\omega_j) \omega_k^{-j}
			= \sum_{j=0}^n\left( \sum_{l=0}^n c_l \omega_j^l\right) \omega_k^{-j}\\
			&\stack{\text{i)}} = \sum_{j,l=0}^n c_l \omega_j^{l-k} 
			= \sum_{l=0}^n c_l \sum_{j=0}^n \omega_{l-k}^j\\
			&\stack{\text{ii)}} = \sum_{l=0}^n c_l (n+1) \delta_{lk} = c_k (n+1)
		\end{align*}
		Dividieren durch $n+1$ liefert die Behauptung.
	\end{proof}
\end{st}

\begin{st}[Trigonometrische Interpolation in $\R$]
	\label{1.25}
	Für $n\in \N$ setze 
	\[
		m:= \begin{cases} \f n2 & \text{falls $n$ gerade} \\
			\f {n-1}2 & \text{falls $n$ ungerade}\end{cases} 
		\qquad 
		\theta := \begin{cases} 0 & \text{falls $n$ gerade} \\ 1 & \text{falls $n$ ungerade}\end{cases}
	\]
	Zu $x_k:=\f 2\pi k{n+1}$ und Daten $\{f_k\}_{k=0}^n\subset \R$ existiert genau eine Funktion
	\[
		t(x) = \f {a_0}2 + \sum_{k=1}^m \Big(a_k \cos(kx) + b_k \sin(kx)\Big) + \f{\theta}2 a_{m+1}\cos\big((m+1)x\big)
	\]
	mit $t(x_k) = f_k$, $k=0,\dotsc,n$.

	Für die Koeffizienten gilt
	\begin{align*}
		a_k &:= \f 2{n+1} \sum_{j=0}^n f_j(\cos(jx_k))\\
		b_k &:= \f 2{n+1} \sum_{j=0}^n f_j(\sin(jx_k))
	\end{align*}
	\begin{proof}
		Nach \ref{1.24} existiert ein eindeutiges $t\in T_n$ mit $t(x_k)=f_k$,
		\[
			t(x) = \sum_{k=0}^n c_k e^{ikx}, \qquad c_k = \f 1{n+1}\sum_{j=0}^n f_j \omega_k^{-j}
		\]
		Setzen wir jetzt $c_{-k} := c_{n+1-k}$, so können wir $q$ auch schreiben als
		\[
			t(x) = \sum_{k=-m}^m c_k e^{ikx} + \theta c_{m+1} e^{i(m+1)x}
		\]
		mit $\theta$ wie oben im Satz definiert.

		Wegen
		\[
			\_{w_k} = e^{-i 2\pi \f k{n+1}} = e^{i 2\pi \f {n+1-k}{n+1}} = \omega_{n+1-k}
		\]
		gilt
		\[
			c_{-k} = c_{n+1-k} = \f 1{n+1}\sum_{j=0}^n f_j \omega_{n+1-k}^{-j} = \f 1{n+1}\sum_{j=0}^n f_j\_{w_k}^{-j} = \_{c_k}
		\]
		Also ist Lemma \ref{1.22} anwendbar und mit der Wahl $a_k := c_k +c_{-k}$, $b_k := i(c_k-c_{-k})$, $k=0,\dotsc, m$ gilt:
		\[
			\sum_{k=-m}^m c_k e^{ikx} = \f{a_0}2 + \sum_{k=1}^m \Big(a_k \cos(kx) + b_k \sin(kx) \Big)
		\]
		Für gerade $n$ entspricht das genau $t(x)$ und es bleibt noch die Darstellung der $a_k$ und $b_k$ zu zeigen.

		Für ungerade $n$ betrachten wir in $t(x)$ den Ausdruck $\theta c_{m+1} e^{i(m+1)x}$.
		In diesem Fall ist $m+1 = \f {n+1}2$ und wir setzen 
		\[
			a_{m+1} := c_{m+1} + c_{-(m+1)} = c_{m+1} + c_{n+1-(m+1)}0= c_{m+1} + c_{2(m+1)-(m+1)} = 2 c_{m+1}
		\]
		also gilt
		\begin{align*}
			\theta c_{m+1} e^{i(m+1)x} 
			&= \theta \f {a_{m+1}}2 e^{2\pi \f l{n+1}(m+1)}
			= \f \theta 2 a_{m+1} e^{i\pi l}
			= \f \theta 2 a_{m+1} \cos(\pi l) \\
			&= \f \theta 2 a_{m+1} \cos\Big( \underbrace{2 \pi \f l{n+1}}_{x_l} \cdot \underbrace{\f {n+1}2}_{m+1} \Big) 
			= \f \theta 2 a_{m+1} \cos((m+1)x_l)
		\end{align*}
		Für $t(x)$ ergibt sich dann
		\begin{align*}
			t(x) &= \sum_{k=-m}^m c_k e^{ikx} + \theta c_{m+1} e^{i(m+1)x}\\
			&= \f {a_0}2 + \sum_{k=1}^m \Big(a_k \cos(kx) + b_k \sin(kx)\Big) + \f \theta 2 a_{m+1} \cos\Big((m+1)x_l\Big)
		\end{align*}
		und die gesuchte Darstellung für $t(x)$ ist gefunden.

		Für die Koeffizienten gilt 
		\begin{align*}
			a_k:= c_k + c_{-k} &= \f 1{n+1}\sum_{j=0}^n f_j (\omega_k^{-j}+\omega_k^j) \\
			&= \f 1{n+1} \sum_{j=0}^n f_j(e^{ijx_k} + e^{ijx_k})\\
			&= \f 2{n+1} \sum_{j=0}^n f_j \cos (jx_k)
		\end{align*}
		für $b_k$ analog mit $b_k := i(c_k -c_{-k})$.
	\end{proof}
	\begin{note}
		Es gilt zwar $t(x_l)=q(x_l)$, aber im Allgemeinen ist $t(x)\neq q(x)$ und auch $t(x)\neq \Re(q(x))$ für $x\neq x_l$.
	\end{note}
\end{st}

\begin{ex*}
	Sei $n=2$ mit den folgenden Stützstellen und Werten. \\
	\begin{table}[H]
		\centering
		\begin{tabular}{c|c|c|c}
			$x_j$ & $x_0=0$ & $x_1=\f 23 \pi$ & $x_2=\f 43 \pi$\\\hline
			$f_j$ & $0$ & $\f 32$ & $\f 32$
		\end{tabular}
	\end{table}
	Es ist $\cos(\pm x_1) = \cos(\pm x_2) = -\f 12$, $\sin(x_1) = -\sin(x_2) =: \xi $
	\begin{align*}
		c_0 &= \f 13 (f_0e^{-i0} + f_1 e^{-i0} + f_2e^{-i0}) = 1\\
		c_1 &= \f 13 (f_0e^{-i0} + f_1 e^{-i\f 23 \pi} + f_2 e^{-i \f 43 \pi}) = \dotsb = -\f 12\\
		c_2 &= \f 13 (f_0e^{-i0} + f_1 e^{-i \f 43 \pi} + f_2 e^{-i \f 83 \pi}) = \dotsb = -\f 12
	\end{align*}
	Das komplexe trigonometrische Interpolationspolynom ergibt sich also durch
	\[
		q(z) = 1 - \f 12 e^{iz} - \f 12 e^{i2z}
	\]
	Für den Realteil gilt
	\[
		\Re(q(z)) = 1 - \f 12 \cos(z) - \f 12 \cos(2z)
	\]
	Für die reelle Interpolierende gilt
	\[
		m = \f n2 = 1,\quad \theta =0
	\]
	Die Koeffizienten ergeben sich durch
	\begin{align*}
		a_0 &= c_0 + c_{-0} = 2c_0 = 2\\
		a_1 &= c_1 + c_{-1} = c_1 + c_2 = -1\\
		b_1 &= i(c_1-c_{-1}) = i(c_1-c_2) = 0
	\end{align*}
	Das reelle trigonometrische Interpolationspolynom ergibt sich dann durch
	\[
		t(x) = 1 - 1 \cos(x)
	\]
	Offensichtlich ist $q(x)\neq t(x) \neq \Re(q(x))$, obwohl $q(x_k) = t(x_k) = f_k$ erfüllt ist.
\end{ex*}

\subsection{Diskrete Fourier Transformation}

\begin{df}[Diskrete Fourier Transformation] \label{1.26}
	Sei $N\in \N$ und $f\in \C^N$ ein Vektor mit komplexen Funktionswerten.
	Wir nennen $\scr F_N: \C^N \to \C^N$, definiert durch
	\[
		\scr F_N(f) = \f 1N \cdot W_N \cdot f
	\]
	die \emph{diskrete Fourier-Transformation (DFT)}, wobei
	\[
		W_N = \big(w_{k}^{-j}\big)_{k,j=0}^{N-1} \in \C^{N\times N} \qquad w_{k}^j = e^{ijk \f {2\pi}N}
	\]
	\begin{note}
		\begin{itemize}
			\item
				Man findet in der Literatur verschiedene Skalierungsfaktoren, wir benutzen $\f 1N$.
			\item
				Wir indizieren Vektoren/Matrizen bei $0$ beginnend.		
			\item
				Die Beziehung zur trigonometrischen Interpolation in Satz \ref{1.24} besteht in
				\begin{align*}
					f &= (f_j)_{j=0}^n, \qquad N:= n+1
				\end{align*}
				und
				\begin{align*}
					(\scr F_n(f))_k = \left( \f 1N W_n f\right)_k &= \f 1N \sum_{j=0}^{N-1}w_k^{-j} f_j \\
					&= \f 1{n+1} \sum_{j=0}^{n}w_k^{-j} f_j = c_k
				\end{align*}
				also werden die Koeffizienten $c_k$ der komplexen trigonometrischen Interpolation durch die DFT erhalten:
				\[
					(c_k)_{k=0}^n = \scr F_N(f)
				\]
		\end{itemize}
	\end{note}
\end{df}

\begin{lem}[Eigenschaften] \label{1.27}
	\begin{enumerate}[i)]
		\item
			$\displaystyle W_N$ ist regulär
		\item
			$\displaystyle W_N = W_N^T$, also $W_N$ symmetrisch
		\item
			$\displaystyle W_NW_N^{*} = N\cdot I$
		\item
			$\displaystyle \scr F_N^{-1}(c) = W^*_N\cdot c = N \cdot \_{\scr F_N(\_c)}$
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item
				$W_N$ ist eine komplexe Vandermonde-Matrix zu den paarweise verschiedenen Stellen
				\[
					x_k = e^{-ik\f {2\pi}N}
				\]
				Also ist die Matrix regulär.
			\item
				$\displaystyle W_N = (w_k^{-j}) \stackrel{\ref{1.24} i)}=  (w_j^{-k}) = W_N^T$
			\item
				$\displaystyle (W_NW_N^*)_{k,l} = \sum_{m=0}^{N-1} w_k^{-m}w_l^m = \sum_{m=0}^{N-1} w_{l-k}^m \stackrel{\ref{1.24} ii)}= N\cdot \delta_{kl} = (N\cdot I)_{k,l}$
			\item
				Wegen
				\[
					\scr F_N (W_N^* c) = \f 1N W_N W_N^* c \stackrel{iii)}= \f N N \cdot c = c
				\]
				ist also $\scr F_N^{-1} (c) = W_N^* c$ und es gilt
				\[
					N \_{\scr F_N(\_c)}
					= N \_{\left(\f {1}{N} W_N \_c\right)} 
					\stackrel{ii)}= \f{N}{N} \_{W_N^T \_c} 
					= W_N^* c 
					= \scr F_N^{-1}(c)
				\]
		\end{enumerate}
	\end{proof}
\end{lem}


\subsection{Schnelle Fourier-Transformation}

Der Berechnungsaufwand der DFT ist $\mathcal O(N^2)$ (für die Matrixvektormultiplikation).

Für $N=2^Q$, $Q\in \N$, lässt sich die Berechnung auf $\mathcal O(N\log N)$ beschleunigen.
Die Idee ist “Divide and Conquer”: Zerlege das Problem der Größe $N$ rekursiv in 2 Teilprobleme der Größe $\f N2$ und führe diese Lösungen zur Gesamtlösung zusammen.

\newcommand{\xeven}{\ensuremath{x_{\text{even}}}}
\newcommand{\xodd}{\ensuremath{x_{\text{odd}}}}
Sei im Folgenden immer $N=2^Q$, $Q\in \N$ und $x=(x_i)_{i=0}^{N-1} \in \C^N$.
Außerdem sei $P_N \in \C^{\N\times \N}$ eine Permutationsmatrix und $\xeven, \xodd \in \C^{\f N2}$ Teilvektoren von $x$, die jeweils nur die geraden (bzw. ungeraden) Einträge besitzen.
Sei $P_N$ so gewählt, dass folgender Zusammenhang besteht:
\[
	P_N \begin{pmatrix}x_0\\ \vdots \\ x_{N-1}\end{pmatrix}
	= \begin{pmatrix} x_0 \\ x_2 \\ \vdots \\ x_{N-2} \\ x_1 \\ x_3 \\\vdots \\ x_{N-1}\end{pmatrix}
	= \begin{pmatrix} \xeven \\ \xodd \end{pmatrix}
\]
Mit anderen Worten: $P_N$ vertauscht die Zeilen des Vektors $x$ so, dass in der ersten Hälfte erst die geraden Einträge und dann in der zweiten Hälfte die ungeraden Einträge auftreten.

\begin{lem}[Rekursion für $W_N$]
	\label{1.28}
	Sei $\omega := e^{i\f {2\pi}N}$ und
	\[
		W_N := (\omega^{-kj})_{k,j=0}^N \qquad
		D_N := \diag\Big(\omega^0, \omega^{-1}, \dotsc, \omega^{-(N-1)}) \in \C^{N\times N}
	\]
	$W_N$ ist genau die Matrix der DFT aus \ref{1.26}.
	Dann ist
	\[
		W_N = \begin{pmatrix}W_{\f N2} & D_{\f N2}W_{\f N2} \\
			W_{\f N2} & - D_{\f N2} W_{\f N2} \end{pmatrix}
		\cdot P_N
	\]
	\begin{proof}
		Da $P_NP_N^T = I$ (da orthogonal), reicht es zu zeigen, dass
		\[
			W_NP_N^T = \begin{pmatrix}W_{\f N2} & D_{\f N2}W_{\f N2} \\
			W_{\f N2} & - D_{\f N2} W_{\f N2} \end{pmatrix}
		\]
		Wobei die Multiplikation mit $P_N^T$ ein Spaltentausch durchführt.
		Sei 
		\[
			W_NP_N^T = \begin{pmatrix}
				A_{11} & A_{12}\\
				A_{21} & A_{22}
			\end{pmatrix}
		\]
		Wir zeigen die Übereinstimmung für die einzelnen Blöcke.
		\begin{align*}
			(A_{11})_{k,j}
			&= (W_N P_N^T)_{k,j} = (W_N)_{k,2j} \\
			&= e^{-i2\pi \f {k2j}N} = e^{-i2\pi \f {kj}{\f N2}} = (W_{\f N2})_{k,j} \\
			(A_{21})_{k,j}
			&= (W_N P_N^T)_{\f N2 + k,j} = (W_N)_{\f N2+k,2j} \\
			&= e^{-i2\pi \f {(\f N2 + k)+2j}{N}} = \underbrace{e^{-i2\pi j}}_{=1} e^{-i2\pi {kj}{\f N2}} = (W_{\f N2})_{k,j} \\
			(A_{12})_{k,j} 
			&= (W_NP_N^T)_{k,\f N2 +j} = (W_N)_{k,2j+1} \\
			&= e^{-i2\pi \f {k(2j+1)}N} = e^{-i2\pi \f {kj}{\f N2}} \underbrace{e^{-i \f {2\pi}N k}}_{\omega^{-k}} = (D_{\f N2}W_{\f N2})_{k,j} \\
			(A_{22})_{k,j} 
			&= (W_NP_N^T)_{\f N2+k,\f N2+j} = (W_N)_{\f N2+k, 2j+1} \\
			&= e^{-2\pi\f {(\f N2 +k)(2j+1)}N} = \underbrace{e^{-i2\pi j}}_{=1} \underbrace{e^{-i\pi}}_{=-1} e^{-i2\pi \f {kj}{\f N2}} \underbrace{e^{-i\f {2\pi}N k}}_{=\omega^{-k}} = (-D_{\f N2}W_{\f N2})_{k,j}
		\end{align*}
	\end{proof}
\end{lem}

\begin{st}[Fast-Fourier-Transformation] \label{1.29}
	Die DFT lässt sich für $N=2^Q$ rekursiv berechnen über
	\[
		\scr F_N(f) = \f 12 
		\begin{pmatrix}I & I \\ I & -I\end{pmatrix} 
		\begin{pmatrix} \scr F_{\f N2}(f_{\text{even}}) \\ D_{\f N2} \scr F_{\f N2}(f_{\text{odd}})\end{pmatrix}
	\]
	Dies nennt man \emph{schnelle Fouriertransformation} (Fast Fourier Transformation).
	\begin{proof}
		\begin{align*}
			\scr F_N(f) &= \f 1N W_N f 
			\stackrel{\ref{1.28}}= \f 1N \begin{pmatrix}W_{\f N2} & D_{\f N2}W_{\f N2}\\ W_{\f N2} & - D_{\f N2}W_{\f N2} \end{pmatrix} P_n f\\
			&= \f 1N \begin{pmatrix}I & I \\ I & -I\end{pmatrix}\begin{pmatrix}W_{\f N2} & 0 \\ 0 & D_{\f N2}W_{\f N2} \end{pmatrix} \begin{pmatrix} f_{\text{even}} \\ f_{\text{odd}} \end{pmatrix}\\
			&= \f 1N \begin{pmatrix}
				I & I\\ I & -I
			\end{pmatrix}
			\begin{pmatrix}
				W_{\f N2} f_{\text{even}} \\
				D_{\f N2}W_{\f N2} f_{\text{odd}}
			\end{pmatrix}\\
			&= \f 1N \begin{pmatrix}
				I & I \\ I & -I
			\end{pmatrix}
			\f N2
			\begin{pmatrix}
				\f 1{\f N2} W_{\f N2} f_{\text{even}}\\
				D_{\f N2} \f 1{\f N2} W_{\f N2} f_{\text{odd}}
			\end{pmatrix}
			= \f 12 \begin{pmatrix}
				I & I \\ I & -I
			\end{pmatrix}
			\begin{pmatrix}
				\scr F_{\f N2}(f_{\text{even}})\\
				D_{\f N2} \scr F_{\f N2}( f_{\text{odd}})
			\end{pmatrix}
		\end{align*}
	\end{proof}
\end{st}

\begin{nt*}[Aufwandsbetrachtung]
		\begin{itemize}
			\item
				Sei der Berechnungsaufwand für $\scr F_n(f)$ gegeben durch $C(N)$ und $C(1)=0$ (denn für $N=1$ ist nichts zu tun).
				Für den Aufwand gilt dann in etwa:
				\[
					C(N) = \underbrace{2C\left(\f N2\right)}_{\text{Teilprobleme der Größe $\f N2$}} + \underbrace{\f N2}_{\text{Multiplikation mit $D_{\f N2}$}} + \underbrace{\f N2}_{\text{Addition}} + \underbrace{\f N2}_{\text{Substraktion}} \le 2 C\left(\f N2\right) + K\cdot N
				\]
				für ein $K\in \R$.
				Für $N=2^Q$ folgt
				\begin{align*}
					C(N) &= 2C\l(\f N2\r) + KN = 2\l(2C\l(\f N4\r) + K\f N2\r) + KN \\
					&= 4C\l(\f N4\r) + KN + KN \\
					= \dotso &= NC(1) + QKN = N + K(\log_2 N)N 
				\end{align*}
				also $\mathcal O(N\log N)$.
			\item
				Dies ist eine wesentliche Beschleunigung der DFT.
			\item
				Damit sind wesentlich größere Probleme behandelbar, denn $W_N$ muss nicht aufgestellt werden.
		\end{itemize}
\end{nt*}

\begin{nt*}
	\begin{itemize}
		\item
			Bei der Interpolation von Punkten im $\R^2$ mit geschlossener Kurve ergibt sich eine $2\pi$-periodische Funktion $r(\phi)$, die den Rand eines sternförmigen Gebiets beschreibt.
		\item
			Eine interessante Anwendung ist z.B. das Entrauschen von Signalen (eindimensional) und Bildern (zweidimensional).
			Dazu führt man eine FFT durch und setzt $c_k= 0$ für diejenigen $k$, die zu hohen Frequenzen gehören. 
			Anschließend wird eine inverse FFT durchgeführt.
	\end{itemize}
\end{nt*}


\section{Spline-Interpolation}


Seien ein kompaktes Intervall $I=[a,b]$, $a=x_0<x_1 < \dotsb < x_n = b$ und Zielwerte $\{f_j\}_{j=0}^n$ gegeben.
Die normale Polynominterpolation führt häufig zu Oszillationen.
Wir suchen deshalb eine interpolierende Funktion, welche auf Teilintervallen aus unterschiedlichen Polynomen besteht, aber trotzdem eine gewisse globale Differenzierbarkeit aufweist.

\begin{df}[Spline-Räume]
	\label{1.30}
	Zu Stützstellen $a=x_0 < x_1 < \dotsb < x_n = b$, $I=[a,b]$ und $m\in \N$ definieren wir den Spline-Raum der Ordnung $m$ als
	\[
		S_m := \Big\{ f\in C^{m-1}(I) : f\big|_{[x_{i-1},x_i]}\in \P_m\Big\}
	\]
	Zu einem Spline $s\in S_m$ bezeichnen wir die Teilpolynome als
	\[
		p_i(x) := s(x)\big|_{[x_{i-1},x_i]} \in \P_m \qquad i=1,\dotsc,n
	\]
\end{df}

\begin{nt*}
	\begin{itemize}
		\item
			Die Anzahl der Freiheitsgrade beträgt $n(m+1)$ für $n$ unabhängige Teilpolynome von Grad $m$.

			Für jeden der $n-1$ inneren Punkte $x_j$ ($j=1,\dotsc,n-1$) gibt es jeweils $m$ Bedingungen, nämlich
			\[
				p_j^{(k)}(x_j) = p_{j+1}^{(k)}(x_j) \qquad k = 0, \dotsc, m-1
			\]
			Damit bleiben $n(m+1)-(n-1)m = n+m$ Freiheitsgrade für Interpolationspolynome aus $S_m$.
			Es gilt
			\[
				\dim (S_m) = n+m
			\]
		\item
			Bei der Interpolation sind $n+1$ Bedingungen gegeben:
			\[
				s(x_j) = f_j \qquad j=0,\dotsc,n
			\]
			Es fehlen also noch $\dim (S_m) - (n+1) = m-1$ Bedingungen.
		\item
			Üblicherweise wählt man deshalb noch zusätzliche Bedingungen für die Ableitungen an den Endpunkten.
		\item
			Bei linearen Splines (aus $S_1$, $m=1$) sind die Interpolationsbedingungen ausreichend (siehe \ref{1.31}).
	\end{itemize}
\end{nt*}

\begin{st}[Existenz und Eindeutigkeit von linearen Splines]
	\label{1.31}
	Für $m=1$ existiert genau ein $s\in S_1$ mit $s(x_j) = f_j$ für $j=0,\dotsc,n$.
	Es hat die explizite Gestalt
	\[
		s(x) \big|_{[x_{j-1},x_j]} = p_j(x) = f_j \f {x-x_{j-1}}{x_j - x_{j-1}} + f_{j-1}\f {x-x_j}{x_{j-1}-x_j}
		\qquad j=1,\dotsc,n
	\]
	\begin{proof}
		Auf jedem Teilintervall ist die lineare Interpolation eindeutig durch die obige Lagrange-Form festgelegt.
		Also auch für den gesamte Spline.
	\end{proof}
\end{st}

\subsection{Kubische Splines}

Bei kubischen Splines ($m=3$) fehlen 2 Bedingungen.
In diesem Fall gibt es folgende Möglichkeiten, zusätzliche sogenannte \emph{Randbedingungen} zu wählen:
\begin{enumerate}[{(R}1{)}]
	\item
		Natürliche Splines
		\[
			p_1''(a) = p_n''(b)=0
		\]
		Mit anderen Worten: keine Krümmung an den Randpunkten
	\item
		Hermite Randbedingungen für gegebenes $\alpha, \beta \in \R$
		\[
			p_1'(a) = \alpha, \quad p_n'(b) = \beta
		\]
	\item
		Periodische Splines
		\[
			p_1'(a) = p_n'(b), \quad p_1''(a) = p_n''(b)
		\]
		Dieser Begriff ist nur sinnvoll, falls $p_1(a) = p_n(b)$, die Kurve also geschlossen ist.
\end{enumerate}

\begin{st}[Existenz und Eindeutigkeit kubischer Splines]
	\label{1.32}
	Für jede Wahl von obigen Randbedingungen (R1) bis (R3) existiert genau ein $s\in S_3$, welches die Randbedingungen (R1),(R2) bzw. (R3) erfüllt und gilt $s(x_i)=f_i$ für $i=0,\dotsc,n$.
	\begin{proof}
		Für (R1) sei die Darstellung des Splines gegeben durch
		\[
			s(x)\big|_{[x_{j-1},x_j]} = p_j(x) = a_j(x-x_{j-1})^3 + b_j(x-x_{j-1})^2 + c_j(x-x_{j-1}) + d_j
		\]
		Die Bedingungen sind
		\begin{alignat*}{5}
			p_j(x_{j-1}) &= f_{j-1}& \quad p_j(x_j) &= f_j & \qquad j&=1,\dotsc,n & &\qquad \text{(Zusammenhang)} &\\
			p_j'(x_j) &= p_{j+1}'(x_j) & \quad p_j''(x_j) &= p_{j+1}''(x_j) &  \qquad j&=1,\dotsc,n-1 & &\qquad \text{(Glattheit)} &\\
			p_1''(x_0)&=0& \quad p_n''(x_n) &= 0 & & & &\qquad \text{(Randbedingung)} &
		\end{alignat*}
		Sei $h_j := x_j - x_{j-1}$.
		Verwende nun $M_j = s''(x_j)$ für $j=0,\dotsc,n$ als neue Variablen.
		Es gilt
		\begin{align*}
			p_j'(x) &= 3a_j(x-x_{j-1})^2 + 2b_j(x-x_{j-1})+c_j \\
			p_j''(x) &= 6a_j(x-x_{j-1}) + 2b_j
		\end{align*}
		für $a_j, b_j, c_j, d_j$ gilt
		\begin{align*}
			M_{j-1} &= p_j''(x_{j-1}) 
				= 2b_j  \\
				&\implies b_j
					=\f {M_{j-1}}2 \\
			M_j &= p_j''(x_j) 
				= 6a_j(x_j-x_{j-1}) + 2b_j \\
				&\implies a_j 
					= \f {M_j - 2b_j}{6h_j} 
					= \f {M_j - M_{j-1}}{6h_j} \\
			f_{j-1} &= p_j(x_{j-1}) 
				= d_j  \\
				&\implies d_j 
					= f_{j-1} \\
			f_j &= p_j(x_j) 
				= a_j h_j^3 + b_jh_j^2 + c_jh_j + d_j \\
				&\implies c_j 
					= \f{f_j-d_j}{h_j} - a_jh_j^2 - b_jh_j 
					= \f {f_j-f_{j-1}}{h_j} - h_j\left(\f {M_j}6 + \f{M_{j-1}}3 \right)
		\end{align*}
		Damit haben wir die Bedingungen des Zusammenhangs und $p_j''(x_j) = p_{j+1}''(x_j)$ verarbeitet.
		Da $a_j,b_j,c_j,d_j$ durch $M_j$ eindeutig bestimmt sind, reicht es die Eindeutigkeit der $M_j$ unter den übrigen Bedingungen zu zeigen.

		Es gilt
		\begin{align*}
			p_j'(x_j) &= 3a_jh_j^2 + 2b_jh_j + c_j \\
			&= \f {M_j-M_{j-1}}2 h_j + h_j M_{j-1} + \f {f_j - f_{j-1}}{h_j} - h_j \l( \f {M_j}6 + \f{M_{j-1}}3\r) \\
			&= \f {f_j-f_{j-1}}{h_j} + h_j \l( \f{M_j}3 + \f{M_{j-1}}6\r) \qquad j=1,\dotsc,n \\
			p_{j+1}'(x_j) &= c_{j+1} = \f {f_{j+1}-f_j}{h_{j+1}} - h_{j+1}\left( \f{M_{j+1}}6 + \f {M_j}3\right) \qquad j=1,\dotsc,n-1
		\end{align*}
		Wegen der Bedingung $p_j'(x_j) = p_{j+1}'(x_j)$, erhalten wir ein lineares Gleichungssystem für $n-1$ Unbekannte $M_1,\dotsc,M_{n-1}$:
		\[
			\f {h_j}6 M_{j-1} + \f {h_j + h_{j+1}}3 M_j + \f{h_{j+1}}6 M_{j+1} = \f {f_{j-1}-f_j}{h_{j+1}} - \f{f_j-f_{j-1}}{h_j}
		\]
		Multipliziert mit $6$ und der Randbedingung $p_1''(x_0) = M_0 = 0 = M_n = p_n''(x_n)$, ergibt sich
		\[
			\begin{pmatrix}
				2(h_1+h_2) & h_2 &  \cdots & 0 \\
				h_2 & 2(h_2+h_3) & \cdots  & 0 \\
				\vdots & \ddots & \ddots & \vdots\\
				0 & \cdots & h_{n-1} & 2(h_{n-1}+h_n)
			\end{pmatrix}
			\begin{pmatrix}
				M_1 \\ \vdots \\ M_{n-1}
			\end{pmatrix}
			=
			\begin{pmatrix}
				\f 6{h_2}(f_2-f_1) - \f 6{h_1}(f_1-f_0)\\
				\vdots \\
				\f 6{h_n}(f_n-f_{n-1})- \f 6{h_{n-1}}(f_{n-1}-f_{n-2})
			\end{pmatrix}
		\]
		Die Matrix ist sogar streng diagonaldominant, positiv definit und symmetrisch, also regulär.
		Damit ist das LGS eindeutig lösbar.

		\fixme[Beweis für (R2) und (R3) fehlt?]
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				Der Beweis war konstruktiv.
			\item
				Das Umschreiben des Systems durch zusätzliche Variablen $M_j$ als Tridiagonalsystem ist eine große Vereinfachung.
				Damit müssen nur $n-1$ Gleichungen mit jeweils nur $3$ gekoppelten Unbekannten gelöst werden (vorher waren das $4n$ Unbekannte und bis zu $5$ gekoppelte Variablen pro Gleichung).
			\item
				Die Lösung eines Tridiagonalsystems ist sehr effizient ($\mathcal O(n)$) lösbar durch eine LR-Zerlegung mit tridiagonalen unteren (bzw. oberen) Dreiecksmatrizen $L$ und $R$ und anschließender Vorwärts- und Rückwärtssubstitution.
		\end{itemize}
	\end{note}
\end{st}

\begin{st}[Optimalität]
	\label{1.33}
	Sei $s\in S_3$ eine interpolierender Spline zu den Stützstellen $x_i$ und den Funktionswerten $f_i$ ($i=0,\dotsc,n$)
	und eine der Randbedingungen (R1)-(R3) erfüllt.

	Dann gilt für alle $f\in C^2([a,b])$ mit denselben Bedingungen (d.h. $f(x_i)=f_i$ und eine der Randbedingungen (R1,R2,R3)):
	\[
		\int_a^b |f''(x)|^2 dx \ge \int_a^b |s''(x)|^2  dx
	\]
	\begin{note}
		 Physikalische Interpretation: \\
		Ein „biegsamer Stab“ (engl. spline) wird mit Nägeln in den Punkten $(x_j,f_j)$ befestigt.
		Der Verlauf $f(x)$ des Splines minimiert die Biegeenergie
		\[
			E = \alpha \int_{x_0}^{x_n}\f {(f''(x))^2}{(1+f'(x)^2)^3} dx \qquad \alpha \text{ Materialkonstante mit Einheiten}
		\]
		Falls $f'(x)$ klein ist, kann dies durch
		\[
			\int_{x_0}^{x_n} (f''(x))^2 dx
		\]
		approximiert werden.
		Damit approximieren kubische Splines den Verlauf eines eingespannten Stabes.
	\end{note}
	\begin{proof}
		Sei $f\in C^2([a,b])$ eine zu interpolierende Funktion.
		Dann gilt
		\begin{align*}
			\int_a^b (f-s)''(x) s''(x) dx 
			&= \sum_{j=1}^n \int_{x_{j-1}}^{x_j} (f-p_j)''(x)p_j''(x)) dx \\
			&= \sum_{j=1}^n \bigg( \Big[(f-p_j)'(x)p_j''(x)\Big]_{x_{j-1}}^{x_j} - \int_{x_{j-1}}^{x_j} (f-p_j)'(x)p_j'''(x) dx \bigg)\\
			&= \underbrace{\Big( f'(b) - p_n'(b)\Big) p_n''(b) - \Big( f'(a)-p_1'(a)\Big) p_1''(a)}_{\displaystyle =0 \text{ wegen } \begin{cases} 
				p_1''(a) = p_n''(b) = 0 & \text{falls (R1)} \\
				f'(b) = p_n'(b) = \beta,\quad f'(a) = p_1'(a) = \alpha  &\text{falls (R2)} \\
				f'(a) = f'(b),\quad p_1'(a)=p_n'(b),\quad p_1''(a)=p_n''(b) & \text{falls (R3)}
				\end{cases}}  \\
			&\qquad - \sum_{j=1}^n \int_{x_{j-1}}^{x_j} (f-s)'(x))s'''(x) dx \\
			&= \sum_{j=1}^n \underbrace{\bigg(-\Big[(f-p_j)(x)p_j'''(x)\Big]_{x_{j-1}}^{x_j}\bigg)}_{=0 \text{, da } f(x_j)=p_j(x_j)} + \underbrace{\int_{x_{j-1}}^{x_j} (f-s)(x)s^{(4)}(x)}_{=0}
		\end{align*}
		Insgesamt gilt dann
		\begin{align*}
			\int_a^b |s''(x)|^2 dx 
			&= \int_a^b (s''(x))^2 dx + \underbrace{\int_a^b (f-s)''(x)s''(x) dx}_{=0} \\
			&= \int_a^b (s''(x) + (f-s)''(x)) s''(x) dx \\
			&= \int_a^b f''(x) s''(x) 
			\intertext{und mit Cauchy-Schwarz:}
			& \le \sqrt{\int_a^b (f''(x))^2 dx} \sqrt{\int_a^b (s''(x))^2 dx} \\
		\end{align*}
		Division ergibt dann
		\begin{align*}
			\sqrt{\int_a^b (s''(x))^2 dx} \le \sqrt{\int_a^b (f''(x))^2 dx}
		\end{align*}
		Quadrieren ergibt die Behauptung.
	\end{proof}
\end{st}

\begin{st}[Fehleraussage und Konvergenz]
	\label{1.34}
	Sei $f\in C^4([a,b])$, $f_i=f(x_i)$, $\{x_i\}_{i=0}^n$ äquidistant mit $\f {b-a}n$.
	Sei $s\in S_3$ mit (R2), d.h. $s'(a)=f'(a)$ und $s'(b)=f'(b)$.
	Dann gilt für $l=0,1,2,3$:
	\[
		\|f^{(l)} - s^{(l)}\|_\infty \le 2 \|f^{(4)}\|_\infty \cdot h^{4-l}
	\]
	also insbesondere
	\[
		\|f-s\|_\infty \le 2\|f^{(4)}\|_\infty \cdot h^4
	\]
	\begin{proof}
		Siehe Stoer-Bulirsch Thm. 2.5.3.3.
	\end{proof}
	\begin{note}
		Es gilt Konvergenz für jedes $f\in C^4$, im Gegensatz zur Polynominterpolation.
	\end{note}
\end{st}

Frage: Kann man auf einfache Weise eine Basis für $S_m$ angeben?

\begin{df}[B-Splines]
	\label{1.35}
	Sei $\{x_i\}_{i\in \Z}$ eine streng monoton wachsende Folge in $\R$ mit $\lim_{i\to \pm \infty} x_i = \pm \infty$.
	Dann sind B-Splines $B_{i,k}:\R\to \R$ von Grad $k\in \N_0$ rekursiv definiert durch
	\[
		B_{i,0} := \begin{cases}
			1 & x_i < x \le x_{i+1} \\
			0 & \text{sonst}
		\end{cases}
	\]
	und
	\[
		B_{i,k}(x) := \omega_{i,k}(x) B_{i,k-1}(x) + \big(1-\omega_{i+1,k}(x)\big)B_{i+1,k-1}(x)
	\]
	mit
	\[
		\omega_{i,k}(x) := \f {x-x_i}{x_{i+k}- x_i}
	\]
\end{df}

\begin{ex*}
	\begin{align*}
		B_{0,0}(x) &= 1\big|_{(x_0,x_1]} \\
		B_{1,0}(x) &= 1\big|_{(x_0,x_2]} \\
		B_{0,1}(x) &= \begin{cases}
			\f {x-x_0}{x_1-x_0} & \text{auf $(x_0,x_1]$} \\
			1 - \f {x-x_1}{x_2-x_1} & \text{auf $(x_1,x_2]$} \\
			0 & \text{sonst}
		\end{cases}
	\end{align*}
\end{ex*}

\begin{st}[Eigenschaften]
	\label{1.36}
	\begin{enumerate}[i)]
		\item
			$\displaystyle B_{i,k}\big|_{(x_j,x_{j+1})} \in \P_k \qquad \forall i,j\in \Z, k\in \N_0$
		\item
			$\displaystyle B_{i,k} \in C^{k-1}(\R)$
		\item
			$\displaystyle \supp (B_{i,k}) \subset [x_i,x_{i+k+1}]$
			Dabei ist der „Träger“ $\supp (f)$ definiert als
			\[
				\supp(f) := \_{\{x\in A : f(x) \neq 0\}}
			\]
		\item
			$\displaystyle B_{i,k} \ge 0$
		\item
			$\displaystyle \sum_{i\in \Z} B_{i,k} = 1 \qquad$ (Zerlegung der Eins)
		\item
			$\displaystyle \{B_{i,m}\big|_{[x_0,x_n]}\}_{i=-m}^{n-1}$ bilden eine Basis von $S_m$.
	\end{enumerate}
	\begin{proof}
		Leichte Übung.
	\end{proof}
\end{st}


\section{Über- und Unterbestimmte Systeme}


\begin{itemize}
	\item
		Was ist eine gute Funktionsapproximation, falls die Anzahl der Bedingungen ungleich der Anzahl der Freiheitsgrade ist?
	\item
		Äquivalent dazu: Wie löst man unterbestimmte oder überbestimmte lineare Gleichungssysteme
		\[
			Ax=b \qquad \text{mit } A\in \R^{m\times n}, x\in \R^n, b\in \R^m
		\]
	\item
		Aus der linearen Algebra gibt es drei bekannte Fälle: eindeutige Lösung, keine Lösung, unendlich viele Lösungen.
		Gibt es in all diesen Fällen eine eindeutige, ausgezeichnete „Lösung“?
\end{itemize}

Die “Minimum Norm Least Squares”-Approximation liefert eine Antwort auf diese Fragen.

\begin{st}[Singulärwertzerlegung (SVD)]
	\label{1.37}
	Sei $A\in \C^{m\times n}$, dann existieren unitäre Matrizen $U\in \C^{m\times m}$, $V\in \C^{n\times n}$ und ein reelle Diagonalmatrix
	\[
		\Sigma := \diag( \sigma_1, \dotsc, \sigma_r)  \in \R^{m\times n}
	\]
	mit $\sigma_1\ge \sigma_2 \ge \dotsb \sigma_r \ge 0$ und $r:= \min(m,n)$ so, dass
	\[
		A = U \Sigma V^*
	\]
	Wir nennen dies Singulärwertzerlegung (Singular Value Decomposition) und die $\sigma_i$ Singulärwerte.
	\begin{note}
		\begin{itemize}
			\item
				Falls $A$ reell ist, sind auch $U,V$ reell, also orthogonal, und es gilt
				\[
					A=U\Sigma V^T
				\]
			\item
				Erinnerung aus der linearen Algebra:
				Falls $A \in \R^{n\times n}$ symmetrisch und positiv semidefinit, dann existiert $V$ orthogonal, $D$ diagonal mit $A=VDV^T$.

				Also ist die Diagonalisierung in Eigenwerte eine SVD.
			\item
				Die SVD ist sogar allgemeiner als die Eigenwertzerlegung:
				für eine Jordanmatrix
				\[
					A = \begin{pmatrix}
						-1 & 1 \\
						0 & -1
					\end{pmatrix}
				\]
				existiert nämlich keine Eigenwertzerlegung, aber durchaus eine Singulärwertzerlegung, nämlich:
				\[
					U = \f 1{\sqrt{1+\phi^2}} \begin{pmatrix}
						\phi & 1 \\
						-1 & \phi
					\end{pmatrix}, \qquad
					\Sigma = \begin{pmatrix}
						\phi & 0 \\
						0 & \phi -1
					\end{pmatrix}, \qquad 
					V = \f 1{\sqrt{1+\phi^2}} \begin{pmatrix}
						-1 & -\phi \\
						\phi & - 1
					\end{pmatrix}
				\]
				für $\qquad \phi := \f {1+\sqrt 5}2$.
			\item
				Die Matrix $\Sigma$ ist eindeutig. 
				$U$ und $V$ sind es jedoch nicht (wechsle simultan das Vorzeichen in den Zeilen/Spalten von $U,V$).
			\item
				Falls $k := \rg(A) < \min(n,m)$, dann sind genau die ersten $k$ Singulärwerte ungleich Null, die restlichen verschwinden.
				Wichtig sind dann nur der linke obere Kasten von $\Sigma$, der linke Block von $U$ und der obere Block von $V$.
				Mann kann dann $A$ mit
				\[
					\tilde \Sigma = \diag(\sigma_1,\dotsc,\sigma_k) \in \R^{k\times k}
					, \qquad 
					\tilde U \in \C^{m\times k}
					, \qquad 
					\tilde V \in \C^{n\times k}
				\]
				schreiben als
				\[
					A = \tilde U \tilde \Sigma \tilde V^*
				\]
				Man nennt diese Darstellung \emph{verkürzte Singulärwertzerlegung} mit wesentlich kleineren Matrizen, also deutlich weniger Speicheraufwand.
				Die Matrix-Vektor-Multiplikation ist mit $\mathcal O(nk+mk)$ statt $\mathcal O(n^2 + m^2)$ möglich (\fixme[Ist das so richtig?]).
		\end{itemize}
	\end{note}
	\begin{proof}
		Die Matrix $A^*A \in \C^{n\times n}$ ist hermitesch (also $(A^*A)^*=A^*A$).
		Also existiert eine Eigenwert-Zerlegung (mit reellen Eigenwerten) von $A^*A$:
		\[
			A^*A = V\Lambda V^*
		\]
		mit $V=(v_1,\dotsc, v_n)\in \C^{n\times n}$ unitär, $\Lambda = \diag(\lambda_1,\dotsc,\lambda_n) \in \R^{n\times n}$, $\lambda_1\ge \dotsc \ge \lambda_n$.
		Die $v_i$ sind damit Eigenvektoren von $A^*A$ zu den Eigenwerten $\lambda_i$.

		Zeige, dass alle $\lambda_i \ge 0$:
		\[
			\lambda_i = \lambda_i \|v_i\|^2 = \lambda_iv_i^*v_i = v_i^* \lambda_i v_i = v_i^* A^*A v_i = (A v_i)^*(A v_i) =  \|Av_i\|^2 \ge 0
		\]
		Sei $k$ die Anzahl an Eigenwerten ungleich Null (also $\lambda_i \neq 0$ für $i=1,\dotsc,k$ und $\lambda_i=0$ für $i=k+1,\dotsc,n$).
		Setze 
		\[
			\sigma_i := \begin{cases}
				\sqrt{\lambda_i} & \text{für $i=1,\dotsc,k$} \\
				0 & \text{für $i=k+1,\dotsc, r$}
			\end{cases}
			\qquad i = 1,\dotsc,r
		\]
		und $\Sigma := \diag(\sigma_1,\dotsc,\sigma_r) \in \R^{m\times n}$.
		Setze 
		\[
			u_i := \f 1{\sqrt{\lambda_i}}Av_i
			\qquad i = 1,\dotsc, k
		\]
		Offensichtlich ist $u_i\in \im(A)$.
		Außerdem sind die $u_i$ sind orthonormal, denn:
		\[
			\<u_i,u_j\> 
			= u_i^*u_j 
			= \f 1{\sqrt{\lambda_i \lambda_j}} v_i^* A^*Av_j
			= \f 1{\sqrt{\lambda_i\lambda_j}}v_i^* \lambda_j v_j
			= \f {\lambda_j} {\sqrt{\lambda_i \lambda_j}}\cdot \delta_{ij} 
			= \delta_{ij}
		\]
		also auch linear unabhängig.

		Zeige $\ker A = \ker (A^*A)$.
		„$\subset$“ ist klar.
		Für die Umkehrung sei $x\in \ker (A^* A)$.
		\[
			A^*A x = 0 \implies x^*A^*Ax = 0 \implies \|Ax\|^2 = 0 \implies Ax=0
		\]
		Daher
		\begin{align*}
			\dim(\im(A)) 
			&= n - \dim (\ker A)) 
			= n-\dim (\ker(A^*A))  \\
			&\quad = n- (n- \dim(\im(A^*A))) 
			= n-(n-k) = k
		\end{align*}
		Also bilden die $\{u_i\}_{i=1}^k$ eine Orthonormalbasis von $\im(A)$.

		Wähle $\{u_{k+1},\dotsc, u_n\}$ als Orthonormalbasis von $\im( A)^\orth \subset \C^m$.
		Definiere 
		\[
			U := (u_1,\dotsc, u_m)
		\]
		$U$ ist offensichtlich unitär.

		Außerdem gilt mit dieser Wahl $A=U\Sigma V^*$:
		\[
			U\Sigma V^* v_i = U \Sigma e_i = U \sigma_i e_i = \begin{cases}
				\sigma_i u_i & i=1,\dotsc,k \\
				0 & i =k+1,\dotsc,n
			\end{cases}
		\]
		Für $i=1,\dotsc,k$ gilt
		\[
			\sigma_i u_i = \sqrt{\lambda_i} \cdot \f 1{\sqrt{\lambda_i}}Av_i = Av_i
		\]
		Für $i=k+1,\dotsc, n$ gilt wegen $v_i \in \ker (A^*A) = \ker (A)$
		\[
			0 = Av_i
		\]
		Damit ist
		\[
			U\Sigma V^* x = Ax \qquad \forall x\in \C^n
		\]
		Also $A= U\Sigma V^*$.
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				Der Beweis ist konstruktiv und liefert damit ein Verfahren zur Erzeugung einer SVD.
			\item
				$\displaystyle \im A = \<u_1,\dotsc,u_k\>$
			\item
				$\displaystyle \im A^* = \<v_1,\dotsc, v_k\>$
			\item
				$\displaystyle \ker A = \<v_{k+1},\dotsc, v_n\>$
			\item
				$\displaystyle \ker A^* = \<u_k,\dotsc, u_{k+1}\>$
		\end{itemize}
	\end{note}
\end{st}
\begin{alg*}[SVD]
\begin{algorithmic}
\Assume $ A\in \C^{n\times m} $
\Ensure $ A=U\Sigma V*$, mit $ U,V $  unitär und $\Sigma$ Diagonalmatrix. 
\State 1. Berechne Eigenwertzerlegung von $ A^*A=S\Lambda \tilde{S}^* $ mit $ \Lambda=\diag\{\lambda_1,..., \lambda_k, \lambda_{k+1},...,\lambda_{n}\} $, sodass $ \lambda_i>0, $ für $ i\le k $ und $ \lambda_i=0, $ für $ i>k $ und $ S=(v_1,...,v_n) $
\State 2. Setze $ \Sigma=\diag{\sqrt{\lambda_1}, ..., \sqrt{\lambda_n}} $
\State 3. Setze $ V=S $
\State 4. Berechne $ U=(u_1,...,u_m) $
\For{$ i=1:k $}
\State $ u_i=\sigma^{-1}Av_i $
\EndFor
\State Ergänze Rest zur orthonormalen Basis.
\end{algorithmic}
\end{alg*}
\begin{note*}
Der 4. Schritt kann ersetzt werden durch eine Eigenwertzerlegung von $ AA^* $, wobei diese sich analog wie in $ 1. $ ergibt.
\end{note*}

\begin{st}[Niedrigrang-Matrixbestapproximation]
	\label{1.38}
	Sei $A=U\Sigma V^* \in \C^{m\times n}$ eine SVD von $A$ mit $\Sigma = \diag(\sigma_1,\dotsc, \sigma_r) \in \C^{m\times n}$ und $r:=\min\{m,n\}$.

	Definiere für $k=1,\dotsc,r-1$:
	\[
		\hat\Sigma_k := \diag\{\sigma_1,\dotsc,\sigma_k,0,\dotsc,0\} \in \R^{m\times n}, \qquad \hat A_k := U \hat \Sigma_k V^*
	\]
	Dann gilt
	\[
		\min_{\substack{B\in \C^{m\times n} \\ \rg(B) \le k}} \|A-B\| = \|A- \hat A_k\| = \sigma_{k+1}
	\]
	Mit anderen Worten: $\hat A_k$ ist diejenige Matrix mit Rang $\le k$, die $A$ am besten approximiert.
	\begin{proof}
		Sei $V=(v_1,\dotsc, v_n)$, $k\in \{1,\dotsc,r-1\}$, $B\in \C^{m\times n}$, $\rg(B) =  k' \le k$.
		Dann gilt
		\[
			\dim(\ker B) = n - k'
		\]
		Dann ist $Z:= \ker(B) \cap \<v_1,\dotsc,v_{k'+1}\> \neq \{0\}$.
		Wähle $0\neq z \in Z$ mit $\|z\|=1$, dann lässt sich $z$ schreiben als
		\[
			z = \sum_{i=1}^{k'+1}a_i v_i
		\]
		Da $v_1,\dotsc,v_{k'+1}$ orthonormal ist folgt außerdem $\sum_{i=1}^{k'+1}|a_i|^2 = 1$.
		Setze $a:=(a_1,\dotsc, a_{k'+1},0,\dotsc,0)^T \in \C^n$.
		Dann gilt
		\begin{align*}
			\|A-B\|^2 &= \sup_{x\neq 0} \f {\|(A-B)x\|^2}{\|x\|^2}
			\ge \f {\|(Az-\overbrace{Bz}^{=0}\|^2}{\underbrace{\|z\|^2}_{=1}}
			= \|Az\|^2 
			= \l\|U\Sigma V^*\sum_{i=1}^n a_i v_i \r\|^2 \\
			&= \|U\Sigma V^* Va \|^2
			= \|U\Sigma a\|^2 
			\stackrel{\text{Rotationsinvarianz}}= \|\Sigma a\|^2  
			= \l\|\l(\begin{smallmatrix}\sigma_1 a_1 \\ \vdots \\ \sigma_{k'+1}a_{k'+1} \\ 0 \\ \vdots \\ 0\end{smallmatrix}\r)\r\| \\
			&= \sum_{i=1}^{k'+1} \sigma_i^2 \cdot |a_i|^2 
			\ge  \sum_{i=1}^{k'+1}\sigma_{k'+1}^2 |a_i|^2 
			= \sigma_{k'+1}^2 \sum_{i=1}^{k'+1} |a_i|^2 
			= \sigma_{k'+1}^2 
			\ge \sigma_{k+1}^2
		\end{align*}
		Andererseits wird dieser Fehler auch realisiert durch $\hat A_k$:
		\begin{align*}
			\|A-\hat A_k\| &= \|U\Sigma V^* - U\hat \Sigma_k V^*\| = \|U(\Sigma - \hat \Sigma_k) V^*\| \stackrel{\text{Rotationsinvarianz}}= \|\Sigma - \hat \Sigma_k\|\\
			&= \|\diag\{0,\dotsc, 0, \sigma_{k+1},\dotsc, \sigma_r\}\|
			= \sigma_{k+1}
		\end{align*}
	\end{proof}
\end{st}

\begin{df}[Pseudoinverse]
	\label{1.39}	
	Zu $A\in \C^{m\times n}$ mit SVD $A=U\Sigma V^*$ definieren wir die \emph{Pseudoinverse}
	\[
		A^+ := V \Sigma^+ U^* \in \C^{n\times m}
	\]
	mit
	\[
		\Sigma^+ := \diag\{\sigma_1^+,\dotsc, \sigma_r^+\} \in \R^{n\times m}
		,\qquad \sigma_i^+ := \begin{cases}
			\f 1{\sigma_i}	& \sigma_i \neq 0 \\
			0 & \sigma_i = 0
		\end{cases}
	\]
\end{df}

\begin{st}[Penrose-Bedingungen]
	\label{1.40}
	Eine Matrix $B\in \C^{n\times m}$ ist die Pseudoinverse von $A$ genau dann wenn gilt:
	\begin{enumerate}[i)]
		\item
			$ABA = A$
		\item
			$BAB = B$
		\item
			$(AB)^* = AB$
		\item
			$(BA)^* = BA$
	\end{enumerate}
	\begin{proof}
		\begin{seg}{$A^+$ erfülle die Bedingungen:} \\
			Sei $A=U\Sigma V^*$, $A^+ := V\Sigma^+U^*$.
			\begin{enumerate}[i)]
				\item
					Es gilt
					\[
						AA^+A = U\Sigma V^* V \Sigma^+ U^* U \Sigma V^* = U\Sigma V^* = A
					\]
					denn
					\[
						\Sigma \Sigma^+ \Sigma = \Sigma
					\]
					(betrachte die Diagonalkomponenten: $\sigma_i \sigma_i^+ \sigma_i = \sigma_i$)
				\item
					Analog zu i).
				\item
					Es gilt
					\begin{align*}
						AA^+ &= U \Sigma V^* V \Sigma^+ U^* = U \diag\{\underbrace{1,\dotsc,1}_{k\text{-mal}},0,\dotsc, 0\} U^*\\ &= \big(U \diag\{1,\dotsc,1,0,\dotsc,0\} U^*\big)^* = (AA^+)^*
					\end{align*}
				\item
					Analog zu iii).
			\end{enumerate}
		\end{seg}
		\begin{seg}{$B$ erfüllt die Bedingungen $\implies$ $B=A^+$} \\
			Zeige zunächst $BA=A^+A$
			\begin{align*}
				BA &\stackrel{\text{iv)}}= (BA)^* = A^*B^* = (AA^+A)^*B^* = A^*(A^+)^*A^*B^*\\ &= (A^+A)^*(BA)* \stackrel{\text{iii) + iv)}}= A^+ABA \stackrel{\text{i)}} = A^+A
			\end{align*}
			$AB=AA^+$ verläuft analog.
			Daraus folgt
			\[
				A^+ \stackrel{\text{ii)}}= A^+AA^+ = A^+AB = BAB \stackrel{\text{ii)}}= B 
			\]
		\end{seg}
	\end{proof}
\end{st}

\begin{st}[Eigenschaften]
	\label{1.41}
	\begin{enumerate}[i)]
		\item
			Ist $A$ quadratisch, dann gilt:
			\[
				\det A \neq 0 \implies A^+ = A^{-1}
			\]
		\item
			$(A^+)^+ = A$
		\item
			$(A^*)^+ = (A^+)^*$
		\item
			Im Allgemeinen gilt \emph{nicht} $(AB)^+ = B^+A^+$:
			\[
				(AB)^+ \neq B^+A^+
			\]
		\item
			$A^*(AA^*)^+ = A^+ = (A^*A)^+ A^*$
		\item
			Falls $A$ vollen Spaltenrang hat, dann ist
			\[
				A^+ = (A^*A)^{-1}A^*
			\]
		\item
			Falls $A$ vollen Zeilenrang hat, dann ist
			\[
				A^+ = A^*(AA^*)^{-1}
			\]
	\end{enumerate}
	\begin{proof}
		Übung
	\end{proof}
\end{st}

\begin{nt*}[Orthogonale Projektion]
	\begin{itemize}
		\item
			Wir nennen lineares $P: \C^m \to V$ \emph{orthogonale Projektion} auf den Unterraum $V\subset \C^m$, falls
			\[
				\<y- Py,z\> = 0 \qquad \forall z\in V, \forall y\in \C^m
			\]
			(„Orthogonalität des Projektionsfehlers“)
		\item
			Man kann zeigen (Übung):
			\begin{enumerate}[i)]
				\item
					$P$ ist idempotent:
					\[
						P = P^2
					\]
				\item
					$\Id - P$ ist eine orthogonale Projektion auf $V^\orth$.
				\item
					Pythogoras:
					\[
						\|y-z\|^2 = \|y-Py\|^2 + \|Py-z\|^2 
						\qquad \forall y\in \C^m,\forall z\in V
					\]
				\item
					$P_y$ ist eine Bestapproximation von $y$ auf $V$:
					\[
						\|y-Py\|^2 = \min_{z\in V}\|y-z\|^2
					\]
			\end{enumerate}
	\end{itemize}
\end{nt*}

\begin{lem}
	\label{1.42}
	Die Multiplikation mit
	\begin{enumerate}[i)]
		\item
			$AA^+$ ist eine orthogonale Projektion auf $\im(A)$.
		\item
			$I-AA^+$ ist eine orthogonale Projektion auf $\im (A)^\orth = \ker A^*$.
		\item
			$A^+A$ ist eine orthogonale Projektion auf $\im(A^*)$.
		\item
			$I-A^+A$ ist eine orthogonale Projektion auf $\im (A^*)^\orth = \ker A$.
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item
				Zeige $\<y-AA^+y, z\>$ für $z\in \im A$ und $y\in \C^m$.
				Sei also $z=Ax$ mit $x\in \C^n$, dann gilt
				\[
					\<y-AA^+y,Ax\> = \<A^*(y-AA^+y),x\> = \<A^*y - A^*AA^+y,x\> = 0
				\]
				da $A^*AA^+ = A^*(AA^+)^* = A^*(A^+)^* A^* = (AA^+A)^* = A^*$
			\item
				Nach obiger Bemerkung ii)
			\item
				Nach i) ist $A^*(A^*))^+$ orthogonale Projektion auf $\im (A^*)$ und
				\[
					A^*(A^*)^+ = A^*(A^+)^* = (A^+A)^* = A^+A
				\]
			\item
				Nach obiger Bemerkung ii)
		\end{enumerate}
	\end{proof}
\end{lem}

\begin{st}[Minimum-Norm-Least-Squares-Approximation]
	\label{1.43}
	Sei $A\in \R^{m\times n}, b\in \R^m$.
	Setze
	\[
		M := \{ \tilde x\in \R^n : \|A\tilde x-b\|^2 = \min_{x\in \R^n}\|Ax-b\|^2\}
	\]
	als Menge der \emph{Least-Squares-Approximation} zum linearen Gleichungssystem $Ax=b$.
	Dann ist $x^* := A^+b \in M$ und für alle $\tilde x\in M, \tilde x \neq x^*$ gilt
	\[
		\|x^*\| < \| \tilde x \|
	\]
	Also ist $x^*$ die eindeutige \emph{Minimum-Norm-Least-Squares-Approximation (MNLS)} zum LGS $Ax=b$.
	\begin{proof}
		Sei $P: \R^m \to \im(A)$ eine orthogonale Projektion auf dem Bild von $A$, siehe \ref{1.42}.
		Setze $y:= Pb$, dann ist
		\[
			\|y-b\| = \min_{z\in \im(A)}\|z-b\| = \min_{x\in \R^n}\|Ax-b\|
		\]
		Dann lässt sich $M$ darstellen als
		\[
			M := \{ \tilde x \in \R^n : A\tilde x = y\}
		\]
		also ein affin-lineare Unterraum von $\R^n$.
		Weiter ist $x^* \in M$, denn 
		\[
			Ax^* = AA^+ b = Pb = y
		\]
		und
		\[
			M = \{x^*\} + \ker A
		\]
		Nach \ref{1.42} ist $A^+A$ eine orthogonale Projektion auf $\im(A^*)$ und 
		\[
			A^+Ax^* = A^+AA^+ b = A^+b = x^*
		\]
		also ist $x^* \in \im (A^*) = \ker( A)^\orth$.

		Sei $\tilde x \in M$, $\tilde x \neq x^*$, dann ist
		\[
			\|\tilde x\|^2 = \|x^* + (\tilde x - x^*)\|^2 = \|x^*\| + 2\< x^*,\underbrace{\tilde x - x^*}_{\in \ker A}\> + \underbrace{\|\tilde x - x^*\|^2}_{>0} > \|x^*\|^2
		\]
	\end{proof}
\end{st}

\subsection{Anwendung: Ausgleichsrechnung}

Such $p\in \P_{n-1}$ mit $p(x_i)=f_i$, $i=1,\dotsc,m$.
Es ergibt sich das lineare Gleichungssystem $Aa = f$.
Definiere $a^* := A^+f$.

\begin{seg}[$m=n$ (Polynominterpolation)]
	Es gilt in diesem Fall
	\[
		\|Aa^*-f\| = 0, \qquad \ker A = \{0\}, \qquad M = \{a^*\}, \qquad a^* = A^{-1}f
	\]
\end{seg}
\begin{seg}[$m>n$]
	Es gilt
	\[
		\|Aa^*-f\| > 0, \qquad \ker A = \{0\}, \qquad M = \{a^*\}
	\]
	und wir erhalten eine eindeutige LS-Approximation.
\end{seg}
\begin{seg}[$m<n$]
	Es gilt
	\[
		\|Aa^*-f\| = 0 = \|A\_a-f\|, \qquad \ker A \supsetneq \{0\}, \qquad \_a \in M = a^* + \ker A \implies \|a^*\| \le \|\_a\|
	\]
	Wir erhalten eine mehrdeutige Least-Squares-Lösung, aber eine eindeutige Minimum-Norm-Least-Squares-Lösung.
\end{seg}
\begin{nt*}
	Falls $m\ge n$ und $A$ vollen Spaltenrang hat, nennt man
	\[
		A^TAx = A^Tb
	\]
	\emph{Gauß'sche Normalengleichungen}.
	Die MNLS-Approximation $x:=A^+b$ erfüllt genau diese Normalengleichungen.
	\begin{proof}
		Mit \ref{1.41} vi) ist $x=A^+b = (A^TA)^{-1}A^Tb$, also
		\[
			A^TAx = A^TA(A^TA)^{-1}A^T b = A^Tb
		\]
	\end{proof}
\end{nt*}



\chapter{Numerische Integration}	



Seien Stützstellen $\{x_i\}_{i=0}^n \subset [a,b]$ und Werte $\{f(x_i)\}_{i=0}^n$ einer Funktion $f:[a,b]\to \R$ gegeben.

Gesucht ist eine Approximation für das Integral
\[
	I(f) = \int_a^b f(x) \;dx
\]
Anwendungsgebiete sind beispielsweise:
\begin{itemize}
	\item
		Integration von Funktionen, deren Stammfunktionen nicht analytisch vorliegen, z.B.
		\[
			\int_a^b e^{-x^2} dx
		\]
	\item
		Integration von Funktionen, die nicht vollständig bekannt sind: Messungen oder Computer-Routinen.
	\item
		Numerisches Lösungen von gewöhnlichen oder partiellen Differentialgleichungen (Numerik 2 und Folgende)
\end{itemize}

\begin{df}[Quadratur]
	\label{2.1}
	Ein Funktional $I_n : C([a,b]) \to \R$ der Form
	\[
		I_n(f) = \sum_{j=0}^n \omega_j f(x_j)
	\]
	heißt \emph{Quadraturformel} mit Stützstellen $\{x_j\}_{j=0}^n$ und Gewichten $\{\omega_j\}_{j=0}^n$.
\end{df}

\begin{ex*}
	\begin{seg}[Mittelpunktsregel ($n=0$)]
		\[
			x_0 := \f {a+b}2, \qquad \omega_0 := b-a
		\]				
	\end{seg}
	\begin{seg}[Trapezregel ($n=1$)]
		\[
			x_0 := a, x_1 := b \qquad \omega_0 := \omega_1 := \f {b-a}2
		\]				
	\end{seg}
	\begin{seg}[Simpsonregel ($n=2$)]
		\[
			I_2(f) = \f {b-a}6 \l( f(a) + 4f\l(\f {a+b}2\r) + f(b) \r)
		\]
	\end{seg}
\end{ex*}

\begin{df}[Exaktheit]
	\label{2.2}
	Eine Quadratur heiß \emph{exakt} auf $\P_k$, falls
	\[
		I_n(p) = I(p) \qquad \forall p\in \P_k
	\]
\end{df}

\begin{nt*}
	\begin{itemize}
		\item
			Eine Quadratur ist linear, d.h. für alle $f,g$ und $\alpha,\beta \in \R$, gilt
			\[
				I_n ( \alpha f + \beta g) = \alpha I_n(f) + \beta I_n(g)
			\]
		\item
			Zum Nachweis der Exaktheit, reicht es
			\[
				I_n(x^k) = I(x^k) \qquad k\in \N
			\]
			nachzuweisen.
		\item
			\begin{itemize}
				\item
					Die Mittelpunktsregel ist exakt auf $\P_1$.
				\item
					Die Trapezregel ist exakt auf $\P_1$.
				\item
					Die Simpsonregel ist exakt auf $\P_3$.
				\item
					$I_n$ ist exakt auf $\P_0$ genau dann, wenn
					\[
						\sum_{i=0}^n \omega_i = b-a
					\]
			\end{itemize}
	\end{itemize}
\end{nt*}


\section{Interpolatorische Quadraturen}

Integriere das Interpolationspolynom $p$ zu den Daten $(x_i,f(x_i))$ für $i=0,\dotsc,n$.

Mit Hilfe der Lagrange-Darstellung von $p$:
\[
	p(x) = \sum_{j=0}^n f(x_j) \cdot L_j^n (x) \qquad 
	\text{mit} 
	\qquad L_j^n(x) := \prod_{\substack{k=0\\k\neq j}}^n \f{x-x_k}{x_j-x_k}
\]
Wir integrieren:
\[
	\int_a^b p(x) dx = \sum_{j=0}^n f(x_j) \cdot \underbrace{\int_a^b L_j^n (x) dx}_{=:\omega_j} =: I_n(f)
\]
Also $I_n(f) = I(p)$.

\begin{df}[Interpolatorische Quadratur]
	\label{2.3}
	Eine Quadratur $I_n(f) = \sum_{j=0}^n \omega_j f(x_j)$ ist eine \emph{Interpolatorische Quadratur}, falls die Gewichte folgendermaßen durch die Lagrange-Polynome gegeben sind:
	\[
		\omega_j := \int_a^b L_j^n \;dx
		\qquad \text{mit} 
		\qquad L_j^n(x) := \prod_{\substack{k=0\\k\neq j}}^n \f{x-x_k}{x_j-x_k}
	\]
\end{df}

\begin{ex*}
	Sei $[a,b] = [0,1]$, $n=2$, $x_0=0, x_1=\f 12, x_2 = 1$.
	Dann ist
	\begin{align*}
		\omega_0 &= \int_0^1 \f {(x-\f 12) (x-1)}{(0-\f 12)(0-1)} \; dx = \int_0^1 2x^2 - 3x+1 \; dx = \f 23 - \f 32 + 1 = \f 16 \\
		\omega_1 &= \int_0^1 \f {(x-0)(x-1)}{(\f 12 - 0)(\f 12 -1)} \; dx = -4 \int_0^1x^2 - x \; dx = -4 \l( \f 13 - \f 12 \r) = \f 23 \\
		\omega_2 &= \int_0^1 \f {(x-0)(x-\f 12)}{(1-0)(1-\f 12)} \; dx = \int_0^1 2x^2 -x \; dx = \f 23 - \f 12 = \f 16
	\end{align*}
	Damit lautet die Quadraturformel
	\[
		I_2(f) = \f 16 \l( f(0) + 4f\l(\f 12\r) + f(1)\r)
	\]
\end{ex*}

\begin{st}[Exaktheit von Interpolatorischen Quadraturen]
	\label{2.4}
	Eine Interpolatorische Quadratur $I_n$ ist exakt auf $\P_n$.
	\begin{proof}
		Sei $p\in \P_n$, dann ist
		\begin{align*}
			I(p) = \int_a^b p(x) dx &= \int_a^b \sum_{j=0}^n p(x_j) L_j^n(x) dx \\
			&= \sum_{j=0}^n p(x_j) \int_a^b L_j^n(x) dx = I_n(p)
		\end{align*}
	\end{proof}
\end{st}

\begin{st}
	\label{2.5}
	Zu Stützstellen $\{x_j\}_{j=0}^n \subset [a,b]$ paarweise verschieden, existiert genau eine Quadratur, welche exakt auf $\P_n$ ist.
	\begin{proof}
		Existenz ist nach \ref{2.4} klar: die Interpolatorische Quadratur tut's.
		Zeige jetzt die Eindeutigkeit:

		Sei $I_n(f) = \sum_{j=0}^n \omega_j f(x_j)$ exakt auf $\P_n$.
		Für Lagrange-Polynome gilt $L_j^n \in \P_n$, also
		\[
			\int_{a}^b \prod_{\substack{k=0\\k\neq j}}^n \f {x-x_k}{x_j-x_k} \; dx \\\
			= \int_a^b L_j^n (x) dx = I(L_j^n) = I_n(L_j^n) = \sum_{i=0}^n \omega_i L_j^n(x_i) = \omega_j
		\]
	\end{proof}
	also ist $I_n$ schon die interpolatorische Quadratur.
\end{st}

\begin{st}
	\label{2.6}
	Sei $I_n(f) = \sum_{j=0}^n \omega_j f(x_j)$ eine Interpolatorische Quadratur mit
	\[
		x_j - a = b - x_{n-j} \qquad j=0,\dotsc,n
	\]
	Dann gilt
	\begin{enumerate}[i)]
		\item
			$\omega_j = \omega_{n-j}$ , d.h. die Interpolatorische Quadratur ist symmetrisch.
		\item
			falls $n$ gerade, so ist $I_n$ exakt auf $\P_{n+1}$.
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item
				Setze $\_{I_n} := \sum_{j=0}^n \omega_{n-j}f(x_j)$.
				Es reicht zu zeigen, dass $\_{I_n}$ exakt ist auf $\P_n$, denn wegen der Eindeutigkeit in \ref{2.5} gilt $\_{I_n}=I_n$ also $\omega_j=\omega_{n-j}$.
				Sei $p\in \P_n$ und setze $\_p(x) := p(a+b-x)$.
				Dann ist $p(x) \in \P_n$ und $I(p) = \_I(\_p)$ wegen „Spiegelung‘.

				Damit folgt
				\begin{align*}
					\_I_n (p) &= \sum_{j=0}^n \omega_{n-j}p(x_j) \\
					&=\sum_{j=0}^n \omega_{n-j} \_p(\underbrace{a+b-x_j}_{=x_{n-j}}) \\
					&=\sum_{j=0}^n \omega_{n-j}\_p(x_{n-j})
					\stackrel{i=n-j} = \sum_{i=0}^n \omega_i \_p(x_i)
					= I_n (\_p) = I(\_p) = I(p)
				\end{align*}
			\item
				Sei $n=2m$ ,d.h. $x_m = \f {a+b}2$ wegen der Symmetrie.
				Wir überlegen uns zunächst
				\begin{align*}
					N(x) &:= \prod_{l=0}^n (x-x_l) \in \P_{n+1} \\
					&= \l( \prod_{l=0}^{m-1}(x-x_l)\r) \cdot \l(x-\f {a+b}2\r) \prod_{l=0}^{m-1}(x-(a+b-x_l))
				\end{align*}
				wegen $((a+b-x)-x_l) = -(x-(a+b-x_l))$ folgt
				\[
					N(a+b-x) = (-1)^{n+1}N(x) = - N(x)
				\]
				Also eine „Punktsymmetrie“ um $x = \f {a+b}2$.
				Damit ist
				\[
					I(N) = 0
				\]
				Zeige nun die Exaktheit von $I_n$ auf $\P_{n+1}$.

				Sei $p\in \P_{n+1}$, dann existiert $q\in \P_n, c\in \R$ mit $p=q+c\cdot N$.
				Sei $p_n$ Interpolationspolynom zu Daten $(x_i,p(x_i))$, $i=0,\dotsc,n$.
				Es gilt
				\[
					p(x_l) = q(x_l) + c\cdot \underbrace{N(x_l)}_{=0} = q(x_l) \qquad l=0,\dotsc,N
				\]
				Also ist $q=p_1$ wegen der Eindeutigkeit der Interpolation.
				\[
					I(p) = I(q+cN) = I(q) + c\underbrace{I(N)}_{=0} = I(q) = I(p_n) = I_n(p)
				\]
		\end{enumerate}
	\end{proof}
\end{st}

\begin{st}[Fehlerabschätzung für Interpolatorische Quadraturen]
	\label{2.7}
	Sei $I_n$ eine interpolatorische Quadratur.
	Dann gelten für das Fehlerfunktional
	\[
		R_n = I(f) - I_n(f)
	\]
	folgende Schranken:
	\begin{enumerate}[i)]
		\item
			$\displaystyle |R_n(f)| \le \f {(b-a)^{n+2}}{(n+1)!}\|f^{(n+1)}\|_\infty \qquad f\in C^{n+1}([a,b])$
		\item
			falls $I_n$ symmetrisch und $n$ gerade, dann gilt
			\[
				|R_n(f)| \le \f {(b-a)^{n+3}}{(n+2)!} \|f^{(n+2)}\|_\infty \qquad f\in C^{n+2}([a,b])
			\]
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item
				Sei $p_n\in \P_n$ ein Interpolationspolynom zu den Daten $\{(x_i,f(x_i)\}_{i=0}^n$.
				Dann ist
				\begin{align*}
					|R_n(f)| 
					&= |I(f) - I_n(f)| \\
					&= |I(f) - I(p_n)| \\
					&= \l|\int_a^b f(x) - p_n(x) \r| \\
					&\le \int_a^b |f(x)-p_n(x)| \\
					&\stackrel{\ref{1.10}}= \int_a^b \l| \f 1{(n+1)!}f^{(n+1)}(\xi(x)) \prod_{i=0}^n (x-x_i)\r| \\
					&\le \f {\|f^{(n+1)}\|_\infty}{(n+1)!} \int_a^b (b-a)^{n+1} \\
					&= \f {(b-a)^{n+2}}{(n+1)!}\|f^{(n+1)}\|_\infty
				\end{align*}
			\item
				Sei $q_{n+1}\in \P_{n+1}$ eine Hermite-Interpolation zu den Daten $\{(x_i,f(x_i)\}_{i=0}^n$ und $(x_l,f'(x_l))$ beliebig.
				Sei $p_n\in \P_n$ ein Standard-Interpolationspolynom zu den Daten $\{(x_i,f(x_i)\}_{i=0}^n$.

				Wie im Beweis von \ref{2.6}	ist für geeignete $q_n\in \P_n, c\in \R$ 
				\[
					q_{n+1} = q_n + c\cdot N(x)
				\]
				wegen $f(x_i) = q_{n+1}(x_i) = q_n(x_i) + c\cdot N(x_i) = q_n(x_i)$ ist $q_n = p_n$ das Interpolationspolynom, also
				\begin{align*}
					I_n(f)
					= I(p_n)
					= I(q_n)
					= I(q_n) + c\cdot I(N)
					= I(q_{n+1}).
				\end{align*}
				Damit gilt
				\[
					|R_n(f)| 
					= |I(f) - I_n(f)| 
					= |I(f) - I(q_{n+1})|
					= |I(f-q_{n+1})|
					= \l| \int_a^b f(x) - q_{n+1}(x) \r|
				\]
				Die Behauptung folgt wie in i) mittels der Hermite-Interpolationsdarstellung aus Satz \ref{1.17}.
		\end{enumerate}
	\end{proof}
	\begin{note}
		Die Abschätung $\displaystyle \l| \prod_{i=0}^n (x-x_i) \r| \le (b-a)^{n+1}$ ist sehr grob und lässt sich für konkrete Stützstellenwahl leicht wesentlich verbessern.
	\end{note}
\end{st}

\begin{st}[Koordinatentransformation]
	\label{2.8}
	Sei $\hat I_n(\hat f) = \sum_{j=0}^n \hat \omega_j \hat f(\hat x_j)$ eine interpolatorische Quadratur auf $[\hat a, \hat b]$.
	\begin{enumerate}[i)]
		\item
			Dann ist
			\[
				I_n(f) := \sum_{j=0}^n \omega_j f(x_j), \qquad \omega_j = \f{b-a}{\hat b - \hat a}\hat \omega_j, \qquad x_j =a + \f{b-a}{\hat b - \hat a}(\hat x_j - \hat a)
			\]
			eine Interpolatorische Quadratur auf $[a,b]$.
		\item
			Gilt für ein geignetes $K\in \R$ und $m \in \N$
			\[
				|\hat I(\hat f) - \hat I_n(\hat f) | \le K \cdot \|\hat f^{(m)}\|_\infty \cdot (\hat b - \hat a)^{m+1}
			\]
			Dann gilt für $I_n$ die Abschätzung
			\[
				|I(f) - I_n(f)| \le K \cdot \|f^{(m)}\|_\infty \cdot (b-a)^{m+1}
			\]
	\end{enumerate}
	\begin{note}
		Also reicht es, interpolatorische Quadraturen auf einfachen Intervallen zu konstruieren.
		Diese sind nach Skalierung auf jedes beliebige Intervall anwendbar.
	\end{note}
	\begin{proof}
		\begin{enumerate}[i)]
			\item
				Zeige, dass $I_n(f)$ exakt auf $\P_n$ ist.
				Sei $p\in \P_n$, setze $\hat p(\hat x) := p(a- \f {b-a}{\hat b- \hat a}(\hat x- \hat a)) \in \P_n$.
				\begin{align*}
					I_n(p) 
					&= \sum_{j=0}^{n} \f {b-a}{\hat b - \hat a} \hat \omega_j p\l(a + \f {b-a}{\hat b - \hat a}(\hat x_j - \hat a)\r) \\
					&=\f {b-a}{\hat b - \hat a}\sum_{j=0}^n \hat \omega_j \hat p(\hat x_j) \\
					&= \f {b-a}{\hat b - \hat a}\hat I_n(\hat p) \\
					&= \f {b-a}{\hat b - \hat a}\int_{\hat a}^{\hat b}\hat p(\hat x) d\hat x \\
					&\stackrel{x:= a+ \f{b-a}{\hat b - \hat a}(\hat x - \hat a)} = \f {b-a}{\hat b - \hat a} \int_a^b p(x) \f {\hat b - \hat a}{b-a} dx \\
					&= I(p)
				\end{align*}
			\item
				Sei $f\in C^m([a,b]), \hat f(\hat x) := f(a + \f {b-a}{\hat b - \hat a}(\hat x - \hat a) \in C^m([\hat a,\hat b])$
				Dann ist
				\begin{align*}
					\hat f'(\hat x) &= \f {b-a}{\hat b - \hat a}f'(x) \\
					\vdots \quad &= \quad \vdots \\
					\hat f^{(m)}(\hat x) &= \l( \f {b-a}{\hat b-\hat a} \r)^m f^{(m)}(x)
				\end{align*}
				Außerdem
				\begin{align*}
					|I(f) - I_n(f)| 
					&= \l| \int_a^b f(x) dx - \sum_{j=0}^n \omega_j f(x_j) \r| \\
					&= \l| \f {b-a}{\hat b- \hat a} \int_{\hat a}^{\hat b} \hat f(\hat x) d\hat x - \sum_{j=0}^n \f {b-a}{\hat b - \hat a} \hat \omega_j \hat f(\hat x_j) \r| \\
					&= \l| \f {b-a}{\hat b - \hat a} (\hat I(\hat f) - \hat I_n(\hat f)) \r| \\
					&\stackrel{\text{Ann}}\le \f {b-a}{\hat b - \hat a}K\cdot \|\hat f^{(m)}\|_\infty (\hat b - \hat a)^{m+1} \\
					&\stackrel{\text{nach obigem}}\le \f {b-a}{\hat b - \hat a} K \cdot \f {(b-a)^m}{(\hat b - \hat a)^m} \|f^{(m)}\|_\infty \cdot (\hat b - \hat a)^{m+1} \\
					&= (b-a)^{m+1}\|f^{(m)}\|_\infty \cdot K
				\end{align*}
		\end{enumerate}
	\end{proof}
\end{st}


\section{Die Newton-Cotes Formeln}


\begin{df}
	\label{2.9}
	\emph{Newton-Cotes-Quadraturen} (N.-C.) sind Interpolatorische Quadraturen zu äquidistanten Stützstellen.
	Wir unterscheiden
	\begin{itemize}
		\item
			geschlosse Newton-Cotes-Formeln:
			\[
				h:= \f {b-a}n, \qquad x_k := a + kh, \quad (k=0,\dotsc,n)
			\]
		\item
			offene Newton-Cotes-Formeln:
			\[
				h:= \f {b-a}{n+2}, \qquad x_k = a+ (k+1)h, \quad (k=0,\dotsc,n)
			\]
			(d.h. die Randpunkte sind keine Stützstellen)
	\end{itemize}
\end{df}

\begin{table}[H]
	\centering
	\caption{Koeffiziententabelle geschlossener Newton-Cotes Formeln auf $[0,1]$}
	\begin{tabular}{l|cl}
		n & $(\omega_0, \dotsc, \omega_n)$ \\ \hline
		1 & $(\f 12, \f 12)$ & „Trapezregel“ \\ 
		2 & $\f 16 (1,4,1)$ & „Simpsonregel“, Kepplersche Fassregel \\
		3 & $\f 18 (1,3,3,1)$ & „$\f 38$-Regel“ \\
		4 & $\f 1{90} (7,32,12,32,7)$ & „Milne-Regel“ \\
		5 & $\f 1{288} (19,75,50,50,75,19)$ & \\
		6 & $\f 1{840} (41,216,27,272,27,216,41)$ &
	\end{tabular}
\end{table}

\begin{nt*}
	\begin{itemize}
		\item
			Nach \ref{2.6} i) sind die $\{\omega_i\}$ symmetrisch, da die Stützstellen $\{x_i\}$ symmetrisch sind.
		\item
			Falls $[a,b] \neq [0,1]$ erhält man die Gewichte durch die Skalierung aus \ref{2.8}, z.B.
			\[
				I_3(f) = \f {b-a}8 \l( f(a) + 3 f\l(a+\f {b-a}3\r) + 3 f\l(a+\f {b-a}3 2\r) + f(b) \r)
			\]
		\item
			Falls $a,b\in \Q$, dann auch $\omega_i \in \Q$.
	\end{itemize}
\end{nt*}

\begin{table}[H]
	\centering
	\caption{Koeffiziententabelle offener Newton-Cotes Formeln auf $[0,1]$}
	\begin{tabular}{l|cl}
		n & $(\omega_0, \dotsc, \omega_n)$ \\ \hline
		0 & 1 & „Mittelpunktsregel“ \\ 
		1 & $(\f 12, \f 12)$ & \\
		2 & $(\f 23, -\f 13, \f 23)$ &  \\
		3 & $\f 1{24} (11,1,1,11)$ &
	\end{tabular}
\end{table}

\begin{nt*}
	\begin{itemize}
		\item
			Ab $n=2$ treten negative Gewichte auf (bei geschlossenen Newton-Cotes Formeln ab $n=7$), was zu numerischen Auslöschungseffekten führen kann.
		\item
			Die Gewichte
			\[
				\omega_j := \int_0^1 L_j^n(x) dx
			\]
			lassen sich kompakt als Lösung eines LGS mit Hilfe einer transponierten Vandermondematrix darstellen (siehe Satz \ref{2.10}).
	\end{itemize}
\end{nt*}

\begin{st}
	\label{2.10}
	Für das Intervall $[0,1]$ sind die Gewichte $\{\omega_j\}_{j=0}^n$ der geschlossenen Newton-Cotes-Formeln die Lösung des linearen Gleichungsystems
	\[
		\begin{pmatrix}
			1 & 1 & 1 & \dotsc & 1 \\
			0 & \f 1n & \f 2n & \dotsc & \f nn \\
			0 & (\f 1n)^2 & (\f 2n)^2 & \dotsc & (\f nn)^2 \\
			\vdots & \vdots & \vdots & \ddots & \vdots \\
			0 & (\f 1n)^n & (\f 2n)^n & \dotsc & (\f nn)^n \\
		\end{pmatrix}
		\begin{pmatrix}
			\omega_0 \\
			\vdots \\
			\omega_n
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 \\ \f 12 \\ \vdots \\ \f 1{n+1}
		\end{pmatrix}
	\]
	\begin{proof}
		$I_n$ ist eine Interpolatorische Qudratur, also exakt auf $\P_n$.
		Wähle $p(x) = x^i$, dann gilt
		\[
			\sum_{j=0}^n \omega_j \l( \f jn \r)^i = I_n(p) = I(p) = I(x^i) = \int_0^1 x^i dx = \f 1{i+1}
		\]
		Dies ist gerade die $i$-te Zeile im System.
	\end{proof}
\end{st}

\begin{kor}[Exaktheit für Newton-Cotes Quadraturen]
	\label{2.11}
	Eine Newton-Cotes Quadratur der Ordnung $n\in \N$ ist exakt auf $\P_n$.
	Falls $n$ gerade, dann sogar auf $\P_{n+1}$.
	\begin{proof}
		Die Newton-Cotes Formeln sind symmetrisch und Interpolationsquadraturen.
		Die Behauptung folgt damit aus \ref{2.4} und \ref{2.6} ii).
	\end{proof}
\end{kor}

\begin{st}
	\label{2.12}
	\begin{enumerate}[i)]
		\item
			Für die Trapezregel ($n=1$) gilt
			\[
				\big|I(f) - I_n(f) \big| \le \f {(b-a)^3}{12} \|f''\|_\infty
			\]
		\item
			Für die Simpsonregel ($n=2$) gilt
			\[
				\big| I(f) - I_n(f) \big| \le \f{(b-a)^5}{2880}\|f^{(4)}\|_\infty
			\]
	\end{enumerate}
	\begin{note}
		Aus \ref{2.7} folgt lediglich
		\[
			|I(f) - I_1(f)| \le \f {(b-a)^3}2 \|f''\|_\infty
		\]
		und
		\[
			|I(f) - I_2(f)| \le \f {(b-a)^5}{24} \|f^{(4)}\|_\infty
		\]
		Der Satz \ref{2.12} ist also eine Verbesserung um Faktor 6 in i) und 120 in ii) wegen der speziellen Wahl der Stützstellen.
	\end{note}
	\begin{proof}
		In Beweis von \ref{2.7} wurde verwendet.
		\[
			\int_a^b \l| \prod_{i=0}^n (x-x_i)\r| dx \le (b-a)^{n+1}
		\]
		\begin{enumerate}[i)]
			\item
				Dies wird um Faktor 6 verbessert durch:
				\[
					\int_a^b |x-a||x-b| = \int_a^b (x-a)(b-x) dx = \dotsb = \f {(b-a)^3}6
				\]
			\item
				analog
		\end{enumerate}
	\end{proof}
\end{st}

\begin{nt*}
	\begin{itemize}
		\item
			Ein Erhöhen von $n$ führt im Allgemeinen nicht zur Konvergenz, d.h. es existieren $f$, so dass
			\[
				\lim_{n\to \infty}I_n(f) \neq I(f)
			\]
		\item
			Für „schöne“ Funktionen kann man Konvergenz zeigen, z.B. bei Polynomen oder bei Funktionen gemäß Satz \ref{1.12}.
			(Konvergenz genau dann, wenn die Polynominterpolation gleichmäßig konvergiert)
		\item
			Gegenbeispiel für Konvergenz: 	
\begin{table}[H]
	\centering
	\caption{$[a,b]=[-1,1], f(x) = \sqrt{|x|}$.}
	\begin{tabular}{l|r}
		n & $I_n(f)$ \\ \hline
		1 & 0.6667 \\
		2 & 0.6667 \\
		3 & 0.0327 \\
		4 & 0.0166 \\
		5 & 0.0480 \\
		6 & 0.2240 
	\end{tabular}
\end{table}
		\item
			Eine Idee zur Verbesserung wäre das Zerlegen des Intervalls: „zusammengesetzten Quadraturen“
	\end{itemize}
\end{nt*}


\begin{df}[Zusammengesetzte Quadratur]
	\label{2.13}
	Sei $\hat n \in \N$, $\hat I_{\hat n}(\hat f) = \sum_{k=0}^{\hat n}\hat \omega_k \hat f(\hat x_k)$ eine Interpolations-Quadratur auf $[0,1]$.

	Sei $[a,b]\subset \R$, $N\in \N$, $H:= \f {b-a}N$, dann ist
	\[
		I_h (f) := H \cdot \sum_{l=1}^N \sum_{k=0}^{\hat n}\hat \omega_k f\Big(a+(l-1)H + \hat x_k H\Big)
	\]
	eine \emph{zusammengesetzte Quadratur}.
\end{df}

\begin{nt*}
	\begin{itemize}
		\item
			$I_h$ ist im Allgemeinen nicht exakt auf $\P_n$ mit $n=(\hat n + 1) N$, sondern nur auf $\P_{\hat n}$.
		\item
			Die zusammengesetzte Mittelpunktsregel ($\hat n = 0, \hat \omega_0 = 1, \hat x_0 = \f 12$) ergibt eine Riemann-Summe:
			\[
				I_h(f) := H \cdot \sum_{l=1}^N f (x_l) \qquad x_l = a + \l(l- \f 12\r) H
			\]
		\item
			Die zusammengesetzte Trapezregel ergibt ($\hat n = 1, \hat \omega_0 = \hat \omega_1 = \f 12, \hat x_0 = 0, \hat x_1 = 1, h := H = \f {b-a}N$):
			\[
				I_h(f) = \f h2 \l(f(a) + 2 \sum_{l=1}^{N-1}f(x_l) + f(b)\r) \qquad x_l := a+ lh \qquad l=0,\dotsc,N
			\]
		\item
			Zusammengesetzte Simpsonregel mit
			\[
				\hat n = 2, \quad h := \f H2, \qquad x_k := a+kh, \quad k=0,\dotsc, 2N
			\]
			ergibt:
			\[
				I_h(f) = \f h3 \l( f(a) + 2 \sum_{l=1}^{N-1}f(x_{2l}) + 4 \sum_{l=1}^{N}f(x_{2l-1}) + f(b)\r)
			\]
		\item
			Verbesserung der Quadratur durch Erhöhen der Intervall-Anzahl, z.B. $f(x)= \sqrt{|x|}$, $[a,b]=[-1,1]$ mit der zusammengesetzten Simpson-Regel.
			\begin{table}[H]
				\centering
				\caption{Es ergibt sich eine konvergente Teilfolge für gerade $N$}
				\begin{tabular}{c|c}
					N & $|I(f)-I_h(f)|$ \\ \hline
					1 & 0.6667 \\
					2 & 0.0572 \\
					3 & 0.1287 \\
					4 & 0.0203 \\
					5 & 0.0598
				\end{tabular}
			\end{table}
	\end{itemize}
\end{nt*}

\begin{st}[Fehlerschranke für zusammengesetzte Quadraturen]
	\label{2.14}
	Sei $\hat I_{\hat n}$ auf $[0,1]$ derart, dass für $m\in \N$, $K\in \R$
	\[
		|\hat I(\hat f) - \hat I_{\hat n}(\hat f)| \le K \cdot \|\hat f^{(m)}\|_\infty
		\qquad \forall \hat f \in C^m([0,1])		
	\]
	Dann erfüllt die zusammengesetzte Quadratur $I_h$ die Fehlerschranke
	\[
		|I(f) - I_{h}(f)| \le K \cdot \| f^{(m)}\|_\infty (b-a) H^m
		\qquad \forall f \in C^m([a,b])	
	\]
	\begin{proof}
		Transformiere $\hat I_{\hat n}$ auf $[a+(l-1)H, a+lH]$, $l=1,\dotsc,N$
		\[
			I_{\hat n}^{(l)}(f) := \sum_{k=0}^{\hat n} \hat \omega_k H f(a+(l-1)H + \hat x_k H)
		\]
		Gemäß \ref{2.8} ii) gilt
		\[
			\l| \int_{a+(l-1)H}^{a+lH} f(x) dx - I_{\hat n}^{(l)}\r| \le K \sup_{x\in [a+(l-1)H,a+lH]} |f^{(m)}(x)| H^{m+1}
		\]
		Für Gesamtfehler also
		\begin{align*}
			|I(f) - I_h(f)| 
			&= \l| \sum_{l=1}^N \int_{a+(l-1)H}^{a+lH} f(x) dx - I_{\hat n}^{(l)}(f)\r| \\
			&\le \sum_{l=1}^N K \cdot \|f^{(m)}\|_\infty H^{m+1} \\
			&= \underbrace{N H}_{=b-a} K \|f^{(m)}\|_\infty H^m
		\end{align*}
	\end{proof}
\end{st}

\begin{kor}
	\label{2.15}
	\begin{enumerate}[i)]
		\item
			Für die zusammengesetzte Trapezregel lautet die Fehlerabschätzung
			\[
				|I(f) - I_h(f)| \le \f {\|f''\|_{\infty}}{12} (b-a) h^2 \qquad \forall f \in C^2 ([a,b])
			\]
		\item
			Für die zusammengesetzte Simpsonregel
			\[
				|I(f) - I_h(f)| \le \f {\|f^{(4)}\|_\infty}{180} (b-a) h^4 \qquad \forall f \in  C^4([a,b])
			\]
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item
				Satz \ref{2.14} mit $m=2$, $K=\f 1{12}$ (Satz \ref{2.12} i) ).
			\item
				Satz \ref{2.14} mit $m=4$, $K=\f 1{2880}$, $h=\f H2$ (Satz \ref{2.12} i) ).
		\end{enumerate}
	\end{proof}
\end{kor}

\begin{kor}[Konvergenz]
	\label{2.16}
	Für zusammengesetzte Quadraturen und $f$ gemäß \ref{2.15} gilt
	\[
		\lim_{h \to 0} I_h(f) = I(f)
	\]
\end{kor}

\section{Gauß-Quadraturen}

\begin{itemize}
	\item
		Satz \ref{2.5} besagt, dass zu vorgegebene Stützstellen $\{x_k\}_{k=0}^n$ eine Interpolations-Quadratur existiert, welche exakt auf $\P_n$ ist.
	\item
		Falls $\{x_i\}$ symmetrisch im Intervall und $n$ gerade, dann ist die Interpolations-Quadratur nach \ref{2.6} sogar exakt auf $\P_{n+1}$.
	\item
		Kann durch geeignete Wahl von Stützstellen der Grad der Exaktheit weiter gesteigert werden? Bis zu welchem Grad?
\end{itemize}

\begin{df}[Orthogonale Polynome]
	\label{2.17}	
	Sei $[a,b] = [-1,1]$ und $\<\cdot, \cdot\>$ das $L^2$-Skalarprodukt auf $C([a,b])$, d.h.
	\[
		\<f,g\> = \int_a^b f(x)g(x) dx
	\]
	Wir definieren rekursiv
	\begin{align*}
		p_0(x) &:= 1 \\
		p_{n+1}(x) &:= x^{n+1}- \sum_{i=0}^n \f {\<x^{n+1},p_i\>}{\<p_i,p_i\>} p_i(x)
	\end{align*}
	\begin{note}
		\begin{itemize}
			\item
				Ein Vektorraum $V$ mit Skalarprodukt wird ein normierter Raum mit $\|f\| := \sqrt{\<f,f\>}$.
				Falls $V$ vollständig bezüglich dieser Norm, nennt man $V$ \emph{Hilbertraum}, sonst \emph{Prä-Hilbertraum}.

				$(C([a,b]),\<\cdot,\cdot\>)$ ist nicht vollständig bezüglich der $L^2$-Norm, daher ein Prä-Hilbertraum.
			\item
				Die orthonormalen Polynome aus \ref{2.17} werden durch das Gram-Schmidt'sche Orthonormalisierungsverfahren konstruiert, angewandt auf Monome $\{x^i\}_{i\in \N_0}$.

		\end{itemize}
	\end{note}
\end{df}

\begin{ex*}
	\begin{align*}
		p_0(x) &= 1 \\
		p_1(x) &= x - \f {\<x,p_0\>}{\<p_0,p\>}p_0(x) = x - \f 02 1 = x \\
		p_2(x) &= x^2 - \f {\<x^2,1\>}{\<1,1\>} - \f{\<x^2,x\>}{\<x,x\>}x = x^2 - \f {\f 23}2 - \f 0{\f 23} = x^2 - \f 13
	\end{align*}
\end{ex*}

\begin{lem}
	\label{2.18}
	\begin{enumerate}[i)]
		\item
			$\{p_0,\dotsc,p_n\}$ sind wohldefiniert und bilden eine Basis für $\P_n$.
		\item
			Orthogonalität: $p_n \orth \P_{n-1}$, d.h. $\<p_n,q\> = 0$ für alle $q\in \P_{n-1}$.
		\item
			$p_n$ hat $n$ \emph{verschiedene, reelle} Nullstellen $x_1,\dotsc, x_n \in (-1,1)$.
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item
				Zeige per Induktion nach $n$.
				Für $n=0$ ergibt sich $p_0(x)=1$ und $\<p_0\> = \P_0$.

				Seien $\{p_0,\dotsc,p_n\}$ wohldefiniert und Basis für $\P_n$.
				Dann ist $p_i\neq 0$, für $i=0,\dotsc,n$, denn sonst wäre $\{p_0,\dotsc,p_n\}$ linear abhängig.
				Also sind alle $\<p_i,p_i\> \neq 0$ und damit $p_{n+1}$ wohldefiniert.

				Es ist $V_{n+1} := \<p_0,\dotsc,p_{n+1}\> \in \P_{n+1}$ nach Konstruktion.
				Wegen
				\[
					x^{n+1} = p_{n+1} + \sum_{i=0}^n \f {\<x^{n+1},p_i}{\<p_i,p_i\>} p_i(x)
				\]
				ist $x^{n+1} \in V_{n+1}$.
				Nach Induktionsvoraussetzung ist auch $\P_n \subset V_{n+1}$ also $\P_{n+1}\subset V_{n+1}$ also $\P_{n+1} = V_{n+1}$.
				Also ist $\{p_i\}_{i=0}^{n+1}$ Erzeugenden-System für $\P_{n+1}$ aus $n+2$ Elementen.
				Wegen $\dim \P_{n+1} = n+2$ ist $\{p_i\}_{i=0}^{n+1}$ linear unabhängig und damit eine Basis
			\item
				Nach i) ist $\P_{n+1} = \<p_0,\dotsc, p_{n-1}\>$, also reicht es zu zeigen, dass $\<p_n,p_i\>=0$ für $i=0,\dotsc, n-1$.
				Zeige per Induktion.
				Für $n=1$ ist $\<p_1,p_0\> = \int_{-1}^1 x = 0$.
				Sei $j\in \{0,\dotsc,n\}$.
				\begin{align*}
					\<p_{n+1},p_j\> 
					&= \l\< x^{n+1}- \sum_{i=0}^n \f {\<x^{n+1},p_i\>}{\<p_i,p_i\>} p_i(x) , p_j\r\> \\
					&= \<x^{n+1},p_j\> - \sum_{i=0}^n \f{\<x^{n+1},p_i\>}{\<p_i,p_i\>} \underbrace{\<p_i(x),p_j\> }_{= \delta_{ij}} \\
					&= \<x^{n+1},p_j\> - \f {\<x^{n+1},p_j\>}{\<p_j,p_j\>} \<p_j,p_j\> = 0
				\end{align*}
			\item
				Angenommen $z\in \C\setminus \R$ ist Nullstelle von $p_n(x) = \sum_{i=0}^n a_i x^i$ mit $a_i\in \R$.
				Dann ist $\_z$ auch Nullstelle von $p$, denn
				\[
					0 = \_{p(z)} = \_{\sum_{i=0}^n a_i z^i} = \sum_{i=0}^n a_i\_{z}^i = p(\_z)
				\]
				Definiere
				\[
					q(x) := \f {p_n(x)}{(x-z)(x-\_z)} \in \P_{n-2}
				\]
				Dann kann man schreiben
				\[
					0 = \<q,p_n\> = \int_{-1}^1 \f {p_n(x)^2}{|x-z|^2} dx \implies p_n(x) = 0
				\]
				was ein Widerspruch ist.

				Angenommen $z\in (-1,1)$ ist eine mehrfache Nullstelle von $p_n$, definiere
				\[
					q(x) := \f {p_n(x)}{(x-z)^2} \in \P_{n-2}
				\]
				dann ist
				\[
					0= \<q,p_n\> = \int_{-1}^1 \f {p_n(x)^2}{(x-z)^2} dx  \implies p_n = 0
				\]
				was ein Widerspruch ist.

				Angenommen $z\in \R\setminus (-1,1)$ ist eine Nullstelle, definiere
				\[
					q(x) := \f {p_n(x)}{x-z} \in \P_{n-1}
				\]
				dann ist
				\[
					0 = \<q,p_n\> = \int_{-1}^1 \f{p_n(x)^2}{x-z} \implies p_n = 0
				\]
				($x-z$ ist entweder $\ge 0$ oder $\le 0$ für alle $x\in [-1,1]$)
				was ein Widerspruch ist.


		\end{enumerate}
	\end{proof}
\end{lem}

\begin{df}[Gauß'sche Quadraturen]
	\label{2.19}
	Sei $n\in \N$, wähle $\{x_i\}_{i=0}^n$ als Nullstellen von $p_{n+1} \in \P_{n+1}$.
	Dann nennen wir die zugehörige Interpolations-Quadratur \emph{Gauß-Quadratur} (G.Q.) auf $[-1,1]$.
\end{df}

\begin{ex*}
	\begin{itemize}
		\item
			Sei $n=1$, $p_2(x) = x^2-\f 13$ mit den Nullstellen $x_0=-\sqrt{\f 13}, x_1 = \sqrt{\f 13}$.
			Gewichte: $\omega_0=\omega_1 = 1$, da Interpolations-Quadratur.

			\begin{table}[H]
				\centering
				\caption{Gauß-Quadraturen auf $[-1,1]$}				
				\begin{tabular}{c|c|c}
					n & $(x_0, \dotsc, x_n)$ & ($\omega_0,\dotsc,\omega_n$) \\ \hline
					0 & 0 & 2 \\
					1 & $\l(-\sqrt{\f 13}, -\sqrt{\f 13}\r)$ & (1,1) \\
					2 & $\l(-\sqrt{\f 35}, 0, \sqrt{\f 35}\r)$ & $\l(\f 59, \f 89, \f 59\r)$ \\
					3 & $\l(- \sqrt{\f 37 + \f 27 \sqrt{\f 65}}, - \sqrt{\f 37 - \f 27\sqrt{\f 65}}, \sqrt{\f 37 - \f 27\sqrt{\f 65}}, \sqrt{\f 37 + \f 27\sqrt{\f 65}} \r)$
					& $\l(\f {18-\sqrt{30}}{36}, \f {18+\sqrt{30}}{36}, \f {18+\sqrt{30}}{36},\f {18-\sqrt{30}}{36}\r)$
				\end{tabular}
			\end{table}
	\end{itemize}
\end{ex*}

\begin{nt*}
	Die sogenannten \emph{Legendre-Polynome} sind definiert durch
	\[
		P_n(x) := \f 1{2^n n!} \f {d^n}{dx^n} (x^2 - 1)^n
	\]
	und erfüllen eine sogenannte „3-Term-Rekursion“:
	\[
		n P_n(x) = (2n-1)xP_{n-1}(x) - (n-1)P_{n-2}(x) \qquad n\ge 2
	\]
	Die orthogonalen Polynome aus Definition \ref{2.17} sind also skalierte Legendre-Polynome
	\[
		p_n(x) = \f {n!}{(2n)!} \f {d^n}{dx^n} (x^2-1)^n
	\]
	Gauß-Quadraturen auf $[-1,1]$ werden auch Gauß-Legendre-Quadraturen genannt.
\end{nt*}

\begin{st}[Symmetrie]
	\label{2.20}
	Gauß-Quadraturen sind symmetrisch.
	\begin{proof}
		Zeige zunächst induktiv: 
		\[
			n \text{ gerade (ungerade) } \implies p_n \text{ gerade (ungerade) }
		\]
		$p_0 = 1$ ist offensichtlich gerade.
		Die Behauptung gelte für $p_i$ mit $i=0,\dotsc,n$.
		\[
			p_{n+1}(x) = x^{n+1} - \sum_{i=0}^n \f{\<x^{n+1},p_i\>}{\<p_i,p_i\>} p_i(x)
		\]
		Sei $n$ ungerade, dann ist $x^{n+1}$ gerade.
		Für $i$ ungerade ist
		\[
			\<x^{n+1},p_i\> = \int_{-1}^1 \underbrace{x^{n+1}}_{\text{gerade}} \underbrace{p_i(x)}_{\text{ungerade}} dx = 0
		\]
		Für $i$ gerade ist $p_i(x)$ gerade, also ist $p_{n+1}$ eine Linearkombination von geraden Funktionen, also ist $p_{n+1}$ gerade.
		Analog für $n$ gerade.

		Falls $x$ Nullstelle von $p_n$, dann ist auch $-x$ Nullstelle von $p_n$.
		Damit sind die Stützstellen der Gauß-Quadratur symmetrisch und somit auch die Gewichte (da Interpolations-Quadratur).
	\end{proof}
\end{st}

\begin{st}[Exaktheit]
	\label{2.21}
	Die Gauß-Quadratur $I_n$ ist exakt auf $\P_{2n+1}$.
	\begin{proof}
		Sei $p\in \P_{2n+1}$.
		Polynomdivision mit $q\in \P_n$ liefert
		\[
			p= p_{n+1} \cdot q + r \qquad  q,r\in \P_n
		\]
		Es folgt
		\begin{align*}
			I(p) 
			&= \int_{-1}^1 p(x) dx 
			= \underbrace{\int_{-1}^1 p_{n+1}q(x) dx}_{=0 \text{ da $p_{n+1}\orth \P_n$}} + \int_{-1}^1 r(x) dx
			= \int_{-1}^1 r(x) dx 
		\end{align*}
		Außerdem
		\begin{align*}
			I_n(p) 
			= \sum_{i=0}^n \omega_i p(x_i) \\
			= \sum_{i=0}^n \omega_i \underbrace{p_{n+1}(x_i)}_{=0} q(x_i) + \sum_{i=0}^n \omega_i r(x_i) \\
			= I_n(r) = I(r)
		\end{align*}
		da $I_n$ exakt auf $\P_n$.
		Also $I_n(p) = I(p)$.
	\end{proof}
\end{st}

\begin{st}[Grenze der Exaktheit]
	\label{2.22}
	Es existiert keine Interpolations-Quadratur $I_n$, welche exakt auf $\P_{2n+2}$ (oder höher) ist.
	\begin{proof}
		Seien $\{x_i\}_{i=0}^n$ die Stützstellen von $I_n$.
		Betrachte 
		\[
			p(x) = \prod_{i=0}^n (x-x_i)^2 \in \P_{2n+2}
		\]
		Es gilt
		\[
			I_n(p) = \sum_{i=0}^n \omega_i \underbrace{p(x_i)}_{=0} = 0
		\]
		aber
		\[
			I(p) = \int_{-1}^1 \underbrace{\prod_{i=0}^n (x-x_i)^2 }_{\ge 0} dx > 0
		\]
		also $I_n(p) \neq I(p)$ also nicht exakt auf $\P_{2n+2}$.
	\end{proof}
\end{st}

\begin{st}[Eindeutigket der Gauß-Quadratur]
	\label{2.23}
	Es existiert genau eine Quadratur $I_n$, welche exakt auf $\P_{2n+1}$ ist.
	\begin{proof}
		Die Existenz ist klar: Gauß-Quadratur \ref{2.21}.

		Sei $I_n$ die Gauß-Quadratur und $\_{I_n}$ eine Quadratur, welche exakt auf $\P_{2n+1}$ ist.
		Es reicht zu zeigen, dass die Stützstellen $\{x_i\}_{i=0}^n$ von $\_{I_n}$ Nullstellen vom orthogonalen Polynom $p_{n+1}$ sind, denn dann sind auch die Gewichte identisch zu $I_n$, da beide Interpolations-Quadraturen und exakt auf $\P_n$ sind.
	
		Sei $q\in \P_n$, $\_p_{n+1} := \prod_{i=0}^n (x-x_i) \in \P_{n+1}$.
		\[
			\int_{-1}^1 \_p_{n+1}(x) q(x) dx 
			= I(\underbrace{\_p_{n+1} q}_{\in \P_{2n+1}})
			\stackrel{\text{Exaktheit}}= \_I_n (\_p_{n+1}q)
			\stackrel{\_p_{n+1}(x_i)=0}= 0
		\]
		Also ist $\_p_{n+1} \orth \P_n$.

		Es gilt $\dim(\P_{n+1}) = \dim( \P_n^\orth \cap \P_{n+1}) + \dim (\P_n)$, also
		\[
			\dim( \P_n^\orth \cap \P_{n+1}) = \dim(\P_{n+1}) - \dim(\P_n) = n+2 - (n+1) = 1
		\]
		Damit ist 
		\[
			 \<p_{n+1}\> = \P_n^\orth \cap \P_{n+1} = \<\_p_{n+1}\>
		\]
		Also sind die $p_{n+1}$ und $\_p_{n+1}$ linear abhängig: $\_p_{n+1} = \lambda p_{n+1}$.
		Damit sind die Nullstellen identisch, also auch die Stützstellen von $I_n$ und $\_I_n$.
	\end{proof}
\end{st}

\begin{ex*}
Vergleich von Newton-Cotes und Gauß-Quadratur mit $f(x)=\cos(x\f \pi 2)$ auf $[-1,1]$: \\
	\begin{table}[H]
		\centering
		\caption{Fehlervergleich auf $[-1,1]$}
		\begin{tabular}{c|r|c}
			n & Gauß-Quadratur & Newton-Cotes-Quadratur \\ \hline
			0 & 0.7268 & 0.7268 \\
			1 & 0.0401 & 1.2732 \\
			2 & 0.0009 & 0.0601 \\
			3 & 0.0000 & 0.0258 \\
			4 & 0.0000 & 0.0009 \\
		\end{tabular}
	\end{table}
\end{ex*}

\begin{st}[Fehlerschranke]
	\label{2.24}
	Sei $f\in C^{2n+2} ([-1,1])$, dann gilt für den Fehler der Gauß-Quadratur:
	\[
		|I(f) - I_n(f)| \le K \cdot 2^{2n+3} \|f^{2n+2}\|_\infty
	\]
	mit
	\[
		K = \f {((n+1)!)^4}{(2n+3)\big((2n+2)!\big)^3}
	\]
	\begin{proof}
		Übung, oder auf Mathematik-Online: Höllig 2004
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				$K$ wird sehr schnell klein:
				\begin{table}[H]
					\centering
					\caption{}
					\begin{tabular}{c|c}
					n & K \\ \hline
					0 & $\f 1{24}$ \\
					1 & $\f 1{4320}$ \\
					2 & $\f 1{2016000}$ \\
					3 & $\f 1{1778112000}$ 
					\end{tabular}
				\end{table}
			\item
				Die Sätze \ref{2.24}, \ref{2.27} i), ii) und \ref{2.12} i), ii) haben alle die gleiche Struktur.

				Sei $d$ maximal, so dass $I_n$ exakt auf $\P_d([a,b])$ (also $d=n$ bei Newton-Cotes für $n$ ungerade, $d=n+1$ bei Newton-Cotes für $n$ gerade und $d=2n+1$ bei Gauß-Quadratur)

				Dann ist
				\[
					|I(f) - I_n(f) | \le K \cdot (b-a)^{d+2} \|f^{d+1}\|_\infty
				\]
				für geeignetes $K$.
		\end{itemize}
	\end{note}
\end{st}

\begin{st}[Positivität]
	\label{2.25}	
	Alle Gewichte $\{\omega_j\}_{j=0}^n$ der Gauß-Quadratur sind positiv: $\omega_j > 0$.
	\begin{proof}
		$I_n$ ist exakt auf $\P_{2n+1}$ also werden quadrierte Lagrange-Polynome exakt integriert:
		\[
			\omega_j = \sum_{i=0}^n \omega_i (\underbrace{L_j^n(x_i)}_{\delta_{ij}})^2 = I_n((L_j^n)^2) = I((L_j^n)^2) =\int_{-1}^1 \underbrace{(L_j^n(x))^2}_{\ge 0} dx > 0
		\]
	\end{proof}
\end{st}

\begin{st}[Konvergenz]
	\label{2.26}
	Sei $(I_n)_{n\in \N}$ eine Sequenz von Gauß-Quadraturen.
	Dann gilt für $f\in C^0([a,b])$:
	\[
		\lim_{n\to \infty} I_n(f) = I(f)
	\]
	\begin{proof}
		Sei $\eps > 0$.
		Nach dem Weierstrass-Approximationssatz existiert $p\in \P_{n_0}$ für genügend großes $n_0\in \N$, so dass
		\[
			|f(x) - p(x)| \le \f {\eps}{2(b-a)} \qquad \forall x\in [a,b]
		\]
		Dann gilt für alle $n\ge n_0$:
		\begin{align*}
			|I(f) - I_n(f)|
			\le \underbrace{|I(f) - I(p)|}_{\le \int_a^b |f(x) - p(x)| dx \le (b-a)\f \eps{2(b-a)}= \f \eps 2} 
			+ \underbrace{|I(p) - I_n(p)|}_{=0 \text{ da $I_n$ exakt auf $\P_{n_0}$}} 
			+ \underbrace{|I_n(p)-I_n(f)|}_{\le \sum_{i=0}^n |\omega_i||p(x_i)-f(x_i)| \le (b-a) \f {\eps}{2(b-a)} } \\
			= \f \eps 2 + 0 + \f \eps 2 = \eps
		\end{align*}
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				Trotz eventuell fehlender gleichmäßiger Konvergenz der Interpolations-Polynome, d.h.
				\[
					\|p_n - f\|_\infty \not \to 0
				\]
				konvergieren die Integrale $I_n(f) \to I(f)$.
			\item
				Die Aussage von \ref{2.6} ist nicht auf Newton-Cotes oder allgemeinere Interpolations-Quadraturen übertragbar, da i.A. $\sum_{i=0}^n |\omega_i| \to \infty$, der Beweis nutzt aber $\sum_{i=0}^n |\omega_i| = b-a$.
		\end{itemize}
	\end{note}
\end{st}

\begin{nt*}[Allgemeinere Integrale]
	Sei $\omega \in C^0(I)$ eine „Gewichtsfunktion“ auf dem Intervall $I\subset \R$ mit $\int_I \omega(x) dx < \infty$ und $\omega(x) \ge 0$ für alle $x\in I$.

	Dann definiert 
	\[
		\<f,g\>_\omega := \int_I f(x) g(x) \omega(x) dx
	\]
	ein Skalarprodukt auf $C^0(I)$.

	Definiere analog zu \ref{2.17} $\{p_i\}_{i=0}^{n+1}$ als Orthogonalsystem bezüglich $\<\cdot,\cdot\>_\omega$ von $\P_{n+1}$.
	Wähle die Quadraturpunkte $\{x_i\}_{i=0}^n$ als Nullstellen von $p_{n+1}$ und setze $\{\omega_j\}_{j=0}^n$ als gewichtete Integrale der Lagrange-Polynome:
	\[
		\omega_i := \int_I L_i^n(x) \omega(x) dx
	\]
	Dann ist $I_n(f) = \sum_{i=0}^n\omega_i (x_i)$ eine Approximation für $I(f) := \int_I f(x) \omega(x) dx$.
	Es existieren einige spezielle Versionen
	\begin{itemize}
		\item
			$I=[-1,1], \omega(x) =1$, dann sind $\{x_i\}$ die Nullstellen der Legendre-Polynome („Gauß-Legendre-Quadraturen“).
		\item
			$I=(-1,1), \omega(x) = \f 1{\sqrt{1-x^2}}$, dann sind die Tschebyscheff-Polynome ein Orthonormalsystem bezüglich $\<\cdot,\cdot\>_\omega$, $p_n= \hat T_n$.
			\[
				x_i := \cos \l(\pi \f {2j+1}{2(n+1)}\r) \qquad j= 0,\dotsc, n
			\]
			nach Satz \ref{1.14}. Und $\omega_0 = \dotsb = \omega_n = \f {\pi}{n+1}$.
		\item
			$I=[0,\infty), \omega(x) = e^{-x}$, dann bilden die Laguerre-Polynome ein Orthogonalsystem:
			\[
				L_0(x) := 1, \qquad L_1(x) := 1-x, \qquad L_{n+1}(x) = (1+2n-x)L_n(x) - n^2 L_{n-1}(x)
			\]
			mit $p_n = (-1)^n L_n$ („\emph{Gauß-Laguerre-Quadratur}“).
		\item
			$I=(-\infty,\infty), \omega(x) = e^{-x^2}$, dann bilden die \emph{Hermite-Polynome} ein Orthogonalsystem:
			\[
				H_0(x) := 1, \qquad H_1(x) := 2x, \qquad H_{n+1}(x) := 2xH_n(x) - 2nH_{n-1}(x)
			\]
			mit $p_n(x)=2^n H_n(x)$.
		\item
			$\alpha, \beta > -1$, $\omega(x) = (1-x)^\alpha (1+x)^\beta$, $I=(-1,1)$.
			Dann bilden \emph{Jacobi-Polynome} ein Orthogonalsystem:
			\[
				J_n(x;\alpha,\beta) = \f 1{2^n n! \omega(x)}\f {d^n}{dx^n}\Big( (x^2-1)^n \omega(x)\Big)
			\]
			Es ergibt sich die „Gauß-Jacobi-Quadratur“.
			Für $\alpha=\beta=0$ ergibt sich die Gauß-Legendre-Quadratur.
			Für $\alpha=\beta= -\f 12$ ergibt sich die Gauß-Tschebyscheff-Quadratur.
	\end{itemize}
	Diese Formeln sind vorteilhaft bei der Integration von Funktionen mit Singularitäten an Endpunkten von $I$.
\end{nt*}

\begin{ex*}
	Sei $f(x) = \f {x^2}{\sqrt{1-x^2}}$ und $\int_{-1}^1 f(x) dx = \int_{-1}^1 x^2 \underbrace{\f 1{\sqrt{1-x^2}}}_{\omega(x)} dx$.
	Wähle die Gauß-Tschebyscheff-Quadratur mit $n=2$ (für $p(x) = x^2$ exakt).
	\begin{align*}
		x_0 := \cos\l(\pi \f 16 \r) = \sqrt{\f 34}, \qquad x_1 &= \cos\l(\pi \f 36\r) = 0, \qquad x_2 = \cos \l(\pi\f 56\r) = -\sqrt{\f 34}
	\end{align*}
	mit $\omega_0 = \omega_1 = \omega_2 = \f {\pi}3$
	\[
		\int_{-1}^1 f(x) dx = \sum_{i=0}^2 \omega_i x_i^2 = \f {\pi}3 \cdot \f 64 = \f \pi2
	\]
	Das stimmt mit der analytischen Lösung überein:
	\[
		\int_{-1}^1 \f {x^2}{\sqrt{1-x^2}} dx = \f 12 \sin^{-1}(x) - \f 12 x\sqrt{1-x^2} \; \Big|_{-1}^1 = \f 12 \l( \f \pi 2 - \l(-\f \pi 2\r) \r) = \f \pi2
	\]
\end{ex*}

\begin{nt*}[Zusammengesetzte Gauß-Quadraturen]
	\begin{itemize}
		\item
			Berechnung der Nullstellen von $p_n$ in hoher Präzision kann für große $n$ schwierig sein.
		\item
			Daher sind auch zusammengesetzte Gauß-Quadraturen praktikable Alternativen.
			\begin{ex*}
				Zusammengesetzte 2-Punkt-Gauß-Quadratur auf $[a,b]$, $N\in \N$, $h=\f {b-a}N$, $n:= N\cdot 2 - 1$, Mittelpunkte $m_l := a+ (l-\f 12)h$ , $l= 1,\dotsc, N$.
				\[
					I_h (f) = \f h2 \sum_{l=1}^N \Big( f\l(m_l- \f 1{2\sqrt 3} h\r) + f\l( m_l + \f 1{2\sqrt 3} h\r) \Big)
				\]
			\end{ex*}
		\item
			Die Fehlerschranke \ref{2.24} überträgt sich mittels \ref{2.8} ii) auch auf zusammengesetzte Gauß-Quadraturen.
	\end{itemize}
\end{nt*}


\section{Fehlerdarstellung nach Peano}

Wir wollen einen abstrakten Rahmen für Fehlerfunktionale vorstellen, welcher insbesondere einige der vorhergehenden Fehleraussagen impliziert.

\begin{df}[Zulässige Funktionale]
	\label{2.27}
	Eine lineares Funktional $R : C^k([a,b]) \to \R$ heißt \emph{zulässig}, wenn es eine Punktauswertung 
	\[
		f^{(m)}(x_0)
		\qquad (x_0\in [a,b] \;\land\; m\in \{0,\dotsc,k\})
	\]
	oder ein gewichtetes Integral
	\[
		\int_a^b f^{(m)}(x) \omega(x) \; dx
		\qquad (m \in \{0,\dotsc,k\})
	\]
	oder eine endliche Linearkombination dieser beiden ist.
\end{df}

\begin{ex*}
	\begin{itemize}
		\item
			Fehlerfunktionale für Quadraturen sind zulässig
			\[
				R(f) = \int_a^b f(x) dx - \sum_{i=0}^n \omega_i f(x_i)
			\]
		\item
			Fehlerfunktionale für Finite-Differenzen-Approximation sind zulässig
			\[
				R(f) = \f {f(b)-f(a)}{b-a} - f'(x_0) \qquad x_0\in [a,b]
			\]
	\end{itemize}
\end{ex*}

\begin{lem}[Vertauschungsregel]
	\label{2.28}	
	Für ein zulässiges Funktional $R$, stetiges $u\in C^0([a,b])$ und $K\in C^k([a,b]^2)$ gilt
	\[
		R\l( \int_a^b K(t,\cdot) u(t) dt \r) 
		= \int_a^b R\Big(K(t,\cdot)\Big) u(t) dt
	\]
	\begin{proof}
		Setze
		\begin{align*}
			y(\cdot) &= \int_a^b K(t,\cdot) u(t) dt \\
			v(t) &= R\big(K(t,\cdot)\big)
		\end{align*}
		Zu zeigen ist dann $R(y) = \int_a^b v(t) u(t) dt$.
		Wir zeigen zunächst für $R(f) = f^{(m)}(x_0)$:
		\begin{align*}
			R(y) = \f {d^m}{dx^m} \Big( \int_a^b K(t,x) u(t) dt \Big) \Big|_{x_0}
			= \int_a^b \Big( \f {d^m}{dx^m} K(t,x) \Big) u(t) dt \Big|_{x_0}
			= \int_a^b v(t) u(t) dt
		\end{align*}
		Für $R(f) = \int_a^b f^{(m)}(x) \omega(x) dx$ verläuft der Beweis analog und für Linearkombinationen aus beiden folgt die Behauptung aus der Linearität der Punktauswertung und des Integrals.
	\end{proof}
\end{lem}

\begin{lem}
	\label{2.29}
	Sei 
	\[
		h_+^l := \begin{cases}h^l & h\ge 0 \\ 0 & \text{sonst}\end{cases}
	\]
	Es gilt für $f\in C^{k+1}([a,b])$:
	\[
		f(x) = (P_kf)(x) + \int_a^b K_k(t,x) f^{(k+1)}(t) dt
	\]
	mit $P_k \in \P_k$ und $K_k \in C^{k-1}([a,b]^2)$ gegeben durch
	\begin{align*}
		(P_kf)(x) &:= \sum_{j=0}^k f^{(j)}(x) \f {(x-a)^j}{j!} \\
		K_k(t,x) &:= \f 1{k!}(x-t)^k_+
	\end{align*}
	\begin{proof}
		Nutze Taylor mit Integralrestglied:
		\begin{align*}
			f(x) &= \sum_{j=0}^k f^{(j)}(a) \f {(x-a)^j}{j!} + \int_a^x \f {(x-t)^k}{k!} f^{(k+1)}(t) dt
			&= P_k f + \int_a^b \f {(x-t)^k_+}{k!} f^{(k+1)} (t) dt
		\end{align*}
	\end{proof}
\end{lem}

\begin{st}[Peano Fehlerdarstellung]
	\label{2.30}
	Sei $R$ ein zulässiges Funktional auf $C^k([a,b])$ welches für ein $n>k$ auf $\P_n$ verschwindet (d.h. $\forall p\in \P_n : R(p)=0$).
	Dann gilt für alle $f\in C^{n+1}([a,b])$:
	\[
		R(f) = \int_a^b K(t) f^{(n+1)}(t) dt
	\]
	mit Peano-Kern 
	\[
		K(t) := \f 1{n!}R\Big((\cdot - t)^n_+\Big)
	\]
	unabhängig von $f$.
	\begin{proof}
		Da $f\in C^{n+1}([a,b])$, ist $f,P_nf,f-P_nf \in C^k([a,b])$
		\begin{align*}
			R(f) &\stackrel{\ref{2.29}}= R(P_n f) + R \Big( \int_a^b \underbrace{K_n(t,\cdot)}_{\in C^k([a,b])} f^{(n+1)}(t) dt \Big)
			&\stackrel{\ref{2.28}} =  \int_a^b R\big(K_n(t,\cdot)\big) f^{(n+1)}(t) dt \\
			&= \int_a^b R \Big( \f 1{n!}(\cdot - t)^n_+ \Big) f^{(n+1)}(t) dt
		\end{align*}
	\end{proof}
\end{st}

\begin{lem}[Verallgemeinerter Mittelwertsatz] \label{2.31}
	Seien $f,g \in C^0([a,b])$ mit $g\ge 0$.
	Dann existiert $\zeta \in [a,b]$ so dass
	\[
		\int_a^b f(x) g(x) dx = f(\zeta) \int_a^b g(x) dx
	\]
	\begin{proof}
		Setze $r(t) := \int_a^b f(x)g(x) dx - f(t) \int_a^b g(x) dx \in C^0([a,b])$.
		Wir suchen die Nullstelle $\xi \in [a,b]$.

		Seien $x_{\text{min}}, x_{\text{max}} \in [a,b]$ mit
		\[
			f(x_{\text{min}}) = \min_{x\in[a,b]} f(x)
			\qquad
			f(x_{\text{max}}) = \max_{x\in[a,b]} f(x)
		\]
		Daraus folgt $r(x_{\text{min}}) = \int_a^b \big(f(x)-f(x_{\text{min}})\big)g(x) dx \ge 0$ und analog $r(x_{\text{max}}) \le 0$.
		Aus dem Zwischenwertsatz folgt dann, dass das gesuchte $\xi \in[x_{\text{min}}, x_{\text{max}}] \subset [a,b]$ existiert sodass $r(t) = 0$.
	\end{proof}
\end{lem}

\begin{kor}[Fehlerdarstellung durch Monomauswertung] \label{2.32}
	Falls Peano-Kern $K(t)$ auf $[a,b]$ keinen Vorzeichenwechsel hat, so gilt: \\
	$\forall f\in C^{n+1}([a,b])$  $\exists $  $\xi\in [a,b]$ mit
	\[
		R(f) = f^{(n+1)}(\xi) \f 1{(n+1)!} R(x^{n+1})
	\]
	\begin{proof}
		\[
			R(f) \stackrel{\ref{2.30}}= \int_a^b K(t) f^{(n+1)}(t) dt 
			\stackrel{\ref{2.31}}= f^{(n+1)}(\xi)\int_a^b K(t) dt
		\]
		Anwendung auf $x^{n-1}$:
		\[
			R(x^{n+1}) = \underbrace{(n+1)!}_{=\f {d^{n+1}}{dx^{n+1}}x^{n+1}} \int_a^b K(t) dt \quad 
			\implies \quad \int_a^b K(t) dt = \f {R(x^{n+1})}{(n+1)!}
		\]
		Dies in die vorige Gleichung eingesetzt ergibt die Behauptung.
	\end{proof}
\end{kor}

\begin{ex*}
	Anwendung auf Simpson-Quadratur auf $[-1,1]$:
	\[
		R(f) = \f 13 \big(f(-1) + 4f(0) + f(1)\big) -\int_{-1}^1 f(x) dx
	\]
	$R: C^0 ([a,b]) \to \R$, $k=0$.
	Die Simpson-Quadratur ist exakt auf $n=3$, also mit \ref{2.30} ist
	\[
		R(f) = \int_{-1}^1 K(t) \cdot f^{(4)}(t) dt \qquad f\in C^4([-1,1])
	\]
	mit
	\begin{align*}
		K(t) &= \f 1{3!}R \big((\cdot - t)_+^3\big) \\
		&= \f 1{18} \big((-1-t)_+^3+ 4(0-t)_+^3 + (1-t)_+^3 \big) - \f 16 \int_{-1}^1 (x-t)_+^3 dx \\
	\end{align*}
	Es ist $(-1-t)_+^3 = 0$ für $t\in [-1,1]$ und
	\begin{align*}
		(-t)_+^3 &= \begin{cases}
			-t^3 & t\in[-1,0] \\
			0 & t\in[0,1]
		\end{cases} \\
		(1-t)_+^3 &= (1-t)^3 \\
		\int_{-1}^1 (x-t)_+^3 dx &= \int_t^1 (x-t)^3 dx = \f 14(1-t)^4
	\end{align*}
	Also für $t\in [0,1]$:
	\begin{align*}
		K(t) &= \f 1{18} (1-t)^3 - \f 1{24} (1-t)^4 \\
			&= \f 1{72}(1-t)^3\big(4-3(1-t)\big) = \f 1{72}(1-t)^3(1+3t) \ge 0
	\end{align*}
	Für $t\in [-1,0]$:
	\begin{align*}
		K(t) &= \f 1{18}\big(-4t^3 + (1-t)^3\big) - \f 1{24}(1-t)^4 \\
		&=  \dotsb \\
		&= K(-t) \ge 0
	\end{align*}
	Da $K(t) \ge 0$ auf $[-1,1]$ folgt mit \ref{2.32} die Darstellung
	\[
		R(f) = f^{(4)}(\xi) \f {R(x^4)}{4!}
	\]
	also
	\begin{align*}
		R(x^4) &= \f 13 (1+4\cdot 0 + 1) - \int_{-1}^1 x^4 dx \\
		&= \f 23 - \f 15 x^5 \Big|_{-1}^1
		= \f {10}{15} - \f {6}{15} = \f 4{15}
	\end{align*}
	Also
	\[
		R(f) = f^{(4)}(\xi) \f 4{15} \cdot \f 1{24} 
		= \f 1{90} f^{(4)}(\xi) 
		= \f 1{2880} (b-a)^5 f^{(4)}(\xi)
	\]
	und damit wie \ref{2.12} ii)
	\[
		|R(f)| \le \f 1{2880} (b-a)^5 \cdot \|f^{(4)}\|_\infty
	\]
	Analog für Finite-Differenzen-Quotienten (Übung)
\end{ex*}


\section{Romberg-Verfahren}


\begin{df}[Romberg-Verfahren]
	\label{2.33}
	Sei $I_h$ eine zusammengesetzte Quadratur auf $[a,b]$ mit Diskretisierungsparameter $h\in \R^+$.  \\
	Sei $f\in C^0([a,b])$. \\
	Setze $b(h) := I_h(f)$. \\
	Wähle $h_i := h_0 2^{-i}$ für $i=1,\dotsc,n$ mit $n\in \N$ (Romberg-Folge), so dass $b(h_i)$ wohldefiniert ist.
	Führe Richardson-Extrapolation gemäß Definition \ref{1.8} auf $\{h_i\}_{i=0}^n$ und $\{I_{h_i}(f)\}_{i=0}^n$ durch, um eine Approximation für
	\[
		\lim_{h\to 0} b(h) = \lim_{h\to 0} I_h(f) = I(f)
	\]
	zu bekommen. 
	
	\begin{note}
		\begin{itemize}
			\item
				Die Wohldefiniertheit der $b(h_i)$ ist für $\f{b-a}{h_0} \in \N$ gewährleistet.
			\item
				Die Auswertung von $b(h) = I_h(f)$ ist für kleine $h$ teuer ($\mathcal O(\f 1h)$), daher lohnt sich die Richardson-Extrapolation.
			\item
				Wie in Abschnitt 1.1 bemerkt, kann die Richardson-Extrapolation verbessert werden, falls die asymptotische Entwicklung nur $h^{qn}$ Monome enthält für $q>1$.
				Dies ist insbesondere für die zusammengesetzte Trapezregel erfüllt, wie im Folgenden hergeleitet wird.
		\end{itemize}
	\end{note}
\end{df}

\begin{df}[Bernoulli-Polynome]
	\label{2.34}
	Wir definieren rekursiv die \emph{Bernoulli-Polynome} $B_k \in \P_k$ durch
	\[
		B_0(x) := 1 \qquad
		\f d{dx} B_k(x) := k \cdot B_{k-1}(x) \quad \land \quad \forall k\ge 1 : \int_0^1 B_k(x) dx = 0
	\]
	Die \emph{Bernoulli-Zahlen} sind gegeben durch
	\[
		\beta_k := B_k(0)
	\]
	\begin{note}
		\begin{itemize}
			\item
				Die $B_k$ sind wohldefiniert durch
				\[
					\_{B_k}(x) := \int_0^x k B_{k-1}(t) dt 
					\quad \text{und} \quad
					B_k(x) := \_{B_k}(x) - \int_0^1 \_{B_k}(t) dt
				\]
			\item
				Die ersten Bernoulli-Polynome sind
				\begin{align*}
					B_0(x) &= 1  \\
					B_1(x) &= x - \f 12 \\
					B_2(x) &= x^2 - x + \f 16 \\
					B_3(x) &= x^3 - \f 32 x^2 + \f 12 x
				\end{align*}
				Die ersten Bernoullizahlen sind dann
				\[
					\beta_0 = 1, \quad
					\beta_1 = -\f 12, \quad
					\beta_2 = \f 16, \quad
					\beta_3 = 0
				\]
			\item
				Mann kann zeigen
				\begin{align*}
					B_k(x) &= \sum_{i=0}^k \binom ki \beta_i x^{k-i} \\
					\f x{e^x - 1} &= \sum_{k=0}^\infty \beta_k \f {x^k}{k!}
				\end{align*}
		\end{itemize}
	\end{note}
\end{df}

\begin{lem}[Eigenschaften]
	\label{2.35}
	Es gilt
	\begin{enumerate}[i)]
		\item
			$B_k(0) = B_k(1)$ für $k\ge 2$
		\item
			$B_k(t) = (-1)^k B_k(1-t)$ für $k\ge 0$
		\item
			$B_{2k+1}(0) = B_{2k+1}(1) = B_{2k+1}(\frac 12) = 0$ für $k\ge 1$
	\end{enumerate}
	\begin{proof}
		Es gilt
		\[
			B_k (\tf 12 + \tau) = B_k(\tf 12 - \tau)  + \int_{\f 12 - \tau}^{\f 12 + \tau} k B_{k-1}(t) dt
		\]
		\begin{enumerate}[i)]
			\item
				Für $\tau = \f 12$ folgt mit $\int_{0}^1 B_{k-1}(t) dt = 0$ für $k\ge 2$
				\[
					B_k(1) = B_k(0) + 0
				\]
			\item
				Zeige induktiv: $B_k$ achsensymmetrisch für gerade $k$ und punktsymmetrisch bezüglich $(\f 12,0)$ für ungerade $k$.
				Für $k=0, k=1$ ist die Aussage klar.
				Gelte die Behauptung für $k-1$.

				Falls $k$ gerade, dann ist $B_{k-1}$ punktsymmetrisch zu $(\f 12, 0)$:
				\[
					B_k (\tf 12 + \tau) = B_k(\tf 12 - \tau)  + \underbrace{\int_{\f 12 - \tau}^{\f 12 + \tau} k B_{k-1}(t) dt}_{=0}
				\]
				und damit $B_k(\f 12 + \tau) = B_k(\f 12 - \tau)$.

				Falls $k$ ungerade, dann ist $B_{k-1}$ achsensymmetrisch zu $x=\f 12$ und
				\[
					\int_{\f 12 - \tau}^{\f 12} B_{k-1}(t) dt = \int_{\f 12}^{\f 12 + \tau} B_{k-1}(t) dt
				\]
				Setze $c := B_k(\f 12)$
				\begin{align*}
					B_k(\tf 12 + \tau) - c 
					&= B_k (\tf 12) + \int_{\f 12}^{\f 12 + \tau} k B_{k=1}(t) dt - B_k(\tf 12) \\
					&= c - \l( \int_{\f 12}^{\f 12 - \tau} k B_{k-1}(t) dt + B_k( \tf 12) \r) \\
					&= c - B_k(\tf 12 - \tau)
				\end{align*}
				also ist $B_k$ punktsymmetrisch zu $(\f 12, c)$.
				Außerdem
				\[
					c = \int_{0}^1 B_k(x) dx = 0
				\]
				womit $B_k$ punktsymmetrisch zu $(\f 12,0)$ ist.
			\item
				Für $k'=2k+1$ ist $B_{k'}$ punktsymmetrisch zu $(\f 12,0)$ nach ii), also $B_{k'}(\f 12) = 0$.
				$B_{k'-1}$ ist achsensymmetrisch, also
				\[
					0 = \int_0^1 B_{k'-1}(t) dt = 2 \int_0^{\f 12} B_{k'-1} (t) dt 
				\]
				Es gilt
				\[
					0 = B_{k'}(\tf 12) = B_{k'}(0) + \int_{0}^{\f 12} k' B_{k'-1}(t) dt = B_{k'}(0)
				\]
				und $B_{k'}(1) = 0$ wegen der Punktsymmetrie.
		\end{enumerate}
	\end{proof}
\end{lem}

\begin{st}[Euler-MacLaurin'sche Summenformel]
	Sei $f\in C^{2m}([a,b])$, $m\in \N$, $h:= \f {b-a}N$, $N\in \N$ und $I_h$ die zusammengesetzte Trapezregel
	\[
		I_h(f) = \f h2 \big(f(a) + f(b)\big) + h \cdot \sum_{j=1}^{N-1} f(a+jh)
	\]
	dann gilt für den Fehler
	\[
		I_h(f) - I(f) = \sum_{k=1}^{m-1} h^{2k} \f{\beta_{2k}}{(2k)!} \Big( f^{(2k-1)}(b) - f^{(2k-1)}(a) \Big) + O(h^{2m})
	\]
	\begin{proof}
		Sei $\phi \in \C^{2m}([0,1])$, dann gilt wegen $\f{B_k'}k = B_{k-1}$, $B_1(1) = \f 12, B_1(0) = - \f 12$:
		\begin{align*}
			&\int_0^1 B_0(t) \phi(t) dt 
			= B_1(t) \phi(t) \Big|_{0}^1 - \int_0^1 B_1(t) \phi'(t) dt \\
			&\quad= \f 12 \big( \phi(1) - \phi(0) \big) - \f 12 B_2(t) \phi'(t) \Big|_0^1 + \int_0^1 \f 12 B_2(t) \phi''(t) dt \\
			&\quad= \f 12 \big( \phi(1) - \phi(0) \big) - \f 12 B_2(t) \phi'(t) \Big|_0^1 + \underbrace{\f 1{2\cdot 3} B_3(t) \phi''(t) \Big|_0^1}_{=0 \text{ \ref{2.35} ii)}} - \int_0^1 \f 1{3!} B_3(t) \phi^{(3)}(t) dt \\
			&\quad= \dotsc \\
			&\quad= \f 12 \big( \phi(1) - \phi(0) \big) - \sum_{k=1}^{m-1} \l( \f {B_{2k}(1)}{(2k)!} \phi^{(2k-1)}(1) - \f {B_{2k}(0)}{(2k)!} \phi^{(2k-1)}(0) \r) + \int_0^1 \f 1{(2m)!} B_{2m}(t) \phi^{(2m)}(t) \; dt \\
			&\quad\stackrel{\ref{2.35} i)}=  \f 12 \big(\phi(1) - \phi(0)\big) - \sum_{k=1}^{m-1} \f {\beta_{2k}}{(2k)!} \big( \phi^{(2k-1)}(1) - \phi^{(2k-1)}(0) \big) + \int_0^1 \f {B_{2m}(t)}{(2m)!} \phi^{(2m)}(t) \; dt
		\end{align*}
		Wähle  $\phi_j(t) := h \cdot f(x_{j-1} + th)$, $x_j := a+jh$, $j=1,\dotsc, N$ (bzw. $j=0,\dotsc, N$).
		Dann gilt
		\begin{itemize}
			\item
				$\displaystyle
				\int_0^1 \phi_j(t) dt = \int_{x_{j-1}}^{x_j} f(x) dx
				$
			\item
				$\displaystyle
				\phi_j^{(k-1)}(t) = h^k f^{(k-1)}(x_{j-1} + th)
				$
			\item
				$\displaystyle
				\phi_j(1) = h f(x_j) = \phi_{j+1}(0)
				$
			\item
				$\displaystyle
				\phi_j^{2k-1}(1) = \phi_{j+1}^{2k-1}(0)
				$
		\end{itemize}
		\fixme[Vorzeichenfehler irgendwo, Beweis nachvollziehen]: Vllt bei $\f 12 \big( \phi(1) - \phi(0) \big)$, da $B_1(0) = - \f 12$  \\
		Damit folgt:
		\begin{align*}
			I(f)&=\int_a^b f(x)\, dx = \sum_{j=1}^N \int_{x_{j-1}}^{x_j} f(x) \, dx = \sum_{j=1}^N \int_0^1 \phi_j(t)\, dt\\
			&= \sum_{j=1}^N \frac{1}{2}(\phi_j(0)+ \phi_j(1))- \sum_{j=1}^N \sum_{k=1}^{m-1} \frac{\beta_{2k}}{(2k)!} ( \phi_j^{(2k-1)} (1)-\phi_j^{(2k-1)}(0))+\sum_{j=1}^N \int_0^1 \frac{B_{2m}(t)}{(2m)!} \phi_j^{(2m)}(t)\, dt\\
			&= \sum_{j=1}^N \frac{h}{2} (f(x_j)+f(x_{j-1})) - \sum_{j=1}^N \sum_{k=1}^{m-1} \frac{\beta_{2k}}{(2k)!} h^{2k} (f^{(2k-1)}(x_j)-f^{(2k-1)} (x_{j-1})) \\
			&\qquad \qquad + \sum_{j=1}^N \int_0^1 \frac{B_{2m}(t)}{(2m)!} h^{2m+1} \cdot f^{(2m)}(x_{j-1}+ ht)\, dt\\
			&= I_h(f)- \sum_{k=1}^{m-1} \frac{\beta_{2k}}{(2k)!} h^{2k} (f^{(2k-1)} (b)-f^{(2k-1)}(a)) +h^{2m}\left [h \sum_{j=1}^N \int_0^1 f^{(2m)} (x_{j-1}+ht)\, \frac{B_{2m}(t)}{(2m)!} dt\right ]
		\end{align*}
		Dies ist die Behauptung falls der Term $[...]$ durch Konstante unabhängig von $h$ beschränkt werden kann. Dies ist der Fall:
		\[
			|h\sum_{j=1}^N \int_0^1 f^{(2m)}(x_{j-1}+t\cdot h) \frac{B_{2m}(t)}{(2m)!} \, dt|\le h \frac{1}{(2m)!} \sum_{j=1}^N ||f^{(2m)}||_\infty \cdot ||B_{2m}||_\infty = \underbrace{h\cdot N}_{b-a}\cdot \frac{1}{(2m)!} ||f^{(2m)}||_\infty ||B_{2m}||_\infty
		\]
		und dies ist unabhängig von $h$.
	\end{proof}
\end{st}
\begin{note*}
	\begin{itemize}
		\item Die zusammengesetzte Trapezregel ist also sehr gut für periodische Funktionen
			\[
				f^{(2k-1)}(b)=f^{(2k-1)}(a) \implies I_h(f)-I(f)=O(h^{2m})
			\]
		\item Zusammengesetzte Trapezregel erlaubt asymptotische Entwicklung in $h^2$. Dies kann durch Wahl $q=2$ in der Richardson-Extrapolation (Romberg-Verfahren) ausgenutzt werden.
	\end{itemize}
	(Anmerkung: $q=2$ für die Richardson-Extrapolation zu wählen bedeutet, dass wir bezüglich $b(h)=\sum_{n\in \N} h^{qn} c_n$ ) \\
	\fixme[Anmerkung vervollständigen!]
\end{note*}
\section{Weiterführende Techniken}
\begin{seg}[Adaptive Quadraturen]
	Problem: Integration von Funktionen, welche Bereiche mit großer Variation und Bereiche mit kleiner Variation besitzen. Es ergeben sich stets zwei grundlegende Probleme:
	\begin{enumerate}[a)]
		\item Zusammengesetzte Quadratur mit zu groben $h$: Bereiche mit hohen Variationen werden nicht aufgelöst. Dies führt zu einem großen Fehler.
		\item $I_h$ mit feinem h: gesamtes Intervall gut aufgelöst, aber in glatten Bereichen ergeben sich viele überflüssige Stützstellen. Dies führt zu einer langen Rechenzeit.
	\end{enumerate}
\end{seg}
\begin{seg}[Idee:]
	Dies motiviert folgendes:
	\begin{itemize}
		\item unterschiedliche Teilintervall-Größen
		\item automatische Anpassung der Teilintervall-Größen:\\
		 Beginne mit grober Intervall-Zerlegung; Berechne Quadratur, berechne einen Fehlerschätzer für alle Teilintervalle; markiere alle Teilintervalle, auf denen der Fehler zu groß ist, halbiere die markierten Intervalle und wiederhole diese Schritte bis die gewünschte Genauigkeit erreicht ist.
	\end{itemize}
\end{seg}
\begin{seg}[Allgemeines Prinzip:] 
Endliche Wiederholung von: \\
	\begin{figure}[H]
		\fbox{Lösung $\rightarrow$ Schätzung $\rightarrow$ Markierung $\rightarrow$ Verfeinerung $\rightarrow$ Lösung $\rightarrow$ Schätzung $\rightarrow \cdots$}  \\
\end{figure}
\end{seg}
\begin{alg*}
	\begin{algorithmic}[Adaptive Quadratur]
		\Require $f:[a,b]\to \R$
		\Require $\Delta^{(0)}:=\{x_0^{(0)},..., x_{n_0}^{(0)}\}$, Anfangszerlegung $a=x_0^{(0)}<...<x_{n_0}^{(0)}=b$. 
		Wir bezeichnen im Folgenden mit $K_i^{(0)}:=[x_{i-1}^{(0)}, x_{i}^{(0)}]$ die Teilintervalle von $\Delta^{(0)}$.
		\Require $L_{max}$: Maximalzahl der Verfeinerungsiterationen
		\Require $\epsilon_{tol}>0$: gewünschte Toleranzgrenze
		\Require $\theta\in (0,1)$: Parameter für den Anteil der in jeder Iteration zu verfeinernden Intervalle.
		\State $l:=0$
		\State fertig$:= 0$ % Alternativ do-while Schleife (weiß nicht wie ich das definiere
		\While {fertig $\neq 1$}
		\State 1. $I_h(f, \Delta^{(l)}):= \sum_{i=1}^{n_l} I_n (f, K_{i}^{(l)})$ als zusammengesetzte Quadratur bezüglich einer einfachen Quadratur $I_n$, zum Beispiel Trapezregel.
		\State 2. Berechne $\eta_i^{(l)}, i=1,..., n_l$ lokale Fehlerschätzer, zum Beispiel $\eta_i^{(l)}=|I_n(f, K_i^{(l)})- I_{\hat n} (f, K_i^{(l)})|$ mit $I_{\hat n}$ "`bessere Quadratur"', zum Beispiel Simpson-Regel.
		\State 3. $\eta^{(l)}=\sum_{i=1}^{n_l} \eta_i^{(l)}$ Gesamtfehlerschätzer
		\State 4.
		\If{$\eta^{(l)}\le \epsilon_{tol}$ oder $l\ge l_{max}$} fertig$:= 1$ \Else $ $ fertig$:= 0$ \EndIf
		\State 5. \If {fertig$\neq 1$}
		\State $\eta_{max}^{(l)}:= \max \eta_i^{(l)}$ maximaler lokaler Fehlerschätzer
		\State $m_i^{l}:=\begin{cases} 1 &, \text{falls $\eta_i^{(l)}>\theta \cdot \eta_{\max}^{(l)}$ (soll verfeinert werden)}\\ 0 &, \text{sonst}\end{cases}$
		\State $\Delta^{(l+1)}:= \Delta^{(l)} \cup \{ \frac{x_{i-1}^{(l)}+x_{i}^{(l)}}{2}$, für $i$ mit $m_i^{(l)}=1\}$
		\State $l:=l+1$
		\EndIf 
		\EndWhile  \\
		\Return $I_h^{(l)}$ (eventuell $\Delta^{(l)}, l, \eta_i^{(l)}, \eta^{(l)}$, etc. )
	\end{algorithmic}
\end{alg*}
\begin{note*}
	Alternativen:
	\begin{itemize}
		\item andere Markierungsstrategien:
			\begin{itemize}
				\item Wähle $k\in \N$, markiere in jeder Iteration die $k$ Intervalle mit $\max \eta_i^{(l)}$
				\item Wähle $\theta$ - Anteil der Intervalle zur Verfeinerung
				\item Verwende ein Extrapolationsverfahren der $\{I_n(f, K_i^{(l)})\}$ um den erwarteten Fehlerabfall vorherzusagen $\rightarrow$ siehe Stoer-Bulirsch.
		\end{itemize}
	\item Art der Verfeinerung ist austauschbar (andere/mehr Zwischenpunkte)
	\item Abbruchkriterium ist modifizierbar:
		\begin{itemize}
			\item Wähle $\sigma\in (0,1)$, Abbruch falls Änderung zu gering:
				\[
					|I_h(f,\Delta^{(l)})-I_{h}(f, \Delta^{(l-1)} )|\le \sigma \epsilon_{tol}
				\]
			\item Falls $\eta_i^{(l)}$ durch verbesserte Quadratur $I_{\hat n}$ berechnet werden, kann man auch $\hat I_h(f, \Delta^{(i)}):=\sum_{i=1}^{n_l} I_{\hat n} (f, K_i^{(n)})$ als bessere Quadratur zurückgeben.
		\end{itemize}
\end{itemize}
\end{note*}
\begin{note*}[Effizienz und Zuverlässigkeit]
	\begin{itemize}
		\item Sei $E_n(f, \Delta^{(l)}):= \sum_{i=1}^{n_l} |I_n(f,K_i^{(l)}) - \int_{K_i^{(l)}} f(x)\, dx|$ Summe der lokalen Integrationsfehler.
		\item Wir nennen einen Fehlerschätzer $\eta^{(l)}$ \emph{zuverlässig}, falls er (bis auf Faktor) obere Schranke für Fehler ist, das heißt es existiert $\gamma >0$ unabhängig von $l$ und $\Delta^{(l)}$ mit 
			\[
				E_n(f, \Delta^{(l)})\le \gamma \cdot \eta^{(l)}
			\]
		\item Wir nennen $\eta^{(l)}$ \emph{effizient}, falls er den Fehler nicht beliebig überschätzt, das heißt es existiert $T>0$ unabhängig von $l$ und $\Delta^{(l)}$ mit 
			\[
				\eta^{(l)}\le T \cdot E_n(f, \Delta^{(l)})
			\]
	\end{itemize}
	\fixme[Ab und zu n und h verwechselt]
\end{note*}

\begin{st}[Effizienz und Zuverlässigkeit der $\eta_i^{(l)}$]
	\label{2.37}
	Sei $\eta^{(l)} = \sum_{i=1}^{n_l} \eta_v^{(l)}$, $\eta_i^{(l)} = | I_n(f, K_i^{(l)}) - I_{\hat n}(f, K_i^{(l)})|$ und
	\[
		R_n (f, K_i^{(l)}) := I_n(f,K_i^{(l)}) - \int_{K_i^{(l)}} f(x) dx
	\]
	als lokales Fehlerfunktional (analog $R_{\hat n}$ für $I_{\hat n}$).
	Sei außerdem $h_0 := \max |K_i^{(0)}|$.

	Falls $q \in (0,1)$ existiert, so dass
	\[
		| R_{\hat n}(f,K)| \le q |R_n(f,K)|
		\qquad \forall K\subset [a,b], |K| \le h_0,
	\]
	so ist $\eta^{(l)}$ zuverlässiger und effizienter Schätzer für $E_h(f,\Delta^{(l)})$.
	\begin{proof}
		Es gilt
		\begin{align*}
			|R_n(f,K_i^{(l)})|
			&\le \Big|I_n(f,K_i^{(l)}) - I_{\hat n}(f,K_i^{(l)}) \Big| + \l|I_{\hat n}(f,K_i^{(l)}) - \int_{K_i^{(l)}}f(x) dx \r| \\
			&= \eta_i^{(l)} + |R_{\hat n}(f,K_i^{(l)}) | \\
			&\le \eta_i^{(l)} + q |R_n(f,K_i^{(l)})|
		\end{align*}
		Also ist
		\[
			\l|R_n(f,K_i^{(l)})\r| \le \f 1{1-q} \eta_i^{(l)}
		\]
		und damit
		\begin{align*}
			E_h(f, K_i^{(l)}) = \sum_{i=1}^{n_{l}} \l|R_n(f, K_i^{(l)}) \r| \le \f 1{1-q} \sum_{i=1}^{n_{(l)}} \eta_i^{(l)} = \f 1{1-q} \eta^{(l)}
		\end{align*}
		also zuverlässig mit $\gamma = \f 1{1-q}$.

		Für die Effizienz gilt
		\begin{align*}
			\l|R_n(f, K_i^{(l)}) \r| 
			&\ge \eta_i^{(l)} - q \l| R_n(f, K_i^{(l)}) \r| \\
		\end{align*}
		Also ist
		\[
			\l|R_n(f, K_i^{(l)})\r| \ge \f 1{1+q} \eta_i^{(l)}
		\]
		und
		\[
			\eta^{(l)} = \sum_{i=1}^{n_{l}}{\eta_i^{(l)}} \le (1+q) \sum_{n=1}^{n_{l}} | R_n(f, K_i^{(l)}) | = (1+q) E_h
		\]
		also effizient mit $\gamma = 1+q$.
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				Falls die adaptive Quadratur abbricht mit $\eta^{(l)} \le \eps_{\text{tol}}$ und die Vorraussetzungen von \ref{2.37} erfüllt sind, dann ist der Integrationsfehler garantiert klein
				\[
					\Big| I_h(f,\Delta^{(l)}) - \int_a^b f(x) dx \big|
					\le \sum_{i=1}^{n_{l}} \Big| I_n(f,K_i^{(l)}) - \int_{K_i^{(l)}} f(x) dx \Big|
					= E_h(f,\Delta^{(l)})
					\le \f 1{1-q} \eta^{(l)}
					\le \f 1{1-q} \eps_{\text{tol}}
				\]
			\item
				Man nennt die Bedingung an die $R_{\hat n, K}$ auch „Saturierungsbedingung“, welche meistens durch genügend feine Initialzerlegung erreicht werden kann.

				Sei z.B. $I_n$ die Trapez- und $I_{\hat n}$  die Simpsonregel.
				Nach der Peano-Fehlerdarstellung gilt für $f\in C^4([a,b])$:
				\begin{align*}
					|R_{\hat n}(f, K)| &= \f {|K|^5}{2880} |f^{(4)}(\hat \xi)| \\
					|R_{n}(f, K)| &= \f {|K|^3}{12} |f^{(2)}(\xi)|
				\end{align*}
				also für $f^{(2)}(\xi) \neq 0$:
				\begin{align*}
					|R_{\hat n}(f,K)| = \f {|K|^2}{240} \cdot \f {|f^{(4)}(\hat \xi)}{|f^{(2)}(\xi)|} |R_n(f,K)|
				\end{align*}
				Sei $m := \min |f''(x)| > 0$, $M := \|f^{(4)}\|_\infty$.
				Wähle $\Delta^{(0)}$ so fein, dass $h_0^2 \le \f {120 m}M$.
				Wegen $|K| \le h_0$ folgt
				\[
					\big| R_{\hat n}(f, K) \big| 
					\le \f {120m}M \cdot \f{M}{240m} |R_n(f,K)|
					= \f 12 |R_n(f,K)|
				\]
				also ist die Saturierungsbedingung mit $q=\f 12$ erfüllt.
		\end{itemize}
	\end{note}
\end{st}

\section{Monte-Carlo-Verfahren}

Wir wählen die Stützpunkte für eine Quadratur nicht als Nullstellen von Polynomen, sondern zufällig.
Die Gewichte wählen wir dabei konstant.

\begin{df}[Monte-Carlo-Approximation]
	\label{2.38}	
	Sei $f:[0,1]^d \to \R$ integrierbar und $n\in \N$.
	Wähle $\{x_i\}_{i=1}^n$ zufällig gemäß einer Gleichverteilung.
	Dann ist
	\[
		\f 1n \sum_{i=1}^n f(x_i)
	\]
	eine Monte-Carlo-Approximation für
	\[
		\int_{[0,1]^d} f(x) dx
	\]
\end{df}

\begin{nt*}
	\begin{itemize}
		\item
			In der Praxis werden die $x_i$ durch einen Zufallszahlen-Generator erzeugt.
	\end{itemize}
\end{nt*}

\subsection{Motivation aus der Stochastik}

(ohne Begriffserläuterungen und Definitionen, für Details siehe z.B. R.E. Caflisch: “Monte-Carlo and quasi-Monte-Carlo Methods”, Acta Numerica, 7:1-49, 1998)

\begin{itemize}
	\item
		Sei $X$ gleichverteilte Zufallsvariable mit Werten in $[0,1]^d$.
		Dann ist $f(X)$ eine Zufallsvariable und der \emph{Erwartungswert} von $f(X)$ ist
		\[
			E[f(X)] := \int_{[0,1]^d} f(x) dx =: I
		\]

		Der sogenannte \emph{empirische Erwartungswert} ist
		\[
			\f 1n \sum_{i=1}^n f(x_i)
		\]
		zu Realisierungen $\{x_i\}$ von $X$ und stellt eine Näherung für $I$ dar.
	\item
		Die Güte und Konvergenz der Monte-Carlo-Approximation muss ebenfalls mit Mitteln der Stochastik bewertet werden.
	\item
		Seien $X_1, \dotsc, X_n$ unabhängig gleichverteilte Zufallsvariablen mit Werten in $[0,1]$.
		Dann ist
		\[
			I_n := \f 1n \sum_{i=1}^n f(X_i)
		\]
		wieder eine Zufallsvariable mit Erwartungswert
		\[
			E[I_n] := E\l[\f 1n \sum_{i=1}^n f(X_i)\r] = \f 1n \sum_{i=1}^n E[f(X_i)] = \f 1n \sum_{i=1}^n I = I
		\]
	\item
		Man kann auch zeigen, dass für die quadratische Abweichung vom Erwartungswert (auch \emph{Varianz} genannt) gilt:
		\[
			\Var(I_n) := E[(I_n-E[I_n])^2] = \f 1n \Var(f(X)) = \f 1n \int_{[0,1]^d} (f(x) - I)^2 dx = \f 1n \sigma_f^2
		\]
		Also gilt für die Standardabweichung (Wurzel aus der Varianz):
		\[
			\sigma_{I_n} = \sqrt{\Var(I_n)} = n^{-\f 12} \sigma_f
		\]
	\item
		Seien ${x_i^{(k)}}_{i=1}^n$ für $k=1,\dotsc, k_{\text{max}}$ Realisierungen der Zufallsvariablen $\{X_i\}_{i=1}^n$ mit $k_{\text{max}}$ groß.
		Dann erhalten wir $k_{\text{max}}$ viele Realisierungen $I_n^{(k)}\in \R$ der Monte-Carlo-Approximation für $I$.

		Der \emph{empirische Erwartungswert} der $I_n^{(k)}$ ist ungefähr $I$
		\[
			\f 1{k_{\text{max}}} \sum_{k=1}^{k_{\text{max}}} I_n^{(k)} \approx I
		\]
		und die empirische Varianz von $I_n$ ist in etwa $\Var(I_n)$
		\[
			\f 1{k_{\text{max}}} \sum_{k=1}^{k_{\text{max}}}(I_n^{(k)} - I)^2 \approx \Var(I_n) = n^{-1} \sigma_f^2
		\]
		Dies ist ein Maß für die Streuung der Werte $I_n^{(k)}$ um $I$.
	\item
		Die Konvergenzrate $\sigma_{I_n} \in O(n^{-\f 12})$ nennt man „algebraische Konvergenzrate“.
		Diese ist recht langsam und ein Nachteil der Monte-Carlo-Approximation.
		
		Beispielsweise erfordert die Verbesserung um Faktor 10 das Erhöhen von $n$ um Faktor 100.
	\item
		Der große Vorteil ist, dass die Konvergenzrate $\sigma_{I_n}$ unabhängig von der Dimension $d$ ist.
\end{itemize}

\begin{nt*}[Anwendungen]
	\begin{itemize}
		\item
			Zur Flächen- und Volumenberechnung:

			Sei $D\subset [0,1]^d$, dann ist das Volumen von $D$ approximiert durch
			\[
				|D| = \int_{[0,1]^d} \1_d(x) dx \approx \f 1n \sum_{i=1}^n \1_D (x_i)
			\]
			also durch die Monte-Carlo-Approximation für das Integral von $f(x) := \1_D(x)$.
		\item
			Hochdimensionale Integration:

			Für Integrale im $\R^d$ können Newton-Cotes-, Gauß- und zusammengesetzte Quadraturen verallgemeinert werden (Koordinatenweise Quadratur).
			Für hohe $d$ ist das jedoch inpraktikabel, da für $n$ Intervalle pro Achse (also $n+1$ Punkte pro Achse) $(n+1)^d$ viele Quadraturpunkte benötig werden.
			Die Anzahl der Stützstellen wächst also exponentiell in $d$ (“Curse of Dimensionality”).

			Im Gegensatz dazu ist die Monte-Carlo-Approximation in $\R^d$ einfach und bildet oft die einzige Möglichkeit, hochdimensionale Integrale hinreichend genau zu berechnen (“Breaking the curse of dimensionality”).
	\end{itemize}		
\end{nt*}

\begin{ex*}[Berechnung von $\pi$]
	Sei $D = K_{\f 12}(\f 12, \f 12)$, dann ist $|D| = \f \pi 4$.
	Wir berechnen das jetzt mit dem Monte-Carlo-Algorithmus.
	\[
		|D| = \int_{[0,1]^2} \1_D = I
	\]
	Für die Varianz gilt
	\[
		\Var(\1_D(X)) = \int_{[0,1]}(f(x) - \tfrac \pi 4)^2 dx = (1-\tfrac \pi 4) \f \pi 4
	\]
	\begin{figure}[H]
		\centering
		\caption{Beispiel für $k_\text{max}=200, n=1,\dotsc, 500$}
		\fixme[Bilder zur Verteilung siehe Programme im Ilias]
	\end{figure}
\end{ex*}


\chapter{Nichtlineare Systeme}

\begin{itemize}
	\item
		Sei $F:\R^n \to \R^n$ eine Funktion mit Nullstelle $x^*$, $F(x^*) = 0 \in \R^n$.
		Gesucht ist ein numerisches Verfahren zur Berechnung einer Sequenz $(x^{(k)})_{n\in \N} \subset \R^n$ mit $\lim_{k\to \infty} x^{(k)} = x^*$.
	\item
		Insbesondere die Lösung für nichtlineare $F$ ist von Interesse, z.B. eine skalare Gleichung mit $n=1$:
		\[
			\qquad F(x) = x^2 - a = 0, \qquad a\in \R^+
		\]
		oder ein System mit $n=2$:
		\[
			F \begin{pmatrix}
				x \\ y
			\end{pmatrix}
			:= \begin{pmatrix}
				x^2 - e^{-xy} - 1 \\
				xy + \sin(xy)
			\end{pmatrix}
			= \begin{pmatrix}
				0 \\ 0
			\end{pmatrix}
		\]
		Falls $F(x) = Ax - b$ mit regulärer Matrix $A$, so stellen iterative LGS-Löser (Jacobi-, Gauß-Seidel-Verfahren, etc.) eine Lösungsmöglichkeit dar, siehe dazu Vorlesung Numerische Lineare Algebra.
	\item
		Die Eindeutigkeit von $x^*$ wird nicht gefordert. 
		Es ist auch von Interesse, mehrere Nullstellen zu finden.
	\item
		Viele der zu behandelnden Methoden sind Iterationsverfahren.
		Zu gegebenem $x^{(0)} \in \R^n$ definiere $x^{(k+1)} = \Phi(x^{(k)})$ mit Iterationssfunktion $\Phi : \R^n \to \R^n$.
	\item
		Wie kann $\Phi$ für verschieden glatte $F$ gewählt werden?
		Für welche $x_0$ kann Konvergenz garantiert werden?
		Wie schnell sinkt der Fehler?
\end{itemize}


\section{Elementare Verfahren im Eindimensionalen}


Man erinnere sich an den Mittelwertsatz aus Analysis 1:

Sei $f$ stetig auf $[a,b]$, $f(a) < 0, f(b) > 0$, dann existiert $x^* \in (a,b)$ mit $f(x^*)=0$.

Der Beweis nutzte die Intervallschachtelung.

\subsection{Das Bisektionsverfahren}

\begin{df}[Bisektionsverfahren] \label{3.1}
	Sei $f \in C^0([a,b])$ und $f(a) \cdot f(b) < 0$.
	Wir definieren eine Folge von Intervalle $[a_k, b_k]$ und iterieren $x^{(k)}$ durch
	\begin{align*}
		[a_0, b_0] &:= [a,b] \\
		x^{(k)} &:= \f {a_k + b_k}2
	\end{align*}
	und
	\begin{align*}
		[a_{k+1}, b_{k+1}] = \begin{cases}
			[a_k, x^{(k)}] & \text{falls } f(a_k) \cdot f(x^{(k)}) < 0 \\
			[x^{(k)}, b_k] & \text{falls } f(a_k) \cdot f(x^{(k)}) \ge 0
		\end{cases}
	\end{align*}
\end{df}

\begin{st}[Konvergenz des Bisektionsverfahrens] \label{3.2}
	Das Bisektionsverfahren konvergiert gegen eine Nullstelle $x^*$ von $f$ und
	\[
		|x^{(k)} - x^*| \le \f 1{2^{k+1}} |b-a|
	\]
	\begin{proof}
		Sei ohne Beschränkung der Allgemeinheit $f(a) < 0$ und $f(b) > 0$.
		Es gilt nach Konstruktion
		\begin{align*}
			|b_k - a_k| &= \f 1{2^k} |b-a| \\
		\end{align*}
		Außerdem sind $(a_k)$ monoton wachsend und $(b_k)$ monoton fallend und $x^{(k)} \in [a_k,b_k]$.
		Damit konvergieren $(a_k)$ und $(b_k)$ und die Grenzwerte stimmen wegen obiger Gleichung überein (bewegen sich beliebig nah aufeinander zu):
		\[
			\lim_{k\to \infty} a_k = \lim_{k\to \infty} b_k = \lim_{k\to \infty} x^{(k)} =: x^*
		\]
		$x^*$ ist eine Nullstelle, denn $f(a_k) \le 0$ und $f(b_k) \ge 0$.
		Aus der Stetigkeit von $f$ folgt
		\begin{align*}
			f(x^*) &= f\big( \lim_{k=\to \infty}a_k \big) \le 0 \\
			f(x^*) &= f\big( \lim_{k=\to \infty}b_k \big) \ge 0
		\end{align*}
		also $f(x^*) = 0$.

		Weiter sind $x^{(k)}, x^* \in [a_k, b_k]$ und
		\begin{align*}
			|x^{(k)} - x^*| = \Big| \f {a_k + b_k}2 - x^* \Big| \le \f 12 |b_k-a_k| = \f {|b-a|}{2^{k+1}}
		\end{align*}
	\end{proof}
\end{st}

\begin{nt*}
	\begin{itemize}
		\item
			In der Praxis wird abgebrochen, falls $k \ge k_{\text{max}}$ für ein $k_{\text{max}}$ so groß, dass
			\[
				|b_k -a_k| \le \eps_{\text{tol}}
				\qquad
				\text{oder}
				\qquad
				|f(x^{(k)})| < \eps_{\text{tol}}
			\]
			oder eine ähnliche Abbruchbedingung.
		\item
			Das Verfahren ist sehr robust, es kann nicht divergieren.
			Trotzdem ist es sehr langsam: man braucht etwa 3 Iterationen, um eine Dezimalstelle an Genauigkeit zu gewinnen.
		\item
			Sei $f(x) = x^2 - 2$, $a=0, b=2$
			\begin{table}[H]
				\centering
				\begin{tabular}{c|l|l|l}
					$k$ & $a_k$ & $b_k$ & $x^{(k)}$ \\ \hline
					0 & 0 & 2 & 1 \\
					1 & 1 & 2 & 1.5 \\
					2 & 1 & 1.5 & 1.25 \\
					3 & 1.25 & 1.5 & 1.375 \\
					4 & 1.375 & 1.5 & 1.4375 
				\end{tabular}
			\end{table}
		\item
			Der Fehlerabfall muss nicht monoton sein!
		\item
			Das Verfahren wird in der Regel eingesetzt, um grob ein Intervall $[a_k,b_k]$ zu bestimmen, in dem eine Nullstelle liegt.
			Zur genaueren Berechnung verwendet man dann schneller konvergierende Verfahren.
		\item
			Das Verfahren setzt keine Differenzierbarkeit voraus.
	\end{itemize}
\end{nt*}

\subsection{Das Newton-Verfahren}

\begin{df}[Newton-Verfahren] \label{3.3}
	Sei $f \in C^1([a,b])$ und $x^{(0)} \in [a,b]$.
	Wir definieren
	\[
		x^{(k+1)} := x^{(k)} - \f {f(x^{(k)})}{f'(x^{(k)})}
		\qquad k\in \N
	\]
	\begin{note}
		\begin{itemize}
			\item
				Die Definition ist nur sinnvoll, falls $f'(x^{(k)}) \neq 0$.
				Hinreichende Bedingungen hierfür später.
			\item
				Betrachte die Taylor-Entwicklung als Motivation:
				Sei $x^{(k)}$ eine Approximation von $x^*$ und $h:= x^* - x^{(k)}$ klein.
				Dann gilt
				\[
					0 
					= f(x^*) 
					= f(x^{(k)}) + h \cdot f'(x^{(k)}) + o(h^2)
					= f(x^{(k)}) + h \cdot f'(x^{(k)})
				\]
				Wir vernachlässigen also Terme höherer Ordnung und erhalten
				\[
					h = - \f {f(x^{(k)})}{f'(x^{(k)})}
				\]
				Daher ist es sinnvoll
				\[
					x^{(k+1)} := x^{(k)} + h = x^{(k)} - \f {f(x^{(k)})}{f'(x^{(k)})}
				\]
			\item
				Man kann das Verfahren auch geometrisch Interpretieren:
				wir wählen $x^{(k+1)}$ nämlich als Nullstelle der Tangente $t_{k+1}$ im Punkt $(x^{(k)}, f(x^{(k)})$.
				\[
					t_{k+1}(x) = f'(x^{(k)}) (x-x^{(k)}) + f(x^{(k)}) \stackrel != 0
				\]
				und damit
				\[
					x = x^{(k)} - \f {f(x^{(k)})}{f'(x^{(k)})}
				\]
				was die Iteration nahelegt.
			\item
				Das Verfahren benötigt pro Iteration eine Auswertung von $f$ und eine Auswertung von $f'$.
		\end{itemize}
	\end{note}
\end{df}

\begin{ex*}
	Sei $a \ge 0$ und $f(x) = x^2 - a$, also $f'(x) = 2x$.
	Wähle $x^{(a)} > 0$, dann ist
	\[
		x^{(k+1)} = x^{(t)} - \f {(x^{(k)})^2 - a}{2 x^{(k)}} = \f 12 (x^{(k)} + \tfrac a{x^{(k)}})
	\]
	$x^{(k)}$ approximiert dann $\sqrt a$

	\begin{table}[H]
		\centering
		\caption{$a=2, x^{(0)}=2$}
		\begin{tabular}{c|c|c}
			$k$ & $x^{(k)}$ & $x^{(k)} - \sqrt 2$ \\ \hline
			0 & 2.00000 & $5.8579 \cdot 10^{-1}$ \\
			1 & 1.50000 & $8.5786 \cdot 10^{-2}$ \\
			2 & 1.41667 & $2.4531 \cdot 10^{-3}$ \\
			3 & 1.41422 & $2.139 \cdot 10^{-6}$ \\
			4 & 1.41421 & $1.5947 \cdot 10^{-12}$ 
		\end{tabular}
	\end{table}

	Es ergibt sich also eine sehr schnelle Konvergenz, fast eine Verdopplung der gültigen Nachkommastellen pro Iteration.
	Die Iterationsvorschrift ist gerade das Heron'sche Verfahren zur Wurzelberechnung von $a$.

	Falls $a,x^{(0)} \in \Q, \sqrt{a} \not \in \Q$, dann sind die $x^{(k)}$ eine rationale Approximation für irrationale $\sqrt a$.
\end{ex*}

\begin{ex*}[Verschiedenes Konvergenzverhalten]
	\begin{enumerate}[i)]
		\item
			Für $f(x) = x^2 - 2$ konvergiert das Verfahren für alle $x^{(0)} > 0$.
		\item
			Sei $f(x)$ ungerade in ein Parallelogramm einbeschrieben (\fixme[Zeichnen]).
			Man erhält dann eine periodische Sequenz mit konvergenten Teilfolgen, jedoch keine Nullstellen.
		\item
			Sei $f(x)$ ungerade und $f'(x) < \f {f(x)}{2x}$ für alle $x \ge x^{(0)} > 0$, dann divergiert das Verfahren.
		\item
			Sei $f(x) = \sin( 8 \pi x) + \f 12 x$.
			Man erhält durchaus Konvergenz, aber nicht unbedingt zur nächsten Nullstelle von $x^{(0)}$, sondern zu einer sehr weit entfernten.
	\end{enumerate}
\end{ex*}


\begin{st}[Konvergenz des Newton-Verfahrens] \label{3.4}
	Sei $f \in C^2([a,b])$ und $x^* \in (a,b)$ mit $f(x^*) = 0$.
	Sei weiterhin
	\begin{align*}
		m &:= \min_{x\in [a,b]}|f'(x)| > 0 \\
		M &:= \max_{x\in [a,b]}|f''(x)|
	\end{align*}
	und $\delta > 0$ genügend klein gewählt, so dass $B_\delta (x^*) \in [a,b]$ und $q := \f M{2m} \delta < 1$.

	Dann ist für alle Startwerte $x^{(0)} \in B_\delta(x^*)$ die Sequenz der Newton-Iterierten $(x^{(k)})_{k\in \N}$ wohldefiniert, $x^{(k)} \in B_\delta(x^*)$ und es gelten folgende Aussagen:
	\begin{enumerate}[i)]
		\item
			Die a-priori Fehlerabschätzung:
			\[
				\big|x^{(k)} - x^*\big| 
				\le \f M{2m} \Big|x^{(k-1)} - x^*\Big|^2 
				\le \f {2m}M q^{2^k}
				\qquad k\ge 1
			\]
		\item
			Die a-posteriori Fehlerschranke:
			\[
				\big|x^{(k)} - x^*\Big| 
				\le \f 1m \Big|f\big(x^{(k)}\big)\Big| 
				\le \f M{2m} \Big|x^{(k)} - x^{(k-1)}\Big|^2
				\qquad k\ge 1
			\]
		\item
			Das Verfahren konvergiert gegen $x^*$:
			\[
				\lim_{k\to \infty} x^{(k)} = x^*
			\]
	\end{enumerate}
	\begin{note}
		\begin{itemize}
			\item
				Man spricht von \emph{lokaler Konvergenz} des Newton-Verfahrens (nur in einer hinreichend kleinen Umgebung um $x^*$).
			\item
				Man muss $x^{(0)}$ also bereits als gute Annäherung der Nullstelle $x^*$ wählen (z.B. aus einigen Iterationen des Bisektionsverfahrens).
			\item
				Der Satz stellt einige Anforderungen an die Funktion.
				Wegen des Mittelwertsatzes ist
				\[
					\f {|f(x) - f(y)|}{|x-y|} = |f'(\xi)| \ge m > 0
					\qquad \forall x,y \in [a,b], x \neq y
				\]
				Damit ist $f(x) \neq f(y)$ also $f$ injektiv und $x^*$ ist die einzige Nullstelle in dem Intervall.
				Außerdem ist $x^*$ einfache Nullstelle, da $f'(x^*) \neq 0$.
			\item
				Falls $f$ nicht injektiv und $f'(x^*) \neq 0$, kann man $[\_a, \_b]$ als kleineres Teilintervall um $x^*$ wählen, sodass $f$ injektiv auf $[\_a,\_b]$ ist, womit der Satz wieder anwendbar wird.
			\item
				Falls $x^*$ mehrfache Nullstelle ist ($f'(x^*)=0$), dann ist die Konvergenz immer noch gewährleistet, aber langsamer (siehe Satz \ref{3.15}).
		\end{itemize}
		\begin{proof}
			Die Taylor-Entwicklung von $f$ an $x$ liefert
			\begin{align*}
				f(x) = f(x_0) + f'(x_0)(x-x_0) + R(x,x_0)
			\end{align*}
			mit Integralrestglied
			\begin{align*}
				R(x,x_0) 
				&= \int_{x_0}^x f''(t) (x-t) dt \\
				&= \int_0^1 f''\Big(x_0+s(x-x_0)\Big)  \Big(x-x_0-s(x-x_0)\Big)(x-x_0) ds \\
				&= (x-x_0)^2 \int_0^1 \underbrace{f''\big(x_0+s(x-x_0)\big)}_{\le M} (1-s) ds
			\end{align*}
			Also
			\[
				|R(x,x_0) | \le (x-x_0)^2 M \int_0^1 (1-s) ds = \f M2 (x-x_0)^2
			\]
			Für die Newtoniteration $\Phi(x) := x - \f {f(x)}{f'(x)}$ gilt
			\begin{align*}
				|\Phi(x) - x^*|
				&= \Big|x-x^* - \f {f(x)}{f'(x)}\Big| \\
				&= \Big| -\f 1{f'(x)} \big(f(x) + (x-x^*)f'(x) \big) \Big| \\
			\intertext{
				Zusammen mit der Taylorentwicklung für $f(x)$ an $x^*$ ergibt sich
			}
				&= \Big| -\f 1{f'(x)} \Big(f(x^*) + f'(x^*)(x-x^*) + R(x,x^*) + (x-x^*)f'(x)\Big) \Big| \\
				&= \Big| \f 1{f'(x)} R(x,x^*) \Big| 
				\le \f 1m |R(x,x^*)| 
				\le \f M{2m} (x^* - x)^2
			\end{align*}
			Also gilt für alle $x\in B_\delta(x^*)$:
			\[
				|\Phi(x) - x^*| \le \f M{2m}(\underbrace{x^*-x}_{|\cdot|<\delta})^2 < \underbrace{\f M{2m} \delta}_{=q<1} \cdot \delta < \delta
			\]
			und damit $\Phi(x) \in B_\delta(x^*)$.

			\begin{enumerate}[i)]
				\item
					Mit obiger Abschätzung folgt
					\begin{align*}
						|x^{(k)}-x^*| = |\Phi(x^{(k-1)}) - x^*| \le \f M{2m} \big|x^{(k-1)} - x^*\big|^2
					\end{align*}
					Setze $\rho^{(k)} = \f M{2m}|x^{(k)}-x^*|$, damit ist
					\begin{align*}
						\rho^{(k)} 
						&= \f M{2m} \big| \Phi(x^{(k-1)}) - x^*\big| \\
						&\le \f M{2m} \f M{2m} |x^{(k-1)} - x^*|^2 
						= (\rho^{(k-1)})^2 
						\le \dotsb \le
						(\rho^{(0)})^{2^k}
					\end{align*}
					Damit folgt die zweite Ungleichung:
					\begin{align*}
						|x^{(k)} - x^*|
						= \f {2m}M \rho^{(k)}
						\le \f {2m}M (\rho^{(0)})^{2^k}
						= \f {2m}M \Big( \f M{2m} \underbrace{|x^{(0)}-x^*|}_{< \delta} \Big)^{2^k}
						< \f {2m}M q^{2^k}
					\end{align*}
				\item
					Die Taylorentwicklung von $f$ an $x_0 = x^{(k-1)}$ liefert
					\begin{align*}
						f(x^{(k)}) = \underbrace{f(x^{(k-1)}) + f'(x^{(k-1)})\big(x^{(k)}-x^{(k-1)}\big)}_{=0 \text{ (da Newtoniteration)}} + R(x^{(k)},x^{(k-1)}) = R(x^{(k)},x^{(k-1)})
					\end{align*}
					Der Mittelwertsatz liefert
					\begin{align*}
						|f(x^{(k)}) -f(x^*)| = \big|f'(\xi)(x^{(k)}-x^*)\big| \ge m|x^{(k)} -x^*|
					\end{align*}
					also ergibt sich mit der Erkenntnis aus der Taylorentwicklung
					\begin{align*}
						|x^{(k)} -x^*|
						\le \f 1m \Big|f(x^{(k)}) - \underbrace{f(x^*)}_{=0}\Big|
						= \f 1m \Big|R\big(x^{(k)}, x^{(k-1)}\big) \Big|
						\le \f M{2m} \Big(x^{(k)} -x^{(k-1)}\Big)^2
					\end{align*}
				\item
					Weil $q<1$ folgt mit i)
					\begin{align*}
						\lim_{k\to \infty} |x^{(k)} -x^*| < \lim_{k\to \infty} \f {2m}M q^{2^k} = 0
					\end{align*}
			\end{enumerate}
		\end{proof}
	\end{note}
\end{st}

\subsection{Das Sekantenverfahren}

\begin{df}[Sekantenverfahren] \label{3.5}
	Sei $f:[a,b] \to \R$ gegeben mit Startwerten $x^{(0)}, x^{(1)} \in [a,b]$, $x^{(0)} \neq x^{(1)}$.
	Wir definieren
	\[
		x^{(k+1)} = x^{(k)} - \f {x^{(k-1)} - x^{(k)}}{f(x^{(k-1)}) - f(x^{(k)})} f(x^{(k)})
		\qquad k\in \N
	\]
	\begin{note}
		\begin{itemize}
			\item
				Das Verfahren ist nur sinnvoll falls $f(x^{(k)}) \neq f(x^{(k-1)})$ für alle $k$
			\item
				Das Verfahren entspricht dem Newton-Verfahren mit Differenzenquotienten anstatt $f'(x^{(k)})$.
			\item
				Es ist auch eine geometrische Interpretation möglich:
				$x^{(k+1)}$ ist nämlich die Nullstelle der Sekante $s(x)$ von $f$ durch die Punkte ($x^{(k-1)}, f(x^{(k-1)})$) und ($x^{(k)},f(x^{(k)})$).
				\begin{align*}
					0 \stack != s(x) = f(x^{(k)}) + \f {f(x^{(k)})-f(x^{(k-1)})}{x^{(k)}-x^{(k-1)}} (x-x^{(k)})
				\end{align*}
				also
				\begin{align*}
					x-x^{(k)} &= - \f {x^{(k)}-x^{(k-1)}}{f(x^{(k)}) - f(x^{(k-1)})} f(x^{(k)}) \\
					\iff \qquad
					x &= x^{(k)} - \f {x^{(k)}-x^{(k-1)}}{f(x^{(k)}) - f(x^{(k-1)})} f(x^{(k)})
				\end{align*}
			\item
				Das Verfahren braucht pro Schritt eine Funktionsauswertung (man speichert die aus dem letzten Schritt).
			\item
				Das Verfahren ist auch auf nicht-differenzierbare Funktionen anwendbar.
		\end{itemize}
	\end{note}
\end{df}

\begin{st}[Konvergenz des Sekantenverfahrens] \label{3.6}
	Sei $f\in C^2([a,b])$ und $x^* \in (a,b)$ mit $f(x^*) = 0$.
	Außerdem $m := \min_{x \in [a,b]}|f'(x)| > 0$ und $M := \max_{x\in[a,b]}|f''(x)|$.

	Sei $\delta > 0$ klein genug, so dass $q := \f M{2m} \delta < 1$.
	Seien weiterhin $x^{(0)}, x^{(1)} \in B_\delta(x^*)$ mit $x^{(0)} \neq x^{(1)}$.
	Dann ist die Folge $(x^{(k)})_{k \in \N}$ des Sekantenverfahrens wohldefiniert mit $x^{(k)} \in B_\delta(x^*)$ und es gelten die Aussagen:
	\begin{enumerate}[i)]
		\item
			Die a-priori Fehlerschranke:
			\[
				|x^{(k)} -x^*| \le \f {2m}M q^{\gamma_k}
			\]
			wobei $(\gamma_k)_{k\in \N}$ die Folge der Fibonacci-Zahlen ist, d.h. $\gamma_0 := \gamma_1 := 1$ und $\gamma_{k+1} := \gamma_k + \gamma_{k-1}$.
		\item
			Die a-posteriori Fehlerschranke:
			\[
				|x^{(k)}-x^*| \le \f 1m \big| f(x^{(k)}) | \le \f M{2m} |x^{(k)} - x^{(k-1)}| \cdot |x^{(k)}-x^{(k-2)}|
			\]
		\item
			Konvergenz
			\[
				\lim_{k \to \infty} x^{(k)} = x^*
			\]
	\end{enumerate}
	\begin{proof}
		Übungsaufgabe
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				Ein Problem beim Sekantenverfahren sind Rundungsfehler, falls $f(x^{(k)})$ und $f(x^{(k-1)})$ sehr nahe beieinander liegen.
			\item
				Es liegt wie beim Newtonverfahren wieder lokale Konvergenz vor.
			\item
				Man kann zeigen, dass
				\[
					|x^{(k)} - x^*| \le C |x^{(k-1)} - x^*|^\phi
				\]
				mit $C$ unabhängig von $k$ und dem goldenen Schnitt $\phi = \f {1+\sqrt 5}2 \approx 1.618$.
		\end{itemize}
	\end{note}
\end{st}

\subsection{Vergleich}

\begin{nt*}[Vergleich der Verfahren]
	\begin{itemize}
		\item
			Bezüglich der Iterationszahl:
			\begin{alignat*}{3}
				\text{Bisektion:}&\qquad& |x^{(k)} -x^*| &\le \f {(b-a)}2 (\tf 12)^k \\
				\text{Newton-Verfahren:}&\qquad& |x^{(k)} -x^*| &\le \f {2m}M q^{2^k} \\
				\text{Sekantenverfahren:}&\qquad& |x^{(k)} -x^*| &\le \f {2m}M q^{\phi^k}
			\end{alignat*}
			Die Bisektion und das Sekantenverfahren benötigt eine Auswertung von $f$ pro Iteration, das Newtonverfahren eine von $f$ und eine von $f'$ pro Iteration.

			Man sieht, dass das Newtonverfahren am schnellsten bezüglich der Iterationszahl konvergiert.
		\item
			Bezüglich dem Aufwand (Auswertungen von $f$ und $f''$):

			Setze $z^{(k)} = x^{(2k)}$ beim Bisektions- und Sekantenverfahren.
			Dann ist der Aufwand für $z^{(k)}$ und $x^{(k)}$ aus dem Newtonverfahren vergleichbar.
			\begin{alignat*}{3}
				\text{Bisektion:}&\qquad& |z^{(k)} -x^*| &\le \f {(b-a)}2 (\tf 14)^k \\
				\text{Newton-Verfahren:}&\qquad& |x^{(k)} -x^*| &\le \f {2m}M q^{2^k} \\
				\text{Sekantenverfahren:}&\qquad& |z^{(k)} -x^*| &\le \f {2m}M q^{\phi^{2k}} 
			\end{alignat*}
			Bezüglich der Funktionsauswertungen konvergiert das Sekantenverfahren am schnellsten.


	\end{itemize}
\end{nt*}

\section{Nullstellensuche bei Polynomen}

Explizite Formeln zur Berechnung von Nullstellen von Polynomen existieren nur für Polynome bis Grad 4.
Für allgemeine Polynome sind daher numerische Verfahren erforderlich, z.B. das Newtonverfahren.

\begin{st}[Monotone Konvergenz für Newton-Verfahren] \label{3.7}
	Sei $p\in \P_n$ mit reeller Nullstelle $x_n$ mit $x_n \ge \Re(x_i)$ für alle anderen Nullstellen $x_i \in \C$, $i=1,\dotsc,n-1$.
	Dann liefert das Newton-Verfahren für alle Startwerte $x^{(0)} > x_n$ eine streng monoton fallende Folge $(x^{(k)})_{k\in\N}$ mit $\lim_{k\to\infty} x^{(k)} = x_n$.
	\begin{note}
		\begin{itemize}
			\item
				Dies ist wesentlich mehr als nur „lokale Konvergenz“ in einer kleinen $\delta$-Kugel um $x_n$ (vgl. \ref{3.4}):
				Für alle Anfangswerte auf einer Halbgeraden von $\R$ ist Konvergenz zu beobachten.
			\item
				Die Aussage gilt entsprechend modifiziert für die „kleinste Nullstelle“.
			\item
				Falls $x_n \not\in \R$, dann ist auch $\_{x_n}$ eine Nullstelle des reellen Polynoms $p$.
				Das Monom
				\[
					(x-x_n)(x-\_{x_n}) = x^2 - rx - q
					\qquad r,q \in \R
				\]
				ist ein Teiler von $p$.
				Mit dem \emph{Verfahren von Bairstow} bestimmt man $r,q$ derart, dass die Polynomdivision aufgeht.
				Dies ergibt $x_n$ und $\_{x_n}$ durch Lösen der quadratischen Gleichung (z.B. mit der Mitternachtsformel/p-q-Formel).
				(Details: Stoer-Bulirsch 5.8)
			\item
				Wenn man $x_n \in \R$ gefunden hat, führt eine „\emph{Deflation}“ zum Polynom $\hat p(x) := \f {p(x)}{x-x_n}$.
				Dies wird zur Suche der nächsten Nullstelle verwendet.
				Iterativ bekommen wir so „alle“ Nullstellen von $p$.
		\end{itemize}
	\end{note}
	\begin{note}
		Sei o.B.d.A. $p$ normiert, dann ist
		\[
			p(x) = \prod_{i=1}^n (x-x_i)
		\]
		und
		\[
			p'(x) = \sum_{k=1}^n \prod_{\substack{i=1 \\ i\neq k}}^n (x-x_i) = \sum_{k=1}^n \f 1{x-x_k} p(x)
		\]
		Seien $y_1,\dotsc,y_l \in \R$ die reellen Nullstellen und $z_1,\dotsc,z_m,\_{z_1}, \dotsc, \_{z_m} \in \C \setminus \R$ die komplexen Nullstellen von $p$:
		\[
			p(x) = \prod_{i=1}^l (x-y_i)\prod_{k=1}^m (x-z_k)(x-\_{z_k})
		\]
		Wir schreiben
		\begin{align*}
			p'(x)  
			&= \bigg( \sum_{i=1}^l \f 1{x-y_i} + \sum_{k=1}^m \Big(\f 1{x-z_k} + \f 1{x-\_{z_k}}\Big) \bigg) p(x) \\
			&= \bigg( \sum_{i=1}^l \f 1{x-y_i} + \sum_{k=1}^m \f {2x - 2\Re(z_k)}{(x-z_k)(x-\_{z_k})}\bigg) p(x)
		\end{align*}
		Für $x>x_n$ gilt $x\neq z_k, x\ge \Re(z_k)$, $x > y_i$ und
		\[
			(x-z_k)(x-\_{z_k}) 
			= (x-z_k)(\_{x-z_k})
			= |x-z_k|^2
			> 0
		\]
		Außerdem $2x-2\Re(z_k) \ge 0$ und $x-y_i \ge 0$.
		Also ist $p(x) > 0$ und auch $p'(x) > 0$ und
		\[
			\Phi(x) := x - \f {p(x)}{p'(x)} < x
		\]
		also ist $(x^{(k)})$ streng monoton fallend, falls $x^{(0)} > x_n$.

		Da alle Summanden in der Darstellung von $p'(x)$ nichtnegativ sind, schätzen wir alle bis auf einen nach unten durch $0$ ab:
		\[
			p'(x) \ge \f 1{x-x_n}p(x)
		\]
		Damit ist
		\[
			\f {p(x)}{p'(x)} \le (x-x_n)
			\implies
			\Phi(x) = x - \f {p(x)}{p'(x)} \ge x-(x-x_n) = x_n
		\]
		also ist $(x^{(k)})$ nach unten durch $x_n$ beschränkt und die Folge konvergiert gegen ein $x^*$ mit $x_n \le x^*$.

		$x^*$ ist Nullstelle von $p$, denn
		\[
			\Phi(x^{(k)}) = x^{(k+1)}
			\to
			\Phi(x^*) = x^*
			\qquad k\to\infty
		\]
		also $x^* = x^* - \f {p(x^*)}{p'(x^*)}$ und $p(x^*) = 0$, also $x^* \le x^n$ und damit $x^*=x_n$.
	\end{note}
\end{st}

\begin{st}[Induzierte Matrixnormen] \label{3.8}
	Zu den Vektornormen definiert für Vektoren $x=(x_i)_{i=1}^n \in \C^n$,
	\begin{align*}
		\|x\|_1 &:= \sum_{i=1}^n |x_i| \\
		\|x\|_\infty &:= \max_{i = 1,\dotsc,n} |x_i|
	\end{align*}
	sind die induzierten Matrixnormen für $A=(a_{ij}) \in \C^{n\times n}$:
	\begin{alignat*}{3}
		\|A\|_\infty &:= \sup_{x\in \C^n} \f{\|Ax\|_\infty}{\|x\|_\infty} = \max_{i=1,\dotsc,n} \sum_{j=1}^n |a_{ij}| &
			& \qquad\text{Zeilensummennorm} \\
		\|A\|_1 &:= \sup_{x\in \C^n} \f{\|Ax\|_1}{\|x\|_1} = \max_{j=1,\dotsc,n} \sum_{i=1}^n |a_{ij}| &
			& \qquad\text{Spaltensummennorm}
	\end{alignat*}
\end{st}

Mit diesem Hilfssatz lassen sich die folgenden Schranken für Nullstellen von Polynomen beweisen.
Indem man $x^{(0)}$ entsprechend größer als diese Schranken wählt, ist gemäß \ref{3.7} monotone Konvergenz gewährleistet.

\begin{st}[Nullstellenabschätzung für Polynome] \label{3.9}
	Sei $p(x) = \sum_{k=0}^n a_k x^k$ mit $a_k \in \R$ normiert ($a_n = 1$) und $x^* \in \C$ eine Nullstelle von $p$.
	Dann gilt
	\[
		|x^*| \le \min(A,B)
	\]
	mit
	\[
		A := \max\bigg(1, \sum_{k=0}^{n-1} |a_k| \bigg) \qquad
		B := \max\bigg(|a_0|, 1+\max_{k=1,\dotsc,n-1} |a_k|\bigg)
	\]
	\begin{note}
		\begin{itemize}
			\item
				Falls $p\in \P_n$ mit $a_n \neq 1$, verwendet man $\hat p(x) := \f {p(x)}{a_n}$ mit denselben Nullstellen.
			\item
				Für $p(x) = x^2-2$ ergibt sich $A=B=2$, tatsächlich ist $|\pm \sqrt2| \le 2$.
			\item
				Für weitere Schranken siehe Plato, Stoer/Bulirsch.
		\end{itemize}
	\end{note}
	\begin{proof}
		Setze
		\[
			M := \begin{pmatrix}
				0 & \cdots & 0 & -a_0 \\
				1 & \ddots & \vdots & \vdots \\
				\vdots & \ddots & 0 & \vdots \\
				0 & \cdots & 1 & -a_{n-1}
			\end{pmatrix}
			\in \R^{n\times n}
		\]
		Wir zeigen
		\[
			\det(\lambda I - M) = p(\lambda) \qquad \forall \lambda \in \R
		\]
		durch Entwicklung nach der letzten Zeile.
		\[
			\lambda I - M := \begin{pmatrix}
				\lambda & \cdots & 0 & a_0 \\
				-1 & \ddots & \vdots & \vdots \\
				\vdots & \ddots & \lambda & \vdots \\
				0 & \cdots & -1 & \lambda + a_{n-1}
			\end{pmatrix}
		\]
		Es gilt
		\begin{align*}
			\det(\lambda I - M) = (-1)^{n+n-1}(-1) \det \begin{pmatrix}
				\lambda & \cdots & \cdots & a_0 \\
				-1 & \ddots & & \vdots \\
				\vdots & \cdots & \lambda & \vdots \\
				0 & \cdots & -1 & a_{n-2}
			\end{pmatrix}
			 + (-1)^{n+n} (\lambda + a_{n-1}) \underbrace{\det (\dotsc)}_{=\lambda^{(n-1)}}
		\end{align*}
		Wegen
		\begin{align*}
			\det T_k = \det \begin{pmatrix}
				\lambda & \cdots & \cdots & a_0 \\
				-1 & \ddots &  &\vdots \\
				\vdots & \ddots & \lambda &\vdots \\
				0 & \cdots & -1 & a_k 
			\end{pmatrix}
			&= (-1)(-1) 
			\det \begin{pmatrix}
				\lambda & \cdots & \cdots & a_0 \\
				-1 & \ddots &  &\vdots \\
				\vdots & \ddots & \lambda &\vdots \\
				0 & \cdots & -1 & a_{k-1} 
			\end{pmatrix}
			+ 1 \cdot a_k 
			\det \begin{pmatrix}
				\lambda & 0 & \cdots &  0 \\
				-1 & \ddots & \ddots &  \vdots \\
				\vdots & \ddots & \ddots & 0  \\
				0 & \cdots & -1 & \lambda
			\end{pmatrix} \\
			&= \det T_{k-1} + a_k \lambda^k
		\end{align*}
		Also ist
		\[
			\det(\lambda I-M) = \underbrace{(\lambda +a_{n-1})}_{=\lambda^n + a_{n-1}\lambda^{n-1}} \lambda^{n-1} + a_{n-2} \lambda^{n-2} + a_{n-3}\lambda^{n-3} + \dotsb + a_1 \lambda + a_0 = p(\lambda)
		\]
		Also ist $x^*$ Nullstelle von $p$ genau dann, wenn $x^*$ Eigenwert von $M$ ist.

		Sei $x^*$ Eigenwert von $M$ und $v\in \C^n$ der zugehörige Eigenvektor, dann ist
		\[
			|x^*| 
			= \f {\|x^*v\|_1}{\|v\|_1} 
			= \f {\|Mv\|_1}{\|v\|_1}
			\le \sup_{y\in\C^n} \f {\|My\|_1}{\|y\|}
			= \|M\|_1
			= \max\bigg(1, \sum_{k=0}^{n-1} |a_k| \bigg)
			= A
		\]
		Analog mit der Zeilensummennorm für $B$:
		\[
			|x^*| \le \|M\|_\infty = \max\Big( |a_0|, 1 + \max_{k=1,\dotsc,n-1} |a_k| \Big)
		\]
	\end{proof}
\end{st}

\section{Allgemeine Fixpunktverfahren}

\begin{df}[Iterationsverfahren] \label{3.10}
	Zu $\Phi: \R^n \to \R^n$ stetig und $x^{(0)} \in \R^n$ definieren wir
	\[
		x^{(k+1)} := \Phi(x^{(k)})
	\]
	Dieses $\Phi$ nennen wir \emph{Iterationsverfahren}.
	\begin{note}
		Falls $x^* = \lim\limits_{k\to \infty} x^{(k)}$, so ist $x^*$ Fixpunkt von $\Phi$, d.h. $\Phi(x^*) = x^*$, denn
		\[
			x^* = \lim_{k\to \infty} x^{(k)} = \lim_{k\to \infty} x^{(k+1)} = \lim_{k\to \infty} \Phi(x^{(k)}) = \Phi\Big(\lim_{k\to \infty} x^{(k)}\Big) = \Phi(x^*)
		\]
		daher wird so ein Verfahren auch „\emph{Fixpunktiterationsverfahren}“ genannt.
	\end{note}
\end{df}

\begin{df}[Konvergenzordnung] \label{3.11}
	Wir nennen ein Iterationsverfahren \emph{lokal konvergent} von mindestens Ordnung $p$, wenn ein $\delta > 0$ existiert, sodass $\forall x^{(0)} \in B_\delta(x^*)$ gilt
	\[
		\|x^{(k+1)} - x^*\| \le C \cdot \|x^{(k)} - x^*\|^p
		\qquad \forall k\in \N
	\]
	für ein $p$ und ein $C > 0$.
	Falls $p=1$ muss zusätzlich $C < 1$ gelten.

	Das maximale $p$ nennen wir \emph{Konvergenzordnung} des Verfahrens.
	Für $p=1$ und $p=2$ sprechen wir von linearer bzw. quadratischer Konvergenz.
	\begin{note}
		Das erfasst genau die „lokal quadratische“ Konvergenz des Newton-Verfahrens bei einfachen Nullstellen.
	\end{note}
\end{df}

\begin{st}[Konvergenzordnung für Iterationsverfahren, $n=1$] \label{3.12}
	Sei $\Phi \in C^p(\R)$ eine stetige Iterationsfunktion mit Fixpunkt $x^*$.
	Das Iterationsverfahren ist lokal konvergent mit mindestens Ordnung $p$ genau dann, wenn
	\begin{align*}
		\begin{cases}
			\Phi^{(m)}(x^*) = 0 \qquad m=1,\dotsc,p-1 & p \ge 2 \\
			|\Phi'(x^*)| < 1 & p=1
		\end{cases}
	\end{align*}

	\begin{proof}
		Entwickle $\Phi(x)$ an der Stelle $x^*$:
		\[
			\Phi(x) = \sum_{m=0}^{p-1} \f {\Phi^{(m)}(x^*)}{m!}(x-x^*)^m + \f {\Phi^{(p)}(\xi)}{p!}(x-x^*)^p
		\]
		für ein $\xi$ zwischen $x$ und $x^*$.
		\begin{seg}[„$\Longleftarrow$“]
			Wegen $\Phi^{(m)}(x^*) = 0$ für $m=1,\dotsc,p-1$ ergibt sich
			\[
				\Phi(x) = \underbrace{\Phi(x^*)}_{=x^*} + \f {\Phi^{(p)}(\xi)}{p!}(x-x^*)^p
			\]
			stellt man um und betrachtet den Grenzwert $x\to x^*$, ergibt sich $\xi = x^*$, also
			\[
				\lim_{x\to x^*} \f {\Phi(x)-x^*}{(x-x^*)^p} = \f {\Phi^{(p)}(x^*)}{p!}
			\]
			Also $\forall \eps > 0 \; \exists \_\delta > 0$ sodass $\forall x\in B_{\_\delta}(x^*)$
			\begin{align*}
				\l| \f{\Phi(x)-x^*}{(x-x^*)^p} - \f {\Phi^{(p)}(x^*)}{p!} \r| \le \eps
			\end{align*}
			und damit (\fixme[Wie?]):
			\[
				|\Phi(x) - x^*| \le \underbrace{\l( \bigg| \f {\Phi^{(p)}(x^*)}{p!} \bigg| + \eps \r)}_{=: C}|x-x^*|^p
			\]
			Für $x\to x^*$ liegt also Konvergenz der Ordnung $p$ vor ($p>1$). 
			Im Fall $p=1$ ist $\Phi^{(p)}(x^*) < 1$ und $\eps$ lässt sich genügend klein wählen, dass $C<1$.

			Es bleibt zu zeigen, dass $x$ tatsächlich lokal gegen $x^*$ konvergiert.

			Setze
			\[
				\delta := \begin{cases}
					\min \Big\{\_\delta, \big(\tf 1{2C}\big)^{\f 1{p-1}} \Big\} & p>1 \\
					\_\delta & p=1
				\end{cases}
			\]
			Sei $x^{(0)} \in B_\delta(x^*)$, wir zeigen induktiv, dass $x^{(k)} \in B_\delta(x^*)$.
			Sei dazu $x^{(k)} \in B_\delta(x^*)$, dann ist
			\begin{align*}
				|x^{(k+1)}-x^*| 
				= |\Phi(x^{(k)}) -x^*| 
				&\le C|x^{(k)}-x^*|^p  \\
				&\le C \delta^{p-1}\delta \\
				&\le C \big( \tf{1}{2C}\big)^{\f{p-1}{p-1}} \delta
				= \f \delta 2
			\end{align*}
			und damit $x^{(k+1)} \in B_{\f\delta 2}(x^*)$. Außerdem konvergiert $\Phi(x)$ lokal, da die Kugel immer kleiner wird.
		\end{seg}
		\begin{seg}[„$\Longrightarrow$“]
			\begin{seg}[$p=1$]
				Sei das Verfahren lokal konvergent mit Ordnung $p=1$, d.h. $\forall k\in \N_0 : x^{(k)} \in B_\delta(x^*)$ und
				\[
					|x^{(k+1)} -x^*| \le C|x^{(k)} -x^*| \qquad C < 1
				\]
				Also für $x^{(k)} \neq x^*$:
				\[
					C \ge \f {|\Phi(x^{(k)}) - \Phi(x^*)|}{|x^{(k)}-x^*|} = |\Phi'(\xi)| 
				\]
				für ein $\xi$ zwischen $x^{(k)}$ und $x^*$.
				Im Grenzwert für $k\to \infty$, also $x^{(k)} \to x^*$ und $\xi \to x^*$ ergibt sich
				\[
					|\Phi'(x^*)| \le C < 1
				\]
			\end{seg}
			\begin{seg}[$p\ge 2$]
				Angenommen es existiert $j<p$ mit $j$ minimal, sodass $\Phi^{(j)}(x^*) \neq 0$.
				Die Taylorentwicklung in $x^*$ liefert:
				\begin{align*}
					\Phi(x) &= \Phi(x^*) + \f{\Phi^{(j)}(x^*)(x-x^*)^j}{j!} + \dotsb + \f{\Phi^{(p)}(\xi)}{p!}(x-x^*)^p \\
					&= \Phi(x^*) + \f {\Phi^{(j)}(x^*)}{j!} (x-x^*)^j \bigg( \underbrace{1 + (x-x^*)\sum_{m=j+1}^p a_m(x-x^*)^{m-j-1}}_{:= b(x)} \bigg)
				\end{align*}
				mit
				\[
					a_m := \begin{cases}
						\displaystyle \f{j!\Phi^{(p)}(\xi)}{p!\Phi^{(j)}(x^*)} & m=p \\
						\displaystyle \f{j!\Phi^{(m)}(x^*)}{m!\Phi^{(j)}(x^*)} & j+1 \le m < p
					\end{cases}
				\]
				$a_m$ ist beschränkt in $B_\delta(x^*)$.
				Wegen der Stetigkeit von $b(x)$ und $b(x^*)=1$ existiert $\_\delta \le \delta$ sodass $b(x) \ge \f 12$ für alle $x\in B_{\_\delta}(x^*)$.
				Für diese $x$ gilt dann
				\[
					|\Phi(x) - x^*| \ge \f {|\Phi^{(j)}(x^*)}{j!}|x-x^*|^j \cdot \f 12
				\]
				Sei $x\neq x^*$ und $x\in B_{\_\delta}(x^*)$, dann folgt mit der lokalen Konvergenz, dass
				\[
					\infty > C \ge \f {\Phi(x)-x^*}{|x-x^*|^p} \ge \f {|\Phi^{(j)}(x^*)|}{j!} |x-x^*|^{j-p} \cdot \f 12
				\]
				Wegen $j-p < 0$ ist die rechte Seite unbeschränkt, Widerspruch zu $C$ beschränkt.
			\end{seg}
		\end{seg}~
	\end{proof}
\end{st}

\begin{ex*}[Verfahren von Heron]
	Sei $a\in \R^+$ und
	\[
		\Phi(x) = \f 12 \bigg(x + \f ax\bigg)
	\]
	das Heron-Verfahren mit Fixpunkt $x^* = \sqrt a$.
	Man betrachte die ersten Ableitungen in $x^*$:
	\begin{alignat*}{2}
		\Phi'(x) &= \f 12 - \f a{2x^2} & \qquad \Phi'(x^*) &= \f 12 - \f {\sqrt a}{2\sqrt{a}^2} = 0 \\
		\Phi''(x) &= \f a{x^3} & \qquad \Phi''(x^*) &= \f a{\sqrt{a}^3} = \f 1{\sqrt a} \neq 0
	\end{alignat*}
	Es ergibt sich also für das Heronverfahren lokal quadratische Konvergenz.
	Das Heronverfahren ist ein Newton-Verfahren und bestätigt damit die lokal quadratische Konvergenz des Newton-Verfahrens aus Satz \ref{3.4}.
\end{ex*}


\section{Verfahren höherer Ordnung}


\begin{seg}[1. Idee: Linearkombination aus Verfahren niederer Ordnung]
	Seien $\Phi_0, \Phi_1$ Iterationsfunktionen mit Konvergenzordnung $p$, gemeinsamem Fixpunkt $x^*$ und $\Phi_0^{(p)}(x^*) \neq \Phi_1^{(p)}(x^*)$.

	Setze
	\[
		\Phi_s(x) := (1-s)\Phi_0(x) + s\Phi_1(x)
	\]
	Es ist klar, dass wegen $\Phi_0^{(m)}(x^*) = \Phi_1^{(m)}(x^*) = 0$ für $m=1,\dotsc,p-1$ auch $\Phi_s^{(m)}(x^*) = 0$ gilt.
	Bestimme jetzt das $s\in \R$, sodass $\Phi_s^{(p)}(x^*) = 0$ ist (Linearkombination der $0$ aus $\Phi_0^{(p)}(x^*)$ und $\Phi_1^{(p)}(x^*)$).

	Damit ergibt sich also ein Iterationsverfahren mit Konvergenzordnung von mindestens $p+1$ gegen $x^*$.
\end{seg}
\begin{seg}[2. Idee: Ansatz für Iterationsfunktionen und Bestimmung der Koeffizientenfunktionen]
	Sei $f\in C^2(\R)$ mit $f(x^*) = 0$, $f'(x^*) \neq 0$.
	Wir setzen
	\[
		\Phi(x) := x - g(x) f(x) - h(x) f(x)^2
	\]
	mit $g(x) \neq 0, h(x) \neq 0$ in einer Umgebung von $x^*$.
	Bestimme jetzt $g(x),h(x)$ so, dass $\Phi'(x^*) = \Phi''(x^*) = 0$ gilt.
	\begin{align*}
		\Phi'(x) &= 1 - g'(x)f(x) - g(x)f'(x) - h(x)2f(x)f'(x) -h'(x)f(x)^2 \\
		0 \stackrel != \Phi'(x^*) &= 1 -g(x^*)f'(x^*) \quad\iff\quad g(x^*) = \f 1{f'(x^*)}
	\end{align*}
	Wähle also $g(x) := \f 1{f'(x)}$, dann ist $g'(x) = - \f {f''(x)}{f'(x)}$.
	Weiter mit der zweiten Ableitung:
	\begin{align*}
		\Phi''(x) &= -g'(x)f'(x) -g'(x)f'(x) - g(x)f''(x) - 2h(x)f'(x)f'(x) + f(x)\Big( \dotso \Big) \\
		0 \stackrel != \Phi''(x^*) &= -2g'(x^*)f'(x^*) -g(x^*)f''(x^*) - 2h(x^*)f'(x^*)^2 \\
		&= 2 \f {f''(x^*)}{f'(x^*)} - \f {f''(x^*)}{f'(x^*)} - 2h(x^*) f'(x^*)^2 \\
		&= \f {f''(x^*)}{f'(x^*)} - 2h(x^*) f'(x^*)^2 
		\quad\iff\quad h(x^*) = \f {f''(x^*)}{2f'(x^*)^3}
	\end{align*}
	Wähle also $h(x) := \f {f''(x)}{2f'(x)^3}$.
\end{seg}

\begin{kor}[Verbessertes Newton-Verfahren] \label{3.13}
	Für $f\in C^2(\R)$ mit $f(x^*)=0$, $f'(x^*) \neq 0$ ist die Iteration
	\[
		x^{(k+1)} := x^{(k)} - \f {f(x^{(k)})}{f'(x^{(k)})} - \f {f''(x^{(k)})f(x^{(k)})^2}{2f'(x^{(k)})}
	\]
	lokal konvergent gegen $x^*$ mit Ordnung $3$.
\end{kor}

Weitere Ideen, um Verfahren höherer Ordnung zu konstruieren, wären:
\begin{description}
	\item[Newton-Raphson-Verfahren]
		Approximiere $f$ durch Hermite-Interpolationspolynom $p\in \P_n$ mit 
		\[
			p^{(m)}(x^{(k)}) = f^{(m)}(x^{(k)}) \qquad m=0,\dotsc,n.
		\]
		Bestimme $x^{(k+1)}$ als Nullstelle von $p$, welche am nächsten an $x^{(k)}$ liegt.

		Für $n=1$ entspricht das genau dem Newton-Verfahren.
	\item[Verallgemeinerung des Sekantenverfahrens]
		Seien $n+1$ paarweise verschiedene Stellen $x^{(k-n)}, \dotsc, x^{(k)}$ gegeben.
		Bestimme das Interpolationspolynom $p\in \P_n$ mit
		\[
			p(x^{(i)}) = f(x^{(i)}) \qquad i=k-n,\dotsc,k
		\]
		Bestimme $x^{(k+1)}$ als Nullstelle von $p$, welche am nächsten bei $x^{(k)}$ liegt.

		Für $n=1$ entspricht das dem herkömmlichen Sekantenverfahren, für $n=2$ dem Verfahren von Muller.
\end{description}


\section{Newton-Verfahren für mehrfache Nullstellen}

\begin{df}[Vielfachheit von Nullstellen] \label{3.14}
	Sei $f\in C^n ([a,b])$.
	$x^* \in (a,b)$ ist \emph{Nullstelle der Vielfachheit $n$} genau dann, wenn
	\begin{align*}
		f^{(m)}(x^*) &= 0 \qquad m=0,\dotsc,n-1 \\
		\land\quad f^{(n)}(x^*) &\neq 0
	\end{align*}
	Falls $n=1$, nennen wir $x^*$ \emph{einfache Nullstelle}, für $n>1$ \emph{mehrfache oder $n$-fache Nullstelle}.
\end{df}

\begin{nt*}
	In Satz \ref{3.4} haben wir gesehen, dass das Newton-Verfahren bei einfachen Nullstellen lokal quadratisch konvergiert.
	Für mehrfache Nullstellen ist das im Allgemeinen nicht der Fall.
\end{nt*}

\begin{st}[Lineare Konvergenz des Newton-Verfahrens bei mehrfachen Nullstellen] \label{3.15}
	Sei $f\in C^{n+2}([a,b])$ mit $n$-facher Nullstelle $x^*$ ($n>1$).
	Dann konvergiert das Newton-Verfahren lokal linear.
	\begin{proof}
		Schreibe $f$ als
		\[
			f(x) = (x-x^*)^n g(x)
		\]
		mit $g(x^*) \neq 0$.
		Wegen $f\in C^{n+2}([a,b])$ ist $g\in C^2([a,b])$.
		Für die Ableitung gilt
		\begin{align*}
			f'(x) &= n(x-x^*)^{n-1}g(x) + (x-x^*)^n g'(x),
		\end{align*}
		also für die Iteration:
		\begin{align*}
			\Phi(x) = x - \f {f(x)}{f'(x)} 
			&= x - \f {(x-x^*)^ng(x)}{n(x-x^*)^{n-1}g(x) + (x-x^*)^ng'(x)} \\
			&= x - \f {\overbrace{(x-x^*)g(x)}^{=:Z(x)}}{\underbrace{ng(x) + (x-x^*)g'(x)}_{=:N(x)}}
		\end{align*}
		Also
		\begin{align*}
			\Phi'(x) &= 1 - \f{\big(g(x) + (x-x^*)g'(x)\big)N(x) - Z(x)\big(ng'(x)+g'(x)+(x-x^*)g''(x)\big)}{N(x)^2}
			\intertext{Mit $Z(x^*) = 0$ und $N(x^*) = ng(x^*)$ folgt}
			&= 1 - \f{g(x^*)ng(x^*)}{n^2g(x^*)^2}
			= 1 - \f 1n
		\end{align*}
		Also folgt aus $|\Phi'(x^*)| < 1$ mit \ref{3.12} mindestens lineare Konverenz.
		Weil aber gleichzeitig $\Phi'(x^*) \neq 0$, liegt keine quadratische Konvergenz vor.
		Es ergibt sich also die Konvergenzordnung $1$.
		\begin{note}
			Der Beweis liefert gleich eine Idee, das Newton-Verfahren für mehrere Nullstellen zu verbessern, siehe Korrolar \ref{3.16}.
		\end{note}
	\end{proof}
\end{st}

\begin{kor}[Verbessertes Newton-Verfahren II] \label{3.16}
	Für $f\in C^{n+2}$ mit $n$-facher Nullstelle $x^*$ ist das Verfahren
	\[
		x^{(k+1)} := x^{(k)} - n \f {f(x)}{f'(x)}
	\]
	lokal quadratisch konvergent.
	\begin{proof}
		Mit der Notation aus dem vorigen Beweis gilt
		\begin{align*}
			\Phi(x) &= x - n\f{f(x)}{f'(x)} = x - n \f {Z(x)}{N(x)} \\
			\Phi'(x) &= 1 - n\f 1n = 0.
		\end{align*}
	\end{proof}
\end{kor}

\begin{ex*}
	Sei
	\[
		f(x) = x^5
	\]
	Dann ist wegen $f'(0) = f''(0) = \dotsb = f^{(4)}(0) = 0$ die $0$ $5$-fache Nullstelle.
	\begin{table}[H]
		\centering
		\caption{Lineare monotone (\ref{3.7}) Konvergenz links, verbesserte Konvergenz rechts}
		\begin{tabular}{c|l|l}
			$k$ & $x^{(k)}$ Newton & $x^{(k)}$ verb. Newton \\ \hline
			0 & 1 & 1 \\
			1 & 0.8 & 0 \\
			2 & 0.64 & 0 \\
			3 & 0.512 & 0 \\
			4 & 0.4096 & 0 \\
			5 & 0.3277 & 0
		\end{tabular}
	\end{table}
\end{ex*}


\section{Fixpunktverfahren im \texorpdfstring{$\R^n$}{Rn}}


\begin{st}[Banach'scher Fixpunktsatz] \label{3.17}
	Sei $M \subset \R^n$ nichtleer und abgeschlossen und $\Phi : M \to M$ eine Kontraktion, d.h. für ein $L \in (0,1)$ gilt
	\[
		\|\Phi(x) - \Phi(y) \| \le L \|x-y\| \qquad \forall x,y \in M
	\]
	Dann gilt
	\begin{enumerate}[i)]
		\item
			$\Phi$ hat genau einen Fixpunkt $x^* \in M$.
		\item
			Für alle Anfangswerte $x^{(0)} \in M$ konvergiert die Fixpunktiteration
			\[
				x^{(k+1)} := \Phi(x^{(k)}) \quad \to \quad x^* \qquad (k\to \infty)
			\]
		\item
			Es gilt die „a-priori-Schranke“
			\[
				\|x^{(k)} - x^*\| \le \f {L^k}{1-L} \|x^{(1)} - x^{(0)}\|
			\]
		\item
			Es gilt die „a-posteriori-Schranke“
			\[
				\|x^{(k)} - x^*\| \le \f {L}{1-L} \|x^{(k)} - x^{(k-1)}\|
			\]
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item
			\item
				\begin{seg}[Eindeutigkeit]
					Seien $x^*, \_{x^*}$ zwei Fixpunkte, dann ist
					\[
						\|x^* - \_{x^*}\| = \|\Phi(x^*) - \Phi(\_{x^*}) \| \le L \|x^* - \_{x^*}\|
					\]
					Wegen $L < 1$ muss $\| x^* - \_{x^*}\| = 0$, also $x^* = \_{x^*}$ gelten.
				\end{seg}
				\begin{seg}[Existenz]
					Sei $x^{(0)} \in M$ und $(x^{(k)})_{k\in \N}$ die Folge der Iterierten, dann ist
					\begin{align*}
						\|x^{(k+i)} - x^{(k)}\| 
						&= \bigg\| \sum_{l=k}^{k+i-1}(x^{(l+1)}-x^{(l)}) \bigg\| 
						\le \sum_{l=k}^{k+i-1}\|x^{(l+1)}-x^{(l)}\| \\
						&\le \sum_{l=k}^{k+i-1}L^{l-k}\|x^{(k+1)} -x^{(k)}\| 
						= \|x^{(k+1)} -x^{(k)}\| \sum_{j=0}^{i-1} L^{j} \\
						&= \|x^{(k+1)} -x^{(k)}\| \f {1-L^i}{1-L}
						\le \f 1{1-L}\|x^{(k+1)}-x^{(k)}\| \\
						&\le \f{L}{1-L}\|x^{(k)} - x^{(k-1)}\|
						\le \dotso
						\le \f{L^k}{1-L} \|x^{(1)} -x^{(0)}\|
					\end{align*}
					Damit ist $(x^{(k)})$ eine Cauchy-Folge mit Grenzwert $x^* \in \R^n$, außerdem $x^* \in M$, da $M$ abgeschlossen.

					Da $\Phi$ stetig ist (sogar lipschitzstetig wegen der Kontraktionseigenschaft), ist $x^*$ Fixpunkt (siehe Bemerkung in der Definition \ref{3.10})
				\end{seg}
			\item
			\item
				Betrachte
				\begin{align*}
					\|x^{(k+i)}-x^{(k)}\| \le \f {L}{1-L} \|x^{(k)} -x^{(k-1)}\| \le \f{L^k}{1-L}\|x^{(1)} - x^{(0)}\|
				\end{align*}
				für $i \to \infty$
		\end{enumerate}
	\end{proof}
\end{st}

\begin{seg}[Motivation: „Fixpunktverfahren mit Relaxation“]
	Sei $F: \R^n \to \R^n$ stetig mit $F(x^*) = 0$.
	Schreibe
	\[
		x^* = x^* - \omega F(x^*)
	\]
	Dies liefert einen Ansatz für eine Iterationsfunktion:
	\[
		\Phi(x) = x - \omega F(x)
		\qquad \omega \in \R^+
	\]
	mit Relaxationsparameter $\omega$.

	Unter welchen Bedingungen an $F$ und $\omega$ konvergiert diese Iteration?
\end{seg}

\begin{df}[Strikte Monotonie] \label{3.18}
	$F: \R^n \to \R^n$ ist \emph{strikt monoton} falls $\gamma > 0$ existiert, so dass
	\[
		\Big( F(x) - F(y) \Big)^T (x-y) \ge \gamma \|x-y\|^2
		\qquad x,y \in \R^n
	\]
\end{df}

\begin{lem} \label{3.19}
	Sei $F \in C^1(\R^n, \R^n)$ mit $F(x) = (f_i)_{i=1}^n$.

	Dann ist $F$ strikt monoton genau dann, wenn
	\[
		DF(x) := \bigg( \f{\d}{\d x_j}f_i\bigg)_{i,j=1}^n
	\]
	gleichmäßig positiv definit ist, d.h.
	\[
		y^T DF(x) y \ge \gamma \ge \gamma \|y\|^2 \qquad x,y \in \R^n
	\]
	\begin{proof}
		\begin{seg}[„$\implies$“]
			\begin{align*}
				y^T DF(x) y &= \lim_{h \to 0} \f 1h y^T\big(F(x+hy) - F(x)\big) \\
				&= \lim_{h\to 0} \f 1{h^2} \big( F(x+hy) - F(x)\big)^T(x+hy-x) \\
				&\ge \lim_{h\to 0} \f 1{h^2} \gamma\|x+hy-x\|^2 
				= \gamma\|y\|^2
			\end{align*}
		\end{seg}
		\begin{seg}[„$\Longleftarrow$“]
			\begin{align*}
				\big(F(x) - F(y)\big)^T (x-y) 
				&= \bigg( \int_0^1 DF(y+t(x-y))(x-y) \;dt \bigg)^T(x-y) \\
				&\ge \int_0^1 \gamma \|x-y\|^2 \;dt 
				= \gamma \|x-y\|^2
			\end{align*}
		\end{seg}
	\end{proof}
\end{lem}

\begin{st}[Fixpunktverfahren mit Relaxation] \label{3.20}
	Sei $F: \R^n \to \R^n$ lipschitzstetig mit Lipschitz-Konstante $L_F$ und strikt monoton mit Konstante $\gamma$.
	Dann gibt es genau eine Lösung $x^*$ von $F(x) = 0$.
	
	Das Iterationsverfahren
	\[
		x^{(k+1)} := x^{(k)} - \omega F(x^{(k)})
	\]
	mit $\omega \in (0, \f {2\gamma}{L_F^2})$ konvergiert global für alle $x^{(0)} \in \R^n$ gegen $x^*$ und es gilt
	\[
		\|x^{(k)} - x^*\| \le L^k \|x^{(0)} - x^*\|
	\]
	mit $L := \sqrt{1- 2 \gamma_\omega + \omega^2 L_F^2}$.
	\begin{proof}
		Die Menge $M := \R^n$ ist abgeschlossen und $\Phi: M \to M : x \mapsto x - \omega F(x)$ ist eine Kontraktion:
		\begin{align*}
			\|\Phi(x) - \Phi(y) \|^2 
			&= \|x-y - \omega(F(x) - F(y)) \|^2 \\
			&= \|x-y\|^2 - 2 \omega \underbrace{\big(F(x)-F(y)\big)^T(x-y)}_{\ge \gamma \|x-y\|} + \omega^2 \underbrace{\|F(x)-F(y)\|^2}_{L_F^2 \|x-y\|^2} \\
			&\le \underbrace{\big(1-2\omega \gamma + \omega^2 L_F^2\big)}_{=:c(\omega)} \|x-y\|^2
		\end{align*}
		Es muss für die Kontraktion $c(\omega) < 1$ gelten:
		\begin{align*}
			c(\omega) = 1 - 2 \omega \gamma + \omega^2 L_F^2 &< 1 \\
			\iff \quad \omega^2 L_F^2 &< 2 \omega \gamma \\
			\iff \quad  \omega &< \f {2\gamma}{L_F^2}
		\end{align*}
		was durch die Vorraussetzung an $\omega$ gegeben ist.

		Die Existenz und Eindeutigkeit des Fixpunkts $x^*$ und $\lim_{k\to \infty}x^{(k)} = x^*$ folgen mit dem Banach'schen Fixpunktsatz mit
		\[
			L := \sqrt{c(\omega)} = \sqrt{1-2\omega \gamma + \omega^2 L_F^2}
		\]
		Die Fehlerschranke folgt aus wiederholter Iteration und der Kontraktionseigenschaft.
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				Die Lipschitz-Konstante von $F$ ($L_F$) muss nicht notwendigerweise kleiner $1$ sein.
			\item
				Der optimale Relaxationsparameter:
				\[
					\omega_{\text{opt}} := \argmin_{\omega} c(\omega)
				\]
				Also insbesondere $c'(\omega_{\text{opt}}) = 0$, bzw
				\[
					- 2 \gamma + 2 \omega L_F^2 = 0
				\]
				und damit
				\[
					\omega_{\text{opt}} = \f {2 \gamma}{2L_F^2} = \f {\gamma}{L_F^2}
				\]
			\item
				Die Kontraktionsrate für $\omega_{\text{opt}}$ beträgt
				\[
					L_{\text{opt}} = \sqrt{1-2 \gamma \omega_{\text{opt}} + \omega_{\text{opt}}^2 L_F^2}
					= \sqrt{1-2 \gamma \f{\gamma}{L_F^2} + \f{\gamma^2}{L_F^2} L_F^2} 
					= \sqrt {1 - \f {\gamma^2}{L_F^2}}
				\]
			\item
				Es handelt sich also anders als bei Newton um \emph{globale} Konvergenz.

		\end{itemize}
	\end{note}
\end{st}

\begin{ex*}
	Sei $f(x) = x - x^{-x^2}$.
	Dann ist
	\[
		f'(x) = 1 + 2xe^{-x^2}
	\]
	Gleichmäßig positive Definitheit und Lipschitz-Stetigkeit via Extrema von $f'(x)$.	
	\[
		f''(x) = (2-4x^2)e^{-x^2} \stackrel != 0
		\qquad \implies \qquad
		x = \pm \sqrt{\f 12}
	\]
	und
	\begin{align*}
		f'(-\sqrt{\tf 12}) = \underbrace{1 - \sqrt 2 e^{-\f 12}}_{= \gamma} \le f'(x) \le f'(+\sqrt{\tf 12}) = \underbrace{1 + \sqrt 2 e^{- \f 12}}_{= L_F}
	\end{align*}
	Man erhält
	\[
		\gamma \approx 0.1422 > 0
		\qquad
		L_F \approx 1.8578 < \infty
	\]
	und damit
	\begin{align*}
		\omega_{\text{opt}} &= \f{\gamma}{L_F^2} \approx 0.0412 &
		L_{\text{opt}} = \sqrt{1- \f{\gamma^2}{L_F^2}} \approx 0.9971
	\end{align*}
	also globale, aber relativ langsame Konvergenz (\texttt{demos\textunderscore nonlin\textunderscore systems(5)}).
\end{ex*}


\section{Newton-Verfahren für Systeme}


\begin{df}[Newton-Verfahren im $\R^n$] \label{3.21}
	Sei $F \in C^1(\R^n, \R^n)$, wähle $x^{(0)} \in \R^n$ und definiere
	\[
		x^{(k+1)} := x^{(k)} - \Big(DF(x^{(k)})\Big)^{-1} F(x^{(k)}) \qquad k\in \N_0
	\]
\end{df}
\begin{nt*}
	\begin{itemize}
		\item
			$DF(x) \in \R^{n\times n}$ bezeichne die Jacobi-Matrix.
			\[
				F(x) = \begin{pmatrix}
					f_1(x) \\ \vdots \\ f_n(x) 
				\end{pmatrix}
				\quad\implies\quad
				\begin{pmatrix}
					\f{\d}{\d x_1}f_1(x) & \cdots & \f{\d}{\d x_n} f_1(x) \\
					\vdots & \ddots & \vdots \\
					\f{\d}{\d x_1}f_n(x) & \cdots & \f{\d}{\d x_n} f_n(x)
				\end{pmatrix}
			\]
		\item
			Falls alle $DF(x^{(k)})$ regulär sind, dann ist die Sequenz $(x^{(k)})_{k\in \N_0}$ wohldefiniert.
		\item
			In der Praxis bricht man die Iteration ab, falls
			\[
				\|F(x^{(k)})\| \le \eps_{\text{tol}}
				\quad\text{oder}\quad
				\|x^{(k-1)} - x^{(k)}\| \le \eps_{\text{tol}}
				\quad\text{oder}\quad
				k \ge k_{\text{max}}
			\]
		\item
			Wir umgehen die Matrixinversion, diese ist nämlich zu aufwändig.
			Statt
			\[
				x^{(k+1)} := x^{(k)} \underbrace{- DF(x^{(k)})^{-1} F(x^{(k)})}_{=:\delta^{(k)}}
			\]
			\begin{enumerate}[i)]
				\item
					Berechne das Newton-Inkrement $\delta^{(k)} \in \R^n$ durch Lösen des LGS
					\[
						DF(x^{(k)}) \delta^{(k)} = -F(x^{(k)})
					\]
				\item
					Aufaddieren der Iterierten:
					\[
						x^{(k+1)} = x^{(k)} + \delta^{(k)}
					\]
			\end{enumerate}
	\end{itemize}
\end{nt*}

\begin{lem}[Globale Konvergenz für affin-lineares $F$] \label{3.22}
	Für $F(x) = Ax - b$ mit $A\in \R^{n\times n}$ regulär und $b\in \R^n$, also $F(x^*)=0$ hat die eindeutige Lösung $x^* = A^{-1}b$.
	Dann gilt für alle $x^{(0)} \in \R^n$, dass
	\[
		x^{(1)} = x^*
		\quad\text{und}\quad
		\|F(x^{(1)}\| = 0
	\]
	also terminiert das Newton-Verfahren nach einer Iteration
	\begin{note}
		Funktionen der Form
		\[
			F(x) = Ax + b
		\]
		mit $A\in \R^{n\times n}$ und $b\in \R^n$ bezeichnen man auch als \emph{affin-linear}.
	\end{note}
	\begin{proof}
		\begin{align*}
			x^{(1)} = x^{(0)} - DF(x^{(0)})^{-1}F(x^{(0)}) = x^{(0)} - A^{-1} (Ax^{(0)}-b) = x^{(0)} - x^{(0)} + A^{-1}b = x^*
		\end{align*}
		Außerdem damit
		\[
			F(x^{(1)}) = F(x^*) = 0
		\]
	\end{proof}
	\begin{note}
		Globale Konvergenz liegt nur selten vor, meist nur lokale Konvergenz, analog zu $n=1$.
	\end{note}
\end{lem}

\begin{st}[Lokale Konvergenz] \label{3.23}
	Sei $F \in C^2(\R^n, \R^n)$, $F(x^*) = 0$ und $\det DF(x^*) \neq 0$.
	Dann ist das Newton-Verfahren lokal konvergent mit Ordnung $2$.
	\begin{proof}
		Ähnlich zu Satz \ref{3.4}.
		Siehe Jarre/Stoer Satz 4.12.
	\end{proof}
\end{st}

Das Newton-Verfahren ist unabhängig von der Basis des $\R^n$.

\begin{st}[Affin-Invarianz] \label{3.24}
	Sei $A\in \R^{n\times n}$ regulär und $b \in \R^n$.
	Setze $\hat F(\hat x) := F(A\hat x+b)$.
	Sei $\hat x^{(0)} \in \R^n$ und $x^{(0)} := A\hat x^{(0)} + b$, dann gilt für alle $\hat x^{(k)}$ und $x^{(k)}$ der Newton-Iterierten für $\hat F$ und $F$:
	\[
		x^{(k)} = A \hat x^{(k)} + b
	\]
	\begin{proof}
		Übung
	\end{proof}
	\begin{note}
		Gegebenenfalls kann hiermit ein $A$ gefunden werden, sodass die linearen Systeme für $\hat F$ besonders einfach oder stabil zu lösen sind.
	\end{note}
\end{st}

\begin{ex*}[Newton-Fraktale]
	Betrachte die komplexe Funktion $f\in C^\infty(\C, \C)$ definiert durch
	\[
		f(x) = z^3 - 1
	\]
	Die Nullstellen sind gerade die dritten Einheitswurzeln:
	\[
		z_1^* = 1  \qquad z_2^* = e^{\f 23 \pi i} \qquad z_3^* = e^{\f 43 \pi i}
	\]
	Das Newton-Verfahren aus Definition \ref{3.3} ist identisch für skalare komplexe Funktionen definiert.
	Die Realisierung im Rechner geschieht jedoch meist durch vektorielle reelle Arithmetik:
	Für $z = a+ ib$:
	\[
		f(z) \stackrel != 0  \quad \iff\quad F \begin{pmatrix}
			a \\ b
		\end{pmatrix}
		= \begin{pmatrix}
			\Re(f(a+ib)) \\ \Im(f(a+ib))
		\end{pmatrix}
		= \begin{pmatrix}
			a^3 - 3ab^2 - 1 \\
			3a^2 b - b^3
		\end{pmatrix}  \stackrel != \begin{pmatrix}
			0 \\ 0
		\end{pmatrix}
	\]
	Damit ergibt sich für die $DF$:
	\[
		DF \begin{pmatrix}
			a \\ b
		\end{pmatrix} = \begin{pmatrix}
			3a^2 - 3b^2 & -6ba \\
			6ba & 3a^2 - 3b^2
		\end{pmatrix}
	\]
	Wende jetzt das vektorielle Newton-Verfahren aus Definition \ref{3.21} an.
	Es ergibt sich dann
	\[
		z^{(k)} := a^{(k)}0+ ib^{(k)}
	\]
	Als Sequenz der Iterierten zum Startwert $z^{(0)} = a^{(0)} +ib^{(0)}$.

	Für viele $z^{(0)}$ ist die Iteration konvergenzt, für einige $z^{(0)}$ jedoch nicht.

	Definiere das sogenannte \emph{Newton-Fraktal zu $f(z)$}.
	\[
		\scr F := \Big\{ z^{(0)} \in \C : \text{ Newton-Verfahren mit Startwert $z^{(0)}$ konvergiert nicht} \Big\}
	\]
	Für $f(z) = z^3 - 1$ ist $\scr F$ eine Nullmenge (hat Maß Null), besitzt aber eine interessante „selbstähnliche Struktur“.

	Für Punkte $z^{(0)}$ mit Konvergenz gegen eines der $z_i^*$ kann man diese in eine von drei Farben einfärben.
	Auch die Helligkeit kann man entsprechend der Iterationszahl bis zum Abbruch definieren.
	Es ergeben sich die „typischen“ bunten Bilder von „Fraktalen“.
\end{ex*}


\section{Variationen von Newton-Verfahren}

Die Berechnung der $DF(x^{(k)})$ und das Lösen von $DF(x^{(k)}) \delta^{(k)} = -F(x^{(k)})$ ist rechenaufwändig insbesondere für hohe Dimensionen, komplizierte $F$, etc.

\begin{seg}[„Vereinfachtes Newton-Verfahren]
	Ein Ansatz ist das sogennante „Vereinfachte Newton-Verfahren“:
	Verwende in jedem Schritt $DF(x^{(0)})$ statt $DF(x^{(k)})$.

	Damit ist nur eine Berechnung der Jacobi-Matrix $DF(x^{(0)})$ zu Beginn notwendig.
	Die sich ergebenden Systeme der Form 
	\[
		F(x^{(0)}) \delta^{(k)} = -F(x^{(k)})
	\]
	können effizient durch einmalige LR-Zerlegung und mehrfaches Vorwärts-/Rückwärts-Einsetzen gelöst werden.

	Andererseits ist die Konvergenzordnung reduziert, es sind also meist mehr Iterationen erforderlich.
\end{seg}

\begin{seg}[„Quasi-Newton-Verfahren“]
	Verwende $A^{(k)} \in \R^{n\times n}$ statt $DF(x^{(k)})$, d.h.
	\[
		x^{(k+1)} := x^{(k)} + \delta^{(k)}
		\qquad A^{(k)} \delta^{(k)}0= -F(x^{(k)})
	\]
	mit $A^{(k)}$ einfach zu berechnen.

	Für $A^{(k)} = DF(x^{(0)})$ ergibt sich das vereinfachte Newton-Verfahren.
	Das vereinfachte Newton-Verfahren ist also auch ein Quasi-Newton-Verfahren

	Für $n=1$ mit
	\[
		A^{(k)} := \f {f(x^{(k)}) - f(x^{(k-1)})}{x^{(k)} - x^{(k-1)}} \approx f'(x^{(k)})
	\]
	Damit ist das Sekanten-Verfahren auch ein Quasi-Newton-Verfahren.

	Betrachte iterative LGS-Löser zum Lösen von $Ax=:$ (siehe NLA) basierend auf der Zerlegung:
	\[
		A = L + D + R 
		\qquad F(x) := Ax + b
	\]
	mit linker-unterer Dreiecksmatrix $L$, Diagonalmatrix $D$ und rechter-oberer Dreiecksmatrix $R$.
	Für das Jacobi-/Gesamtschrittverfahren:
	\[
		x^{(k+1)} = x^{(k)} - D^{-1} F(x^{(k)})
	\]
	Für das Gauß-Seidel-/Einzelschrittverfahren:
	\[
		x^{(k+1)} = x^{(k)} - (D+L)^{-1} F(x^{(k)})
	\]
	Diese sind also auch Quasi-Newton-Verfahren.
\end{seg}

Im Allgemeinen besitzt das Newton-Verfahren zur lokale Konvergenz, die Menge der Startwerte mit Konvergenz kann klein sein, Divergenz ist möglich.

Ein Grund für die Divergenz kann das Phänomen sein, dass $\|F(x^{(k+1)})\| > \|F(x^{(k)})\|$.
Das widerspricht dem Ziel $F(x^*) = 0$ (Beispiel: \texttt{newton\textunderscore examples(3)}).

\begin{seg}[„Gedämpftes Newton-Verfahren“]
	Wähle kleinere Schritte, um ein Abfallen der $\|F(x^{(k)})\|$ zu gewährleisten. 
	Hiermit wird die Menge der Startwerte mit Konvergenz vergrößert.

	Wähle dazu $\omega_k \in (0,1]$ und setze
	\[
		DF(x^{(k)}) \delta^{(k)} = - F(x^{(k)})
	\]
	mit der Iterationsvorschrift
	\[
		x^{(k+1)} := x^{(k)} + \omega_k \delta^{(k)}
	\]
	Für $\omega_k = 1$ entspricht das dem herkömmlichen „ungedämpften“ Newton-Verfahren.

	Für $F \in C^2$ gilt
	\begin{align*}
		F(x^{(k)} + \omega \delta^{(k)}) &= F(x^{(k)}) + \omega \underbrace{DF(x^{(k)})\delta^{(k)}}_{-F(x^{(k)})} + O(\omega^2) \\
		&= (1 - \omega)F(x^{(k)}) + O(\omega^2)
	\end{align*}
	Falls $F(x^{(k)}) \neq 0$ kann für $\omega$ genügend klein kann
	\[
		\|F(x^{(k)} + \omega \delta^{(k)}) \| < \|F(x^{(k)}\|
	\]
	garantiert werden.
	Für $\beta \in (0,1)$ gilt für genügend kleines $\omega$
	\[
		\|F(x^{(k)} + \omega \delta^{(k)})\|  \le (1 - \beta_\omega)) \|F(x^{(k)})\|
	\]
	\begin{ex*}
		\texttt{demos\textunderscore nonlin\textunderscore systems(7)}.
		$f(x) = \arctan(x)$
		Mit $\omega = 1$ ergibt sich Konvergenz in $x^{(0)} \in (-1.3, 1.3)$.
		Mit der Wahl $\omega = 0.1$ ergibt sich Konvergenz auf $x^{(0)} \in [-10, 10]$.

		Die Konvergenz ist jedoch mit Dämpfung langsamer.
	\end{ex*}
\end{seg}



\chapter{Optimierung}



Wir suchen
\[
	\min_{x \in \R^n} f(x)
\]
unter den Nebenbedingungen
\begin{align*}
	g_i(x) &\le 0 \qquad i=1,\dotsc, n_g \\
	h_i(x) &= 0 \qquad i = 1,\dotsc, n_h
\end{align*}
\begin{itemize}
	\item
		Wir nennen $f(x)$ \emph{Zielfunktion} der Optimierung, $g_i(x) \le 0$ \emph{Ungleichungsnebenbedingungen} und $h_i(x)=0$ \emph{Gleichheitsnebenbedingungen}.
	\item
		Wir nennen
		\[
			X := \{x\in \R^n : g_i(x) \le 0 \land h_i(x) = 0 \}
		\]
		die Menge der \emph{zulässigen Punkte}.
	\item
		Es reicht das Minimierungsproblem zu betrachten, denn
		\[
			\max_{x\in \R^n} f(x) = -\min_{x\in\R^n} -f(x)
		\]
	\item
		Bei den Nebenbedingungen reicht „$\le 0$“ und „$= 0$“ zu betrachten, denn
		\begin{align*}
			g_i(x) \ge 0 \quad &\iff \quad -g_i(x) \le 0 \\
			a(x) \le b \quad &\iff \quad a(x) - b \le 0
		\end{align*}
	\item
		Wichtige Fragen sind:
		\begin{itemize}
			\item
				die nach Existenz und Eindeutigkeit von Optima 
			\item
				hinreichende und notwendige Bedingungen zuum Vorliegen eines Optimums in einem vorgegebenem Punkt
			\item
				numerische Verfahren zur approximativen Lösung, deren Konvergenz und Fehleraussagen
		\end{itemize}
	\item
		Falls $f,g_i,h_i$ glatte Funktionen sind, haben viele Lösungsansätze polynomielle Komplexität in $n$.
	\item
		Falls statt $x\in\R^n$, eine diskrete Grundmenge vorliegt, z.B. $\Z^n$, so ist die Optimierung wesentlich schwieriger.
		Man spricht dann auch von \emph{diskreter Optimierung} oder \emph{kombinatorischer Optimierung}, deren Lösung häufig Berechnungskomplexität expoentiell in $n$ aufweist.
\end{itemize}

\begin{seg}[Spezielle Klassen von Problemen]
	\begin{description}
		\item[Lineares Optimierungsproblem, „\emph{lineare Programme}“ (LP)]
			Sei $f(x) = c^Tx$ mit $c\in \R^n$ und  $g_i(x) = a_i^Tx + b_i, h_i(x) = c_i^Tx + d_i$.

			Die Menge $X$ der zulässigen Punkte ist ein Schnitt von Halbräumen und Hyperebenen und insbesondere konvex.
			
			Wende das „Simplex-Verfahren“ an (siehe NLA oder Stoer-Bulirsch).
			\begin{itemize}
				\item
					Das Optimum muss in einer Ecke von $X$ liegen.
				\item
					Suche eine Ecke von $X$, laufe dann über Kanten von $X$ zu Ecken mit geringerem Zielwert.
				\item
					Ende mit Optimum $x^*$, oder Feststellung der Unbeschränktheit.
			\end{itemize}
		\item[Unrestringierte Quadratische Optimierung, „\emph{Quadratische Programme}“]
			\[
				\min_{x\in\R^n} \f 12 x^TAx - b^Tx + c 
			\]
			mit $A$ positiv definit und symmetrisch.
			Die Existenz und Eindeutigkeit eines Minimums ist gesichert.
			Hinreichende und notwendige Bedingung für ein lokales Minimum ist
			\[
				\nabla f(x) = 0 \quad \iff \quad Ax = b
			\]
			Siehe Kapitel 4.1 Gradientenverfahren und CG-Verfahren.
		\item[Konvexe Optimierung]
			Sei $f$ konvex und $X$ konvex (d.h. $g_i$ konvex und $h_i$ affinn linear).
			Siehe Kapitel 4.2.
		\item
			In Matlab: \texttt{linprog,quadprog,fmincon} sind Beispiele für Lösungsalgorithmen.
		\item
			Sehr gute Buchreferenz: Jarre/Soer: Optimierung, Springer 2004.
	\end{description}
\end{seg}

\section{Unrestringierte Optimierung}

\begin{seg}[Problem]
	Finde Lösung $x^*\in \R^n$ von
	\[
		\min_{x\in\R^n}f(x)
	\]
	für $f: \R^n \to \R$ ohne weiteren Nebenbedingungen.
\end{seg}

\begin{df}[Minima] \label{4.1}
	Sei $f: \R^n \to \R, X \subset \R^n$.
	$x^* \in \R^n$ heißt
	\begin{itemize}
		\item
			\emph{lokales Minimum}, falls eine Umgebung $U$ von $x^*$ existiert mit
			\[
				f(x) \ge f(x^*) \qquad \forall x\in U \cap X
			\]
		\item
			\emph{strikt lokales Minimum}, falls eine Umgebung $U$ von $x^*$ existiert mit
			\[
				f(x) > f(x^*) \qquad \forall x\in (U \cap X) \setminus \{x^*\}
			\]
		\item
			\emph{globales Minimum}, falls
			\[
				f(x) \ge f(x^*) \qquad \forall x \in X
			\]
		\item
			\emph{stationärer Punkt}, falls $f\in C^1$ und
			\[
				\nabla f(x^*) = 0
			\]
	\end{itemize}
	\begin{note}
		\begin{itemize}
			\item
				Die Definitionen für Maxima sind analog.
			\item
				Im folgenden werden verschiedene Optimierungsverfahren behandelt.
				Man darf typischerweise keine schönen Konvergenzaussagen erwarten für allgemeine $f$ (z.B. kann häufig nur das Finden von stationären Punkten gewährleistet werden).
				Unter Einschränkungen von $f$ (z.B. spezielle Funktionsklasse) sind schöne Konvergenzaussagen erreichbar.
		\end{itemize}
	\end{note}
\end{df}

\subsection{Direkte Suche, \texorpdfstring{$n=1$}{n=1}}

Gesucht ist
\[
	\min_{x\in \R} f(x)
\]
ohne Stetigkeits- oder Differenzierbarkeitsannahme an $f$.
Die eindimensionalen Probleme sind wichtig als Zwischenschritt zur mehrdimensionalen Optimierung.

\begin{df}[Unimodale Funktion] \label{4.2}
	Die Funktion $f:[a,b] \to \R$ ist \emph{unimodal}, falls $x^* \in [a,b]$ existiert mit 
	\begin{align*}
		&f\big|_{[a,x^*]} \quad \text{ist streng monoton fallend} \\
		\land \quad 
		&f\big|_{[x^*,b]} \quad \text{ist streng monoton steigend}
	\end{align*}
	\begin{note}
		\begin{itemize}
			\item
				$x^*$ ist eindeutiges Minimum von $f$.
			\item
				Ist $f$ unimodal auf $[a,b]$, dann ist $f$ unimodal auf $[\_a,\_b] \subset [a,b]$.
			\item
				Sei $a< x_1 < x_2 < b$ und $f$ unimodal.
				Falls $f(x_1) > f(x_2)$, dann ist $x^* \in [x_1,b]$, sonst $x^* \in [a,x_2]$.
		\end{itemize}
	\end{note}
\end{df}

\begin{seg}[Intervallschachtelung mit Zwischenpunkten]
	Wähle für ein festes $\tau \in (0,1)$, $x_1, x_2$ so dass
	\[
		x_2-a = \tau (b-a) = b-x_1
	\]
	Weiter soll $\tau$ derart sein, dass $x_1$ (oder $x_2$) ein Zwischenpunkt auf dem kleineren der beiden Intervalle $[a,x_2]$, $[x_1,b]$ ist, also
	\begin{align*}
		(x_1 - a) &= \tau (x_2 - a) \\
		(1-\tau)(b-a) &= \tau \tau (b-a)
		\quad \iff \quad 1 - \tau = \tau^2 \quad\iff\quad \tau = \phi = \f{\sqrt 5 - 1}2 \approx 0.618
	\end{align*}
	Damit ist nur ein neuer Punkt und Funktionswert erforderlich, denn der alte Zwischenpunkt und Funktionswert können wiederverwendet werden.
\end{seg}

\begin{df}[Verfahren des Goldenen Schnitts] \label{4.3}
	Gegeben $f:[a,b] \to \R$.
	Setze $\tau := \f {\sqrt 5 - 1}2$ und
	\begin{gather*}
		a^{(0)} := a, \quad b^{(0)} := b
		x_2^{(k)} := a{(k)} + \tau(b^{(k)}-a^{(k)}), \quad x_1^{(k)} := b^{(k)} - \tau(b^{(k)}-a^{(k)})
	\end{gather*}
	Falls $f(x_1^{(k)}) > f(x_2^{(k)})$, setze
	\[
		[a^{(k+1)},b^{(k+1)}] := [x_1^{(k)}, b]
	\]
	sonst
	\[
		[a^{(k+1)},b^{(k+1)}] := [a, x_2^{(k)}]
	\]
\end{df}

\begin{ex*}[Verfahren des Goldenen Schnitts]
	Für unimodales $f$ siehe \texttt{demos\textunderscore optimization(1)}.
	Für stückweise konstante zufällige Funktion, siehe \texttt{demos\textunderscore optimization(2)}.
\end{ex*}

\begin{st}[Konvergenz für $f$ unimodal] \label{4.4}
	Für $f: [a,b] \to \R$ unimodal gilt
	\[
		|b^{(k)} - a^{(k)}| = \tau^k |b-a|
	\]
	und
	\[
		\lim_{k\to \infty} a^{(k)} = \lim_{k\to \infty} b^{(k)} = x^*
	\]
	mit $x^*$ globalem Minimum von $f$.
	\begin{proof}
		Ähnlich zu Satz \ref{3.2}.
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				Das Verfahren ist eher theoretisch interessant, da die Vorraussetzung unimodal selten erfüllt ist.
			\item
				Für $f\in C^1$ ist nur eine schwächere Aussage möglich (siehe Satz \ref{4.5})
		\end{itemize}
	\end{note}
\end{st}

\begin{st}[Konvergenz für $f\in C^1$] \label{4.5}
	Sei $f\in C^1([a,b])$ mit $f(x_1^{(0)}) < \min\{f(a),f(b)\}$.
	Dann ist
	\[
		x^* := \lim_{k\to \infty} a^{(k)} = \lim_{k\to \infty}b^{(k)}
	\]
	stationärer Punkt, d.h. $f'(x^*) = 0$.
	\begin{proof}
		Setze 
		\[
			x^{(k)} := \begin{cases}
				x_1^{(k)} & f(x_1^{(k)}) \le f(x_2^{(k)}) \\
				x_2^{(k)} & \text{sonst}
			\end{cases}
		\]
		Damit ist $f(a^{(k)}) \ge f(x^{(k)}) \le f(b^{(k)})$.
		Deshalb, da $f'$ stetig:
		\[
			f'(x^*) = \lim_{k\to \infty} \underbrace{\f {f(x^{(k)})-f(a^{(k)})}{x^{(k)}-a^{(k)}}}_{\le 0} = \lim_{k\to \infty} \underbrace{\f {f(b^{(k)})-f(x^{(k)})}{b^{(k)}-x^{(k)}}}_{\ge 0} = 0
		\]
	\end{proof}
	\begin{note}

		\begin{itemize}
			\item
				Das $x^*$ muss kein Minimum sein.
				Betrachte $f(x) = x^4$ auf dem Intervall $[a,b] = [-1,1]$.
				Das Verfahren aus Definition \ref{4.3} liefert die Sequenz $a^{(k)} < 0, b^{(k)} > 0$ und
				\[
					x^* := \lim_{k\to \infty}a^{(k)} = \lim_{k\to \infty} b^{(k)} = 0
				\]
				liefert ein korrektes globales Minimum.

				Betrachte nun $\tilde f\in C^1$ derart, dass es an allen Stellen $a^{(k)},b^{(k)},x_1^{(k)},x_2^{(k)}$ übereinstimmt mit $f$.
				Aber $\tilde f$ besitzt lokale Minima zwischen jeweils $2$ Punkten min negativem Funktionswert.

				Das Verfahren liefert dann die identische Punktsequenz für $\tilde f$, aber $0$ ist kein Minimum.
			\item
				Für $f$ analytisch, so ist $x^*$ lokales Minimum (siehe Jarre/Stoer).
		\end{itemize}
	\end{note}
\end{st}


\subsection{Gradientenverfahren}

Wir wollen
\[
	\min_{x\in \R^n} f(x)
\]
mit $f \in C^1 (\R^n, \R)$ betachten.

\begin{seg}[„Methode des steilsten Abstieges“]
	\begin{itemize}
		\item
			$\nabla f(x)$ ist die Richtung des steilsten Anstiegs, also ist
			\[
				d := -\nabla f(x)
			\]
			eine „\emph{Abstiegsrichtung}“.
		\item
			Falls $\nabla f(x) \neq 0$, dann gilt für genügend kleines $\omega \in \R^+$
			\[
				f(x + \omega d) < f(x)
			\]
		\item
			Falls $\nabla f(x) = 0$, dann wurde $x$ als stationärer Punkt gefunden.
		\item
			Zu $x^{(0)} \in \R^n$ gehe wiederholt kleine Schritte in Abstiegsrichtung.
	\end{itemize}
\end{seg}

\begin{df}[Gradientenverfahren] \label{4.6}
	Seien $f \in C^1(\R^n, \R)$, $x^{(0)} \in \R^n$ und $\omega_k \in \R^+$ \emph{Schrittweiten} (für $k \in \N_0$).
	Definiere
	\[
		x^{(k+1)} := x^{(k)} + \omega_k d^{(k)}
	\]
	mit
	\[
		d^{(k)} := - \nabla f(x^{(k)})
	\]
	\begin{note}[Optimalität]
		\begin{itemize}
			\item
				In der Praxis ist der gefundener Punkt häufig ein lokales Minimum, für manche $f$ auch beweisbar (z.B. $f \in C^2$ und $\nabla^2 f(x^*)$ positiv definit, dann ist $x^*$ in jedem Fall lokales Minimum).

				Es gibt aber auch Gegenbeispiele (z.B. stationärer Punkt $x^*$ wird in einem Sattelpunkt angenommen).
		\end{itemize}
	\end{note}
	\begin{note}[Abstiegsrichtung]
		\begin{itemize}
			\item
				Falls man $d^{(k)}$ anders wählt, jedoch
				\[
					d^{(k)} \cdot \nabla f(x^{(k)}) < 0
				\]
				gewährt bleibt (stumpfer Winkel), so ist $d^{(k)}$ eine Abstiegsrichtung und das resultierende Verfahren ein „\emph{Abstiegsverfahren}“.
				
				Das Gradientenverfahren ist ein spezielles Abstiegsverfahren.
		\end{itemize}
	\end{note}
	\begin{note}[Schrittweite]
		Siehe \texttt{demos\textunderscore optimization(3)}.
		\begin{itemize}
			\item
				$\omega_k$ darf nicht zu groß sein, sonst wird das Optimum möglicherweise übersprungen.
			\item
				$\omega_k$ darf auch nicht zu klein sein, sonst stagniert man in einem nicht-stationären Punkt.
			\item
				Eine Möglichkeit bietet die exakte “\emph{line search}”, also die 1D-Optimierung auf der Halbgeraden
				\[
					\omega_k = \argmin_{\omega \in \R^+} f(x^{(k)} + \omega d^{(k)})
				\]
				Diese Optimierung ist aber eventuell aufwändig.
			\item
				Eine andere Möglichkeit ist die „\emph{Armijo Schrittwahl}“: wähle $\sigma \in (0,1)$ und $\omega_k$ als größte Zahl in $\{2^{-j}\}_{j\in \N_0}$ so dass
				\[
					f(x^{(k)} + \omega_k d^{(k)}) \le f(x^{(k)}) + \omega_k \sigma \nabla f(x^{(k)})^T \cdot d^{(k)}
				\]
		\end{itemize}
	\end{note}
\end{df}

\begin{lem}[Wohldefiniertheit Armijo] \label{4.7}
	Für genügend kleines $\omega_k$ und $\nabla f(x^{(k)}) \neq 0$ ist Armijo-Regel
	\[
		f(x^{(k)} + \omega_k d^{(k)}) \le f(x^{(k)}) + \omega_k \sigma \nabla f(x^{(k)})^T \cdot d^{(k)}
	\]
	erfüllt.
	\begin{proof}
		Sei $\omega_k \in \R^+$ beliebig.
		Die Taylorentwicklung liefert für ein $\xi$ zwischen $x^{(k)}, x^{(k)} + \omega_k d^{(k)}$:
		\begin{align*}
			f(x^{(k)}+ \omega_k d^{(k)}) &= f(x^{(k)}) + \omega_k \nabla f(\xi)^T \cdot d^{(k)} \\
			&= f(x^{(k)}) + \omega_k \sigma \nabla f(x^{(k)})^T \cdot d^{(k)}
			+ \omega_k \big(\sigma (\nabla f(\xi)) - \nabla f(x^{(k)})^T d^{(k)} + (1-\sigma)\nabla f(\xi)^T d^{(k)} \Big)
		\end{align*}
		Für $\omega_k \to 0$ ist $\nabla f(\xi) - \nabla f(x^{(k)}) \to 0$ und $(1-\sigma) \nabla f(\xi)^T d^{(k)} \to -(1-\sigma)\|\nabla f(x^{(k)})\|^2 < 0$ für genügend kleines $\omega_k$.
		Damit ist
		\[
			\omega_k \big(\sigma (\nabla f(\xi)) - \nabla f(x^{(k)})^T d^{(k)} + (1-\sigma)\nabla f(\xi)^T d^{(k)} \Big)
		\]
		und die Armijo-Regel ist erfüllt.
	\end{proof}
\end{lem}

\begin{st}[Konvergenz] \label{4.8}
	 Jeder Häufungspunkt $x^* \in \R^n$ von $\{x^{(k)}\}_{k\in \N}$ aus dem Gradientenverfahren mit Armijo-Regel ist ein stationärer Punkt von $f$, d.h. $\nabla f(x^*) = 0$.
	 \begin{proof}
	 	Sei $x^*$ Häufungspunkt und $(x^{(k)})_{k\in \N}$ o.B.d.A. konvergent gegen $x^*$ (sonst wähle Teilfolge).
		Angenommen $\nabla f(x^*) \neq 0$.
		Es gilt
		\[
			\|x^{(k+1)} - x^{(k)}\| = \omega_k \|\nabla f(x^{(k)})\|^2 \to 0 \qquad k\to \infty
		\]
		Die Stetigkeit von $\nabla f$ liefert $\|\nabla f(x^{(k)})\| \to \|\nabla f(x^*)\|$ also $\omega_k \to 0$.

		Sei $k_0$ groß genug, sodass $\omega_k \le \f 12$ für $k \ge k_0$.
		Das $\omega_k$ nach der Armijo-Regel die größte Zahl in $\{2^{-j}\}_{j\in\N_0}$ ist, so dass die Ungleichung erfüllt ist, liefert die Armijo-Regel für $k\ge k_0$ zwangsläufig:
		\[
			f(x^{(k)} + \underbrace{2 \omega_k}_{\mathclap{ \in \{2^{-j}\}_{j\in \N_0}}} d^{(k)}) > f(x^{(k)}) + 2 \omega_k \sigma \nabla f(x^{(k)})^T d^{(k)}
		\]
		Mit der Taylorentwicklung folgt für $\xi^{(k)}$ zwischen $x^{(k)}$ und $x^{(k)} + 2 \omega_k d^{(k)}$:
		\begin{align*}
			f(x^{(k)} + 2\omega_k d^{(k)}) 
			&= f(x^{(k)}) + 2 \omega_k \nabla f(\xi^{(k)})^T d^{(k)}
		\end{align*}
		Also
		\begin{align*}
			f(x^{(k)}) + 2\omega_k \nabla f(\xi^{(k)})^T d^{(k)} &> 2 \omega_k \sigma \nabla f(x^{(k)})^T d^{(k)} \\
		\intertext{Wegen $2 \omega_k > 0$ folgt}
			\iff \qquad
			\nabla f(\xi^{(k)})^T \cdot d^{(k)} &> \sigma \nabla f(x^{(k)})^T \cdot d^{(k)}
		\intertext{Aus $\omega_k \to 0$ und $x^{(k)} \to x^*$ und $d^{(k)} \to -\nabla f(x^*)$ folgt, dass $\xi^{(k)} \to x^*$ und es ergibt sich im Grenzwert $k\to \infty$:}
			\nabla f(x^*)^T\cdot (-\nabla f(x^*)) &\ge \sigma \nabla f(x^*)^T \cdot (-\nabla f(x^*)) \\
			\iff \qquad -\|\nabla f(x^*)\|^2 &\ge - \sigma \|\nabla f(x^*)\|^2 \\
			\iff \qquad 1 &\le \sigma
		\end{align*}
		ein Widerspruch zur Wahl von $\sigma \in (0,1)$.
		Also ist $\nabla f(x^*) = 0$.
	 \end{proof}
	 \begin{note}
		 \begin{itemize}
		 	\item
		 		Man kann eine ähnliche Aussage für “line search” erhalten
			\item
				Die Menge $\{x^{(k)}\}$ muss keinen Häufungspunkt besitzen, z.B.
				\[
					f(x) := c^T x \qquad c \in \R^n \setminus\{0\}
				\]
				Es ergibt sich dann $\nabla f(x) = c =: -d^{(k)}$.
				Für $\omega_k = 2^{-0} = 1$ ist die Armijo-Regel erfüllt:
				\[
					f(x^{(k)} +\omega_k d^{(k)}) = f(x^{(k)} - c) = c^T x^{(k)} - \|c\|^2 \le c^T x^{(k)} - \sigma \|c\|^2 = f(x^{(k)}) + \omega_k \sigma \nabla f(x^{(k)})^T \cdot d^{(k)}
				\]
				damit ist
				\[
					x^{(k)} := x^{(0)} - k c
				\]
				unbeschränkt und besitzt keinen Häufungspunkt
			\item
				Falls die Menge $\{x : f(x) \le f(x^{(0)}) \}$ kompakt ist, dann ist $\{x^{(k)}\}$ beschränkt und es existiert eine konvergente Teilfolge.
				Damit existiert auch ein Häufungspunkt $x^*$.
			\item
				Häufungspunkte müssen nicht eindeutig sein (z.B. „Spiralfunktion“, Jarre/Stoer 6.2.2.).
			\item
				Das Gradientenverfahren ist sehr robust aber langsam konvergent.
				Siehe \texttt{demos\textunderscore optimization(4)}, Rosenbrook-Funktion:
				\[
					f(x_1, x_2) = 50 (x_2-x_1^2)^2 + (x_1-1)^2
				\]
				Damit ergibt sich langsame Konvergenz für das Gradientenverfahren.
			\item
				In der Praxis benutzt man viele verschiedene Startpunkte $x^{(0)}$ (z.B. Zufallszahlen, dann Monte-Carlo-Optimierung) um die Chance zu erhöhen, ein globales Optimum unter den gefundenen lokalen Optima zu finden.
		 \end{itemize}
	 \end{note}
\end{st}


\subsection{Gradientenverfahren für quadratische Zielfunktionen}


\begin{df}[Unrestringiertes QP] \label{4.9}
	Wir nennen
	\[
		\min_{x\in \R^n} \underbrace{\f 12 x^T A x - b^T x + c}_{=:f(x)}
	\]
	mit $A \in \R^{n\times n}$ symmetrisch und positiv definit, $b\in \R^n, c\in \R$ ein
	\emph{unrestringiertes, konvexes quadratisches Programm} (QP).
\end{df}

\begin{st}[Existenz und Eindeutigkeit des Optimums] \label{4.10}
	Der Punkt
	\[
		x^* := A^{-1} b
	\]
	ist eindeutiges globales Minimum des QP.
	\begin{proof}
		Es gilt
		\[
			\f 12 (x-x^*)^T A(x-x^*) = \f 12 x^TAx - x^TAx^* + \f 12 (x^*)^TAx^*
		\]
		Damit folgt für $x \neq x^*$:
		\begin{align*}
			f(x) - f(x^*) &= \f 12 x^T Ax - \f 12 (x^*)^TAx^* - b^Tx + b^Tx^* + c - c \\
			&= \f 12 x^T Ax - x^T Ax^* + \f 12 (x^*)^T Ax^* + x^T Ax^* - (x^*)^T Ax^* + (x^* -x)^Tb \\
			&= \underbrace{\f 12(x-x^*)^T A(x-x^*)}_{>0} \underbrace{- (x^*-x)^T\underbrace{Ax^*}_{=b} + (x^*-x)^Tb}_{=0} > 0
		\end{align*}
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				Das Vorzeichen „$-b$“ im QP ist willkürlich.
				Mit dieser Wahl ist das QP äquivalent zu einem LGS:
				\[
					x^* \text{ löst QP} \quad \iff \quad x^* \text{ löst LGS } Ax=b
				\]
			\item
				$x^*$ ist striktes lokales Optimum, denn
				\[
					\nabla f(x^*) = Ax^* - b = A(A^{-1}b) -b = 0
				\]
				und $\nabla^2 f(x^*) = A$ ist positiv definit.
			\item
				Falls $A$ positiv definit, aber nicht symmetrisch, verwende den symmetrischen Anteil von $A$ im QP:
				\begin{align*}
					x^T A x 
					&= x^T \underbrace{A_S}_{\f 12 (A + A^T)} x + x^T \underbrace{A_a}_{= \f 12 (A-A^T)} x 
					&= x^T A_S x
				\end{align*}
				denn
				\[
					x^T A_a x = \f 12 (x^T Ax - \underbrace{x^T A^T x}_{x^T A x}) = 0
				\]
		\end{itemize}
	\end{note}
\end{st}

Bei einem QP ist der „line search“, d.h. die 1D-Optimierung entlang einer Halbgeraden explizit lösbar:

\begin{st}[1D-Optimierung] \label{4.11}
	Für $\_x \in \R^n$, $d\in \R^n \setminus \{0\}$ wird
	\[
		\min_{x \in \{\_x + \omega d : \omega \in \R\}} \f 12 x^T A x - b^T x + c
	\]
	gelöst durch $\hat x = \_x + \omega d$ mit
	\[
		\omega := \f {d^T r}{d^T A d} \qquad r := b - A\_x
	\]
	\begin{note}
		Man nennt $r$ \emph{Residuum}.
	\end{note}
	\begin{proof}
		Setze ($A$ symmetrisch)
		\begin{align*}
			g(\omega) :&= \f 12 (\_x+\omega d)^TA(\_x + \omega d) - b^T(\_x + \omega d) +c \\
			&= \f 12 d^T Ad \omega^2 + (\_x Ad - b^T d) + \f 12 \_x^T A\_x - b^T \_x + c  \\
			g'(\omega) &= d^T Ad \omega + \underbrace{(\_x^TA - b^T)d}_{=-d^T r}
		\end{align*}
		Also
		\[
			g'(\omega) = 0 \qquad \iff \qquad \omega = \f {d^T r}{d^T A d}
		\]
	\end{proof}
\end{st}

Die Abstiegsrichtung
\[
	d^{(k)} = -\nabla f(x^{(k)}) = b - Ax^{(k)}
\]
ist explizit verfügbar, also kann das Gradientenverfahren folgendermaßen formuliert werden:

\begin{kor}[Gradientenverfahren für QP] \label{4.12}
	Das Gradienten-Verfahren mit line search lautet für beliebigen Startwert $x^{(0)} \in \R^n$:
	\[
		x^{(k+1)} := x^{(k)} + \omega_k d^{(k)}
	\]
	mit Abstiegsrichtung
	\[
		d^{(k)} := -Ax^{(k)} + b
	\]
	und Schrittweite
	\[
		\omega_k := \f {(d^{(k)})^T d^{(k)}}{(d^{(k)})^T A d^{(k)}}
	\]
\end{kor}

\begin{st}[Orthogonalität der $d^{(k)}$] \label{4.13}
	Für die Abstiegsrichtung $d^{(k)}$ aus Korrolar \ref{4.12} gilt
	\[
		d^{(k)} \orth d^{(k+1)}
	\]
	\begin{proof}
		Für $d^{(k+1)}$ gilt
		\begin{align*}
			d^{(k+1)} &= -Ax^{(k+1)} + b = -A(x^{(k)} + \omega_k d^{(k)}) + b \\
			&= d^{(k)} - \omega_k A d^{(k)}
		\end{align*}
		und damit
		\begin{align*}
			\< d^{(k)} , d^{(k+1)} \> 
			&= \<d^{(k)}, d^{(k)}\> - \f {\<d^{(k)},d^{(k)}}{(d^{(k)})^TAd^{(k)}} \<d^{(k)}, Ad^{(k)}\> \\
			&= 0
		\end{align*}
	\end{proof}
	\begin{proof}
		In der Praxis führt dies zu „zigzagging“ in Abstiegsrichtungen.
	\end{proof}
\end{st}

\begin{df}[$A$-Skalarprodukt und Norm] \label{4.14}
	Zu $A$ symmetrisch und positiv definit definieren wir ein Skalarprodukt auf $\R^n$ durch
	\[
		\<x, y\>_A := x^T A y
		\qquad x,y \in \R^n
	\]
	und die induzierte Norm
	\[
		\|x\|_A := \sqrt{\<x,x\>_A}
		\qquad x \in \R
	\]
	\begin{note}
		\begin{itemize}
			\item
				Die Skalarprodukt-Eigenschaften von $\<\cdot, \cdot\>_A$ sind erfüllt (einfaches Nachrechnen von Bilinearität, Symmetrie und positiver Definitheit).
			\item
				Analog die Norm-Eigenschaften.
		\end{itemize}
	\end{note}
\end{df}

\begin{lem*}[Translationsinvarianz]
	Seien $(x^{(k)}), (\_x^{(k)})$ zwei Sequenzen der Gradienten-Verfahren-Iterierten zu $f(x)$, bzw. $\_f(\_x)$ mit Grenzwert $x^*$, bzw. $\_x^*$ und identischer Schrittwahl-Regel.

	Falls $\_f(\_x) = f(\_x + a)$ und $x^{(0)} = \_x^{(0)} + a$ für $a \in \R$, so gilt
	\[
		x^{(k)} = \_x^{(k)} + a
		\qquad \land \qquad
		x^* = \_x^* + a
	\]
	\begin{proof}
		Wir zeigen die Aussage induktiv.
		Für $k=0$ ist es gerade die Vorraussetzung.
		Sei also $x^{k} = \_x^{(k)} + a$.
		Wir zeigen, dass in den Iterationen
		\begin{align*}
			x^{(k+1)} &:= x^{(k)} + \omega_k d^{(k)} \\
			\_x^{(k+1)} &:= \_x^{(k)} + \_\omega_k \_d^{(k)}
		\end{align*}
		jeweils $\omega_k, \_\omega_k$ und $d^{(k)}, \_d^{(k)}$ übereinstimmen:
		\begin{align*}
			\_d^{(k)} = -\nabla \_f(\_x^{(k)}) = - \nabla f(\_x^{(k)} + a) = - \nabla f(x^{(k)}) = d^{(k)}
		\end{align*}
		$\omega_k = \_\omega_k$ zeigen wir am Beispiel von line search (ähnlich für Armijio):
		\[
			\omega_k = \argmin_{\omega\in\R} f(x^{(k)} + \omega d^{(k)}) = \argmin_{\omega\in \R} f(\_x^{(k)} + a + \omega \_d^{(k)})  = \argmin_{\omega\in \R} \_f(\_x^{(k)} + \omega \_d^{(k)}) = \_\omega_k
		\]
		also
		\[
			x^{(k+1)} = x^{(k)} + \omega_k d^{(k)} = \_x^{(k)} + a + \_\omega_k \_d^{(k)} = x^{(k+1)} +a
		\]
		und damit auch für den Grenzwert $x^* = \_x^* + a$ ($k \to \infty$).
	\end{proof}
\end{lem*}

\begin{st}[Fehlerschranke Gradientenverfahren] \label{4.15}
	Sei $(x^{(k)})_{k\in \N}$ die Sequenz aus Korrolar \ref{4.12}, dann konvergiert die Sequenz gegen das Minimum $x^*$ vom QP und es gilt die Fehlerschranke
	\[
		\|x^{(k)} - x^*\|_A  \le \bigg(\underbrace{1 - \f {2}{\kappa(A) + 1}}_{\text{Reduktionsfaktor}}\bigg)^k \|x^{(0)} - x^*\|_A
	\]
	wobei für positiv definite, symmetrische Matrizen die Konditionszahl $\kappa$ von $A$ gegeben ist durch
	\[
		\kappa(A) := \f {\lambda_{\text{max}}}{\lambda_{\text{min}}}
	\]
	\begin{note}
		\begin{itemize}
			\item
				Falls $\kappa(A)$ sehr groß ist, dann ist der Reduktionsfaktor nahe $1$ (langsame Konvergenz).
			\item
				Falls $\kappa(A)$ klein, dann ist der Reduktionsfaktor klein (schnelle Konvergenz).
			\item
				Falls $\kappa(A) = 1$, so ist der Reduktionsfaktor gleich Null (Konvergenz in einem Schritt).
		\end{itemize}
	\end{note}
	\begin{proof}
		Sei zunächst $f(x) = \f 12 x^T A x$, also $b = 0$, $c = 0$ und $x^* = A^{-1}b = 0$.
		Dann ist
		\begin{align*}
			2 f(x) = x^T A x &= \|x\|_A^2
			&
			\nabla f(x) &= Ax
		\end{align*}
		und
		\[
			x^{(k+1)} 
			= x^{(k)} + \omega_k d^{(k)} 
			= x^{(k)} - \omega_k \nabla f(x^{(k)})
			= (I - \omega_k A) x^{(k)}
		\]
		mit $\omega_k$ derart, dass $\|x^{(k+1)}\|_A^2 = 2 f(x^{(k+1)})$ minimal ist.

		Seien $\lambda_1 \ge \dotsc \ge \lambda_n > 0$ die Eigenwerte von $A$ zu orthonormalen Eigenvektoren $v_1, \dotsc, v_n \in \R^n$.

		Sei $x^{(k)} = \sum_{i=1}^n a_i v_i$, dann ist
		\begin{align*}
			2 f(x^{(k)}) 
			&= \bigg(\sum_{i=1}^n a_i v_i \bigg)^T \underbrace{A \bigg( \sum_{j=1}^n a_j v_j \bigg)}_{\sum_{j=1}^n a_j \lambda_j v_j}
			= \sum_{i,j=1}^n a_i a_j \lambda_j \delta_{ij}
			= \sum_{i=1}^n a_i^2 \lambda_i
		\intertext{und}
			2 f(x^{(k+1)}) 
			&= (x^{(k+1)})^T A x^{(k+1)} \\
			&= (x^{(k)})^T (I- \omega_k A)^T A (I - \omega_k A) x^{(k)} \\
			= \dotsc
			&= \sum_{i=1}^n a^2 \lambda_i \Big(1-\omega_k \lambda_i\Big)^2 \\
			&\le \sum_{i=1}^n a_i^2 \lambda_i (1-\_\omega_k \lambda_i)^2 \qquad \_\omega_k \text{ beliebig} \\
			&\le \Big( \max_{j=1,\dotsc,n} \Big| 1-\_\omega_k \lambda_j \Big| \Big)^2 \sum_{i=1}^n a_i^2 \lambda_i
		\end{align*}
		Also
		\[
			\|x^{(k+1)} \|^2 \le C^2 \|x^{(k)}\|_A^2
			\qquad C := \max_{j=1^n} | 1 - \_\omega_k \lambda_j |
		\]
		Wir zeigen jetzt noch $C \le 1 - \f {2}{\kappa(A)+1} =: \_C$ durch geeignetes $\omega_k$:
		setze
		\[
			\_\omega_k := \f 2{\lambda_1 + \lambda_n}
		\]
		dann ist
		\begin{align*}
			1 - \omega_k \lambda_n 
			\ge \_\omega_k \lambda_j 
			\ge 1 - \_\omega_k \lambda_1 
			= 1 - \f{2\lambda_1}{\lambda_1 + \lambda_n} 
			= \f {\lambda_n - \lambda_1}{\lambda_n + \lambda_1}
			= - \bigg(\f{\lambda_1 - \lambda_n}{\lambda_n + \lambda_1} \bigg)
			= - \bigg( 1 - \f {2\lambda_n}{\lambda_1 + \lambda_n} \bigg)
			= - (1- \_\omega_1 \lambda_n)
		\end{align*}
		Also
		\[
			|1 - \_\omega_k \lambda_j |
			\le | 1 - \_\omega_k \lambda_n |
			= \bigg| 1 - \f{2\lambda_n}{\lambda_1}{\lambda_n} \bigg|
			= 1 - \f {2}{\kappa(A)+1}
			= \_C
		\]
		Also $C \le \_C$ und
		\[
			\|x^{(k+1)}\|_A \le \_C \|x^{(k)}\|_A 
		\]
		und somit mit $x^* = 0$.
		\[
			\|x^{(k)} - x^*\|_A \le \_C^k \|x^{(0)}\|_A
		\]

		Für allgemeine $f(x) = \f 12 x^T Ax - b^Tx +c$ mit $x^* := A^{-1}b$ folgt die Behauptung durch Translationsinvarianz des Gradientenverfahrens:

		Der Ausdruck „$+c$“ ist für das Minimum unwesentlich und kann ignoriert werden, betrachte also $f(x) = \f 12 x^T Ax - b^T x$.

		Definiere $\_f(\_x) := f(\_x + x^*)$ mit Gradientenverfahren-Iterierten $(x^{(k)})$ und $\_x^{(0)} := x^{(k)} - x^*$.
		Nach dem letzten Lemma der Translationsinvarianz ist dann $\_x^{(k)} = x^{(k)} - x^*$ und das Minimum von $\_f(\_x)$ wird in $\_x^* = x^* - x^* = 0$ angenommen.
		Jetzt ist für $\_f(\_x) = \_x\_A\_x + \_b^T\_x$ jedoch das Minimum eindeutig durch $0 = \_x^* = A^{-1}b$ gegeben, also ist $\_b = 0$.

		Für $\_f(\_x) = \f 12 \_x^T A \_x$ gilt die Behauptung bereits, also
		\[
			\|x^{(k)} - x^*\|_A = \|\_x^{(k)}\| \le \_C^k \|\_x^{(0)}\|_A = \_C^k \|x^{(0)} - x^*\|_A
		\]
	\end{proof}
\end{st}

\begin{ex*}[\texttt{demos\textunderscore optimization(5)}]
	Sei $f(x) = \f 12 x^T A x - b^Tx$ mit $A = I, b = (1,1)^T, x^* = (1,1)^T$.
	\begin{itemize}
		\item
			$x^{(1)} = x^*$ unabhängig von $x^{(0)}$, Konvergenz nach einer Iteration.
		\item
			Die Iteration steht im Einklang mit der Fehlerschranke, denn
			\[
				\kappa(A) = \f 11 = 1
				\qquad
				\bigg( 1 - \f 2{\kappa(A) + 1}\bigg) = 0
			\]
			und die Fehlerschranke
			\[
				\|x^{(1)} - x^*\|_A \le 0 \cdot \|x^{(0)} - x^*\| = 0
			\]
	\end{itemize}
	Allgemeiner konvergiert das Gradientenverfahren mit line search für radialsymmetrischen Funktionen, d.h.
	\[
		f(x) = \phi(\|x-x^*\|)
		\qquad \phi : \R \to \R
	\]
	(mit $\phi(r)$ Minimum bei $r=0$) in einem Schritt.
\end{ex*}

\begin{ex*}[\texttt{demos\textunderscore optimization(6)}]
	Sei
	\[
		A = U \begin{pmatrix}
			10 & 0 \\
			0 & 1
		\end{pmatrix} U^T
	\]
	mit Rotationsmatrix $U$, $b = (0,0)^T, x^* = (0,0)$.
	\begin{itemize}
		\item
			Fehler $0$ ist nicht in endlich vielen Schritten erreicht.
		\item
			„zigzagging“ der $d^{(k)}$
		\item
			$\kappa(A) = 10$ mit Fehlerschranke $(1- \f {2}{\kappa(A)+1}) = \f 9{11}$.
			\begin{table}[H]
				\centering
				\begin{tabular}{c|c|c}
					$k$ & $\|x^{(0)} - x^*\|_A$ & $(\f 9{11})^k \|x^{(0)} - x^*\|_A$ \\ \hline
					0 & 2.40311 & 2.40311 \\
					5 & 8.81089 $\cdot 10^{-1}$ & 8.81095 $\cdot 10^{-1}$ \\
					10 & 3.23047 $\cdot 10^{-1}$ & 3.323052 $\cdot 10^{-1}$ \\
					15 & 1.18444 $\cdot 10^{-1} $& 1.18446 $\cdot 10^{-1}$
				\end{tabular}
			\end{table}
	\end{itemize}
\end{ex*}


\subsection{CG-Verfahren}


Finde Lösung $x^*$ des QP, d.h. Lösung des LGS
\[
	Ax = b
\]
mit $A$ symmetrisch und positiv definit, $b \in \R^n$.

Das Gradientenverfahren konvergiert recht langsam.
Im allgemeinen ist der Fehler $0$ nicht mit endlich vielen Schritten erreichbar

Verwende „$A$-konjugierte Richtungen“ (“conjugate gradients”) um die Konvergenz zu verbessern.

\begin{df}[$A$-konjugierte Vektoren] \label{4.16}
	Sei $A \in \R^{n\times n}$ symmetrisch und positiv definit.
	Die Vektoren $\{d^{(k)}\}_{k=0}^n \subset \R^n \setminus \{0\}$ heißen \emph{$A$-konjugiert}, falls
	\[
		(d^{(k)})^T A d^{(l)} = 0
		\qquad 
		\forall k \neq l
	\]
	\begin{note}
		\begin{itemize}
			\item
				Die $A$-Konjugiertheit entspricht also der Orthogonalität bezüglich des Skalarprodukts $\<\cdot, \cdot\>_A$.
			\item
				Das zugehörig Abstiegsverfahren konvergiert (abgesehen von Rundungsfehlern) in endlich vielen Schritten.

		\end{itemize}
	\end{note}
\end{df}

\begin{st}[Konvergenz] \label{4.17}
	Seien $\{d^{(k)}\}_{k=0}^{n-1} \subset \R^n \setminus \{0\}$ $A$-konjugiert.
	Das Abstiegsverfahren mit line search für das QP, d.h.
	\[
		x^{(0)} \in \R^n \text{ beliebig},
		\qquad x^{(k+1)} := x^{(k)} + \omega_k d^{(k)},
		\qquad \omega_k = \f{(d^{(k)})^T r^{(k)}}{\|d^{(k)}\|_A^2},
		\qquad r^{(k)} := b - Ax^{(k)}
	\]
	liefert $x^{(n)} = x^*$ das Minimum des QP bzw. die Lösung des LGS.
	\begin{proof}
		Die $\{d^{(k)}\}$ sind linear unabhängig, denn für $\sum_{i=0}^{n-1} a_i d^{(i)} = 0$ ist
		\begin{align*}
			(d^{(j)})^T A \sum_{i=0}^{n-1} a_i d^{(i)} &= 0  \qquad j= 0, \dotsc, n-1
			\implies \qquad a_j \underbrace{\|d^{(j)}\|_A^2}_{\neq 0} &= 0 
		\end{align*}
		also $a_j = 0$ für $j=0,\dotsc,n-1$.
		Damit bilden die $\{d^{(k)}\}$ eine Basis des $\R^n$.

		Sei $x^{(0)} = \sum_{i=0}^n a_i^{(0)} d^{(i)}$.
		Wegen $x^{(k+1)} = x^{(k)} + \omega_k d^{(k)}$ gilt
		\[
			x^{(k)} = \sum_{i=0}^{k-1} (a_i^{(0)} - \omega_i)d^{(i)} + \sum_{i=k}^{n-1} a_i^{(0)} d^{(i)}
		\]
		also $x^{(1)} = \sum_{i=0}^{n-1} (a_i^{(0)}+\omega_i) d^{(i)}$.
		Formuliere die Behauptung um:
		\begin{align*}
			x^{(n)} = x^* \qquad &\iff \qquad Ax^{(n)} = A x^{(k)} = b \\
			\qquad &\iff \qquad \forall j=0,\dotsc,n-1 : (d^{(j)})^T A x^{(n)} = (d^{(j)})^T b
		\end{align*}
		Aus letzterem folgt
		\begin{align*}
			(d^{(j)})^T A x^{(n)} 
			&= (d^{(j)})^T A \sum_{i=0}^{n-1} (a_i^{(0)}-\omega_i) d^{(i)} \\
			&= \|d^{(j)}\|_A^2 (a_j^{(0)}+\omega_j) \\
			&= \|d^{(j)}\|_A^2 \bigg( a_j^{(0)} - \f{(d^{(j)})^T (Ax^{(j)}-b)}{\|d^{(j)}\|_A^2} \\
			&= \|d^{(j)}\|_A^2 a_j^{(0)} - \underbrace{(d^{(j)})^T Ax^{(j)}}_{= (d^{(j)})^T A d^{(j)}a_j^{(0)}} + (d^{(j)})^T b \\
			&= (d^{j})^T b
		\end{align*}~
	\end{proof}
\end{st}

\begin{lem}[Orthogonalität] \label{4.18}
	Unter den Vorrausetzungen von \ref{4.17} gilt
	\[
		(d^{(k)})^T r^{(l)} = 0
		\qquad \forall k < l < n
	\]
	\begin{proof}
		\begin{align*}
			(d^{(k)})^T r^{(l)}
			&= (d^{(k)})^T (b-Ax^{(k)}) \\
			&= (d^{(k)})^T \bigg( b - A \bigg(x^{(k)} + \sum_{i=k}^{l-1} \omega_i d^{(i)} \bigg) \bigg) \\
			&= (d^{(k)})^T r^{(k)} - \underbrace{(d^{(k)})^T \sum_{i=k}^{l-1} \omega_i A d^{(i)}}_{= \omega_k \|d^{(k)}\|_A^2} \\
			&= (d^{(k)})^T r^{(k)} - \f{(d^{(k)})^T r^{(k)}}{\|d^{(k)}\|_A^2} \|d^{(k)}\|_A^2 = 0
		\end{align*}
	\end{proof}
\end{lem}

Die $d^{(k)}$ kann man z.B. als $A$-orthogonale Basis eines Krylov-Raums wählen.

\begin{df}[Krylov-Raum] \label{4.19}
	Zu einem $A \in \R^{n\times n}$, $r\in \R^n$ und $m\in \N_0$ definieren wir den \emph{Krylov-Unterraum} der Ordnung $m$  durch
	\[
		\scr K_m(A, r) := \Span\{ r, Ar, \dotsc, A^{m-1}r \} \subset \R^n
	\]
\end{df}

\begin{st} \label{4.20}
	Sei $d^{(0)} = r^{(0)} \in \R^n \setminus \{0\}$ beliebig.
	Die Rekursion
	\begin{align*}
		r^{(k+1)} &:= r^{(k)} + \omega_k Ad^{(k)}
		& d^{(k+1)} &:= r^{(k)} - \beta_k d^{(k)} \\
		\omega_k &:= \f {(d^{(k)})^T r^{(k)}}{\|d^{(k)}\|_A^2}
		& \beta_k &:= \f{(d^{(k)})^T Ar^{(k+1)}}{\|d^{(k)}\|_A^2}
	\end{align*}
	erzeugt eine Folge $\{d^{(k)}\}_{k=0}^m$ $A$-konjugierter Vektoren, $d^{(k)} \neq 0$ bis $r^{(m+1)} = 0$.
	Für alle $k$ gilt
	\[
			\scr K_k(A, r^{(0)}) = \Span\{r^{(0)}, \dotsc, r^{(k-1)}\} = \Span \{d^{(0)}, \dotsc, d^{(k-1)} \}
	\]
	\begin{proof}
		Wir zeigen die Gleichung per Induktion.
		Für $k=1$ ist $d^{(0)} = r^{(0)} = A^0r^{(0)}$.

		Die Aussage gelte für $k$. Für $r^{(k)}$ folgt
		\[
			r^{(k)} = \underbrace{r^{(k-1)}}_{\in \scr K_k(A, r^{(0)})} - \omega_{k-1} \underbrace{A d^{(k-1)}}_{\in \scr K_{k+1}(A, r^{(0)})}
		\]
		also ist $r^{(k)} \in \scr K_{k+1}(A, r^{(0)})$ und
		\[
			\scr K_k(A, r^{(0)}) \subset \Span\{r^{(0)}, \dotsc, r^{(k)}\} \subset \scr K_{k+1}(A, r^{(0)})
		\]
		Es gilt $\dim \scr K_{k+1} - \dim \scr K_k \le 1$, weil die Erzeugendensysteme sich nur um einen Vektor unterscheiden.
		Damit muss Gleichheit an einer der beiden Stellen in obiger Teilmengenbetrachtung gelten.
		Gemäß Lemma \ref{4.18} ist $r^{(k)} \orth \Span\{d^{(0)}, d^{(k-1)}\} = \scr K_k$ und $r^{(k)} \neq 0$ also ergibt sich die Gleichheit in der zweiten Relation:
		\[
			\Span\{r^{(0)}, \dotsc, r^{(k)}\} = \scr K_{k+1} (A,r^{(0)})
		\]
		Es ist $r^{(k)} = d^{(k)} + \beta_{k-1} d^{(k-1)}$ und 
		\[
			\Span\{d^{(0)}, \dotsc, d^{(k)}\} 
			= \Span\{d^{(0)}, \dotsc, d^{(k-1)}, r^{(k)}\} 
			= \Span\{r^{(0)}, \dotsc, r^{(k-1)}, r^{(k)}\}
			= \scr K_{k+1} (A, r^{(0)})
		\]
		also ist 
		\[
			\scr K_k(A, r^{(0)}) = \Span\{r^{(0)}, \dotsc, r^{(k-1)}\} = \Span \{d^{(0)}, \dotsc, d^{(k-1)} \}
		\]
		gezeigt.

		Zeige die $A$-Konjugiertheiti der $d^{(k)}$ per Induktion.
		Nach Vereinbarung ist $d^{(0)} = r^{(0)} \neq 0$.

		Seien also $\{d^{(i)}\}_{i=0}^k$ $A$-konjugiert.
		Betrachte $(d^{(i)})^T A d^{(k+1)}$ für $i=0,\dotsc, k$.
		\begin{seg}[$i=k$]
			\begin{align*}
				(d^{(k)})^T A d^{(k+1)} 
				&= (d^{(k)})^T A (r^{(k+1)} - \beta_k d^{(k)}) \\
				&= (d^{(k)})^T A r^{(k+1)} - \f {(d^{(k)})^T A r^{(k+1)}}{\|d^{(k)}\|_A^2} \|d^{(k)}\|_A^2 \\
				&= 0
			\end{align*}
		\end{seg}
		\begin{seg}[$i<k$]
			\begin{align*}
				(d^{(i)})^T A d^{(k+1)}
				&= (d^{(i)})^T A r^{(k+1)} - \f {(d^{(k)})^T A r^{(k+1)}}{\|d^{(k)}\|_A^2} \underbrace{(d^{(i)})^T A d^{(k)}}_{=0} \\
				&= (Ad^{(i)})^T r^{(k+1)} = 0
			\end{align*}
			da 
			\[
				Ad^{(i)} \in \scr K_{i+2}(A, r^{(0)})
				\subset \scr K_{k+1} (A, r^{(0)})
				= \Span\{d^{(0)}, \dotsc, d^{(k)}\} \stackrel{\ref{4.18}}\orth r^{(k+1)}
			\]
		\end{seg}
	\end{proof}
	\begin{note}[Anschauung/Herleitung der Formeln aus \ref{4.20}]
		\begin{itemize}
			\item
				$\omega_k$: optimale Schrittweite für Abstiegsverfahren gemäß \ref{4.11}
			\item
				$r^{(k+1)}$ ist Residuum des Gradientenverfahrens:
				\[
					r^{(k+1)} := b - Ax^{(k+1)}
					= b - A(x^{(k)} + \omega_k d^{(k)})
					= r^{(k)} - \omega_k A d^{(k)}
				\]
			\item
				$d^{(k+1)}$ und $\beta_k$ können als Orthogonalisierungsschritt aufgefasst werden gemäß folgendem Korollar.
		\end{itemize}
	\end{note}
\end{st}

\begin{kor} \label{4.21}
	Die Basis $\{d^{(k)}\}_{k=0}^m$ ist die Gram-Schmidt-Orthogonalisierte Basis, welche aus $\{r^{(k)}\}_{k=0}^m$ entsteht bezüglich dem $A$-Skalarprodukt $\<\cdot,\cdot\>_A$.
	\begin{proof}
		Sei $\{\_d^{(k)}\}$ die Gram-Schmidt-orthogonalisierte Sequenz aus $\{r^{(k)}\}$ bezüglich $\<\cdot,\cdot\>_A$ aus $\{r^{(k)}\}$ generiert.
		Zeige per Induktion $d^{(k)} = \_d^{(k)}$.
		Für $k=0$ ist $\_d^{(0)} = r^{(0)} = d^{(0)}$.

		Gelte  $d^{(k)} = \_d^{(k)}$, dann ist
		\[
			\_d^{(k+1)} := r^{(k+1)} - Pr^{(k+1)}
		\]
		mit $P : \R^n \to \Span\{d^{(0)}, d^{(k)}\}$ eine $A$-orthogonale Projektion, d.h.
		\[
			\<x-Px, d^{(i)}\>_A = 0
			\qquad i = 0, \dotsc, k
		\]
		Sei zunächst $i<k$, dann ist
		\begin{align*}
			\< r^{(k+1)} - \beta_k d^{(k)}, d^{()} \>_A
			&= \< r^{(k+1)}, d^{()} \>_A - \beta_k \underbrace{\<d^{(k)}, d^{()} \>_A}_{=0} \\
			&= (r^{(k+1)})^T A d^{(i)}
			= 0
		\end{align*}
		weil $r^{(k+1)} \orth \scr K_{k+1} (A, r^{(0)}) \ni Ad^{(i)}$.

		Für $i = k$ ist
		\begin{align*}
			\<r^{(k+1)} - \beta_k d^{(k)}, d^{(k)}\>_A
			= \< r^{(k+1)}, d^{(k)}\>_A - \f{(d^{(k)})^T A r^{(k+1)}}{\|d^{(k)}\|_A^2} \<d^{(k)},d^{(k)}\>_A
			= 0
		\end{align*}
		also $P r^{(k+1)} = \beta_k d^{(k)}$ und damit $\_d^{(k+1)} = d^{(k+1)}$.
	\end{proof}
	\begin{note}
		Dieses Korrolar stellt eine Vereinfachung von $\omega_k$ und $\beta_k$ dar.
		\begin{itemize}
			\item
				Es gilt
				\[
					\omega_k = \f {(d^{(k)})^T r^{(k)}}{\|d^{(k)}\|_A^2} = \f {(r^{(k)}-\beta_{k-1}d^{(k-1)})^T r^{(k)}}{\|d^{(k)}\|_A^2} = \f{\|r^{(k)}\|^2}{\|d^{(k)}\|_A^2}
				\]
			\item
				Es gilt
				\begin{align*}
					(d^{(k)})^T A r^{(k+1)}
					&= (A d^{(k)})^T r^{(k+1)} \\
					\intertext{da $r^{(k+1)} := r^{(k)}-\omega_k A d^{(k)}$}
					&= \f 1{\omega_k} (r^{(k)} - r^{(k+1)})^T r^{(k+1)}
					&\stack{r^{(k+1)} \orth r^{(k)}} =   - \f {\|r^{(k-1)}\|^2}{\omega_k} \\
					&= - \f {\|d^{(k)}\|_A^2}{\|r^{(k)}\|^2} \|r^{(k-1)}\|^2
				\end{align*}
				also
				\[
					\beta_k = \f{(d^{(k)})^T A r^{(k+1)}}{\|d^{(k)}\|_A^2} = - \f{\|r^{(k+1)}\|^2}{\|r^{(k)}\|^2} =: - \gamma_k
				\]
		\end{itemize}
	\end{note}
\end{kor}

\begin{df}[CG-Verfahren] \label{4.22}
	Sei $A \in \R^{n\times n}$ symmetrisch und positiv definit, $b \in \R^n$.
	Wähle $x^{(0)} \in \R^n$, $d^{(0)} := r^{(0)} := b - Ax^{(0)}$ und setze iterativ
	\begin{align*}
		x^{(k+1)} &:= x^{(k)} + \omega_k d^{(k)} \\
		\omega_k &:= \f {\|r^{(k)}\|^2}{\|d^{(k)}\|_A^2} \\
		r^{(k+1)} &:= r^{(k)} - \omega_k Ad^{(k)} \\
		\gamma_k &:= \f {\|r^{(k+1)}\|^2}{\|r^{(k)}\|^2} \\
		d^{(k+1)} &:= r^{(k+1)} + \gamma_k d^{(k)}
	\end{align*}
	solange bis $\|r^{(k+1)}\| = 0$.
	\begin{note}
		\begin{itemize}
			\item
				Zu $f(x) = \f 12 x^T A x - b^T x$ ist
				\[
					r^{(k)} := b - Ax^{(k)} = - \nabla f(x^{(k)})
				\]
				die $d^{(k+1)}$ daher die „A-konjugierte-Gradienten“.
			\item
				Falls $\|r^{(k+1)}\| = 0$, dann ist $\nabla f(x^{(k+1)}) = 0$, als wurde das Optimum des QPs gefunden.
			\item
				Das CG-Verfahren ist ein Krylovraum-Verfahren
			\item
				Es ist nur eine Matrixmultiplikation erforderlich, für große Matrizen anwendbar.
		\end{itemize}
	\end{note}
\end{df}

\begin{st}[Optimierung über verschobenen Krylovraum] \label{4.23}
	Das CG-Verfahren aus Definition \ref{4.22} liefert
	\[
		x^{(k)} = \argmin_{x \in x^{0} + \scr K_k(A, r^{(0)})} f(x)
	\]
	\begin{proof}
		Zeige per Induktion.
		Für $k=0$ ist $\scr K(A, r^{(0)}) = \{0\}$ und $x^{(0)} = \argmin_{x \in \{x^{(0)}\}} f(x)$ ist klar.

		Sei $x^{(k)}$ die $k$-te CG-Iterierte und Minimierer.
		Jedes $\_x^{(k+1)} \in x^{(0)} + \scr K_{k+1}(A, r^{(0)})$ ist zerlegbar:
		\begin{align*}
			\_x^{(k+1)} = \_x^{(k)} + ad^{(k)} 
		\end{align*}
		für $\_x^{(k)} \in x^{(0)} + \scr K_k (A, r^{(0)})$ und $a \in \R$.
		Also
		\begin{align*}
			f(\_x^{(k+1)}) 
			&= f(\_x^{(k)}) + a(d^{(k)})^T A \_x^{(k)} + \f 12 a^2 (d^{(k)})^T A d^{(k)} - a(d^{(k)})^T b \\
			&= f(\_x^{(k)}) + a(d^{(k)})^T A (x^{(k)} + \tilde x^{(k)}) + \f 12 a^2 (d^{(k)})^T A d^{(k)} - a(d^{(k)})^T b \\
			&= f(\_x^{(k)}) + a (d^{(k)})^T A x^{(k)} + \tilde x^{(k)}) + \f 12 a^2 (d^{(k)})^T A d^{(k)} - a(d^{(k)})^T b \\
			\intertext{mit $\tilde x^{(k)} := \_x^{(k)}- x^{(k)} \in \scr K_k(A, r^{(0)}) = \Span \{d^{(0)}, \dotsc, d^{(k-1)}\}$}
			&= f(\_x^{(k)}) + g(a)
		\end{align*}
		mit $g(a) = (d^{(k)})^T (Ax^{(k)} + b)a + \f 12 (d^{(k)})^T A d^{(k)} a^2$.
		Die Minimierung von $f(\_x^{(k+1)}$ ist damit entkoppelt.
		\begin{align*}
			\_x^{(k+1)} = \argmin_{x\in x^{(0)} + \scr K_{k+1}}
			\qquad &\iff \qquad \_x^{(k)} = \argmin_{x \in x^{(0)} + \scr K_k} f(x) \qquad\land\qquad a^* := \argmin_{a\in \R} g(a) \\
			\qquad &\iff \qquad \_x^{(k)} = x^{(k)} \qquad \land \qquad g'(a^*) = 0 \\
			\qquad &\iff \qquad \_x^{(k)} = x^{(k)} \qquad \land \qquad a^* = \f {(d^{(k)})^T r^{(k)}}{\|d^{(k)}\|_A^2} \\
			\qquad &\iff \qquad \_x^{(k+1)} = x^{(k)}  + \omega_k d^{(k)} = x^{(k+1)}
		\end{align*}
	\end{proof}
\end{st}

\begin{st}[Fehlerschranke CG-Verfahren] \label{4.24}
	Für das CG-Verfahren gilt für den Fehler zu $x^* := A^{-1} b$
	\[
		\|x^{(k)} - x^*\|_A \le 2 \bigg( 1 - \f 2{{\sqrt {\kappa(A)}}+1} \bigg)^k \|x^{(0)} - x^*\|_A
	\]
	\begin{proof}
		Ähnlich wie \ref{4.15}, sie Plato 11.5
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				Obiger Fehler ist für große $\kappa(A)$ und $k$ besser als der Faktor aus \ref{4.15} des Gradientenverfahrens.
			\item
				Das CG-Verfahren weißt kein „zigzagging“ auf, da alle $d^{(k)}$ linear unabhängig sind.
		\end{itemize}
	\end{note}
\end{st}

\begin{ex*}[\texttt{demos\textunderscore optimization(7)}]
	Sei $f(x) = \f 12 x^T A x$ und $A = U I U^T$ mit Rotationsmatrix $U$.

	Nach zwei Schritten ist der Fehler 0.
\end{ex*}


\subsection{PCG-Verfahren}


Falls $\kappa(A)$ sehr groß ist, benötigt das CG-Verfahren viele Iterationen.
Wende also Vorkonditionierung an (Preconditioning).

Sei dazu $L$ reguläre, untere Dreiecksmatrix, dann ist
\[
	Ax = b
	\qquad \iff \qquad
	LAL^T z = Lb
\]
Setze jetzt $A_z := LAL^T$ und $b_z := Lb$ (mit $A_z$ symmetrisch und positiv definit) an.
Wähle $L$ so, dass $\kappa(A_z)$ klein und $L$ dünn besetzt ist.

Wende das CG-Verfahren für $z$ zur Lösung von $A_z z = b_z$ an.
Nenne dessen Größen $z^{(k)}, d_z^{(k)}, r_z^{(k)} , \omega_k, \gamma_k$.
Berechne nicht $z^{(k)}, d_z^{(k)}, r_z^{(k)}$, sondern direkt
\begin{align*}
	x^{(k)} &:= L^{-T}z^{(k)} \\
	d^{(k)} &:= L^{-T}d_z^{(k)} \\
	r^{(k)} &:= L^{-T}r_z^{(k)}
\end{align*}
mit Hilfsvariable $s(k) := L r_z^{(k)}$.
Es taucht statt $L$ die einzelne Matrix $M := LL^T$ auf (genannt \emph{Vorkonditionierer}).
Es muss in jeder Iteration ein LGS mit Matrix $M$ gelöst werden.

\begin{df}[PCG-Verfahren] \label{4.25}
	Setze $s^{(0)} := b - Ax^{(0)}, d^{(0)} = r^{(0)} = M^{-1}s^{(0)}$ und setze iterativ
	\begin{align*}
		\omega_k &:= \f {(s^{(k)})^T r^{(k)}}{\|d^{(k)}\|_A^2} \\
		x^{(k+1)} &:= x^{(k)} + \omega_k d^{(k)} \\
		s^{(k+1)} &:= s^{(k)} - \omega_k A d^{(k)} \\
		r^{(k+1)} &:= M^{-1}s^{(k+1)} \\
		\gamma_k &:= \f {(s^{(k+1)})^T r^{(k+1)}}{(s^{(k)})^T r^{(k)}} \\
		d^{(k+1)} &:= r^{(k+1)} + \gamma_k d^{(k)}
	\end{align*}
	\begin{note}
		Für Details, siehe Deufhard/Hohmann 8.4 oder Jarre/Stoer 6.3.1.
	\end{note}
	\begin{note}[Wahl von $M$]
		\begin{itemize}
			\item
				Schlechte Wahlen sind $M = I$ (CG-Verfahren), $M=A$ ($A_z = I$, Konvergenz in einer Iteration, aber $Mr^{(0)} = s^{(0)}$ ist so kompliziert wie das ursprüngliche QP).
			\item
				Eine mögliche Wahl ist $M := \diag(A)$ (Diagonalvorkonditionierung)
		\end{itemize}
	\end{note}
\end{df}

\begin{ex*}[\texttt{demos\textunderscore optimization(8)}]
	Sei $n = 50$ mit
	\[
		A = \begin{pmatrix}
			2 & - 1 & \cdots & 0 \\
			\vdots & \ddots & \ddots & -1 \\
			0 & \cdots & -1 & 51 
		\end{pmatrix}
	\]
\end{ex*}


\subsection{Newton-Verfahren}


Betrachte für $f \in C^2(\R^n, \R)$:
\[
	\min_{x \in \R^n} f(x)
\]
Finde dazu den stationären Punkt $x^*$ mit Newton-Verfahren zur Nullstellensuche von $F(x) = \nabla f(x)$, also $DF(x) = \nabla^2 f(x)$.

\begin{kor}[Newtonverfahren zur Optimierung] \label{4.26}
	Falls $f \in C^2 (\R^n, \R)$ mit $x^*$ striktes lokales Minimum, so ist das Newton-Verfahren mit $x^{(0)} \in \R^n$:
	\[
		x^{(k+1)} := x^{(k)} - \Big( \nabla^2 f(x^{(k)}) \Big)^{-1} \nabla f(x^{(k)})
	\]
	lokal quadratisch konvergent gegen $x^*$.
	\begin{proof}
		Klar mit \ref{3.23}, denn $\nabla f(x^*) = 0$ und $\nabla^2 f(x^*)$ positiv definit, also regulär.
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item
				Das Newton-Verfahren liefert nicht immer eine Abstiegsrichtung und die Konvergenz ist nur lokal.
			\item
				Das Gradientenverfahren hat einen großen Konvergenzbereich, konvergiert aber langsam
			\item
				Kombination zu „globalisierten Newton“-Verfahren: erst Gradienten-Schritte, bis nahe am Optimum, dann Newton-Schritte um schnelle lokale Konvergenz zu erhalten.
			\item
				Für große $n$ ist die Berechnung von $\nabla^2 f(x)$  problematisch.
		\end{itemize}
	\end{note}
\end{kor}


\subsection{Gauß-Newton-Verfahren}


Betrachte ein spezielles Optimierungsproblem: „Nichtlineare Ausgleichsrechnung/Interpolation“.
Gegeben sei (siehe Kapitel 1) eine parametrisierte skalare Funktion $\Phi(x, a)$  für $x \in \R^n$ nichtlinear und $C^2$ bezüglich des Parametervektors $a \in \R^p$.
Weiter sind Daten $x_1, \dotsc, x_m \in \R^n$ und Zielwerte $y_1, \dotsc, y_m \in \R$ gegeben.

Suche $a \in \R^p$ als Lösung von
\[
	\min_{a \in \R^p} \sum_{i=1}^m \Big| \Phi(x,a) - y_i \Big|^2
\]
(Optimierung über $a$)
oder kompakter mit dem Residuenvektor
\[
	R(a) := \begin{pmatrix}
		\Phi(x_1,a) - y_1 \\
		\vdots \\
		\Phi(x_m, a) - y_m
	\end{pmatrix}
\]
suche $a \in \R^p$ als Lösung von
\[
	\min_{a \in \R^p} \|R(a)\|^2
\]
Die Ableitungen sind berechenbar, für Newton ist die Hesse-Matrix erforderlich.
Löse zur Vereinfachung die Sequenz von linearen Ausgleichsproblemen ohne Hesse-Matrix.

Falls $\Phi(x,a)$ linear in $a$ ist:
\[
	(\Phi(x_i, a))_{i=1}^m = A a
	\qquad y = (y_i)_{i=1}^m
\]
dann entspricht die Lösung von $\min_{a} \| Aa - y\|^2$ der Lösung der Normalengleichung $A^TA a = A^T y$.

Sei in unserem Fall $a^{(k)}$ die Approximation des nichtlinearen Ausgleichsproblems.
Bestimme $a^{(k+1)}$ als Lösung eines linearisierten Problems:
\[
	\min_{a^{(k+1)}} \sum_{i=1}^m \Big| \Phi(x_i, a^{(k)}) + \nabla_a(\Phi(x_i, a))^T (\underbrace{a^{(k+1)} - a^{(k)}}_{=: \delta^{(k)}}) - y_i \Big|^2
\]
mit
\[
	DR (a^{(k)}) = \begin{pmatrix}
		\nabla_a \Phi(x_1, a^{(k)})^T \\
		\vdots \\
		\nabla_a \Phi(x_m, a^{(k)})^T \\
	\end{pmatrix}
\]

\begin{df}[Gauß-Newton-Verfahren] \label{4.27}
	Wähle $a^{(0)} \in \R^p$ und setze iterativ
	\begin{align*}
		a^{(k+1)} &:= a^{(k)} + \delta^{(k)} \\
		d^{(k)} &:=  \argmin_{\delta} \Big\| R(a^{(k)}) + DR(a^{(k)}) \delta \Big\|^2
	\end{align*}
\end{df}

\begin{ex*}[\texttt{demos\textunderscore optimization(9)}]
	Sei
	\[
		\Phi(x, a) = a_1 e^{-|x - a_2|^2 \cdot 50} + a_3 e^{-|x-a_2|^2 \cdot 50}
	\]
\end{ex*}

\end{document}

\documentclass[a4paper,10pt]{scrartcl}
\usepackage{mathe-vorlesung}

\title{Numerische Mathematik 1}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Approximation und Interpolation}

Sei $V= \{\Phi(x;a_0,\dotsc, a_n) | a_0,\dotsc,a_n\in \R\}$ eine Menge stetiger Funktionen für $x\in \R$, deren Elemente durch Parameter $a_0,\dotsc,a_n$ parametrisiert sind.
Seien Stützstellen $\{x_i\}_{i=0}^n\subset \R$ und Zielwerte $\{t_i\}_{i=0}^n$ gegeben (z.B. Messungen oder Funktionwerte einer komplizierten Funktion $f_i=f(x_i)$, oder Ergebnis eines Computerprogramms).

Finde Parameter $a_0,\dotsc, a_n$, so dass
\[
	\Phi(x;a_0,\dotsc, a_n) = f_i \qquad i= 0,\dotsc,n
\]
(„Interpolation“) oder allgemeiner:
\[
\sum_{i=0}^n(\Phi(x_i; a_0,\dotsc a_m) -f_i)^2 \qquad \text{minimal}
\]
(„Least-squares-Approximation“)

\begin{ex}
	\begin{enumerate}
		\item 
			Polynominterpolation:
			\[
				\Phi(x; a_0,\dotsc, a_n) = \sum_{k=0}^na_kx^k \in \P_n
			\]
		\item
			Trigonometrische Interpolation:
			\[
				\Phi(x;a_0,\dotsc,a_m,b_1,\dotsc, b_m) = \f {a_0}2 + \sum_{k=1}^m (a_k\cos kx + b_k \sin kx)
			\]
		\item
			Spline-Interpolation:
			Sei $a=x_0\lt x_1 \lt \dotsb \lt x_n = b$, $q,r\in \N_0$, $q\le r$
			\[
			V := \{f\in \C^q([a,b]) | f_{[x_i,x_{ibm}]}\in \P_r, i=0,\dotsc,n-1\}
			\]
			(global $q$-mal stetig differenzierbare Fuktionen, stückweise polynomial vom Grad $r$)
		\item
			Exponentielle Interpolation (nichtlinear)
			\[
			\Phi(x;a_0,\dotsc, a_m,\lambda_0,\dotsc, \lambda_m) = \sum_{k=0}^m a_ke^{\lambda_k x}
			\]
		\item
			Rationale Interpolation
			\[
				\Phi(x; a_0,\dotsc, a_m, b_0,\dotsc, b_{\_m}) = \f {a_0 + a_1x + \dotsb + a_m x^m}{b_0 + b_1x + \dotsb + b_{\_m}x^{\_m}}
			\]
	\end{enumerate}
\end{ex}

Potentielle Fragestellungen:
\begin{itemize}
	\item 
		Ist die Approximations-/Interpolationsaufgabe zu gegebenem $V$ und Daten $(x_i,f_i)$ lösbar? eindeutig?
	\item
		Wie finden wir algorithmisch die Koeffizienten?
	\item
		Können wir Fehleraussagen treffen?
	\item
		Was ist die optimale Stützstellenwahl?
	\item
		\dots
\end{itemize}


\subsection{Polynominterpolation}

Sei $\{x_i\}_{i=0}^n$, ${f_i}_{i=0}^n$ gegeben mit $x_i\neq x_j$ für $i\neq j$.

\begin{st}[Existenz und Eindeutigkeit]
	\label{st:1.1}
	Es existiert genau ein Polynom $p\in \P$ mit $p(x_i)=f_i$ für $i=0,\dotsc,n$.
	\begin{proof}
		$p(x) = \sum_{k=0}^n a_k x^k$ löst das Interpolationsproblem $p(x_i)=f_i$ genau dann, wenn $a=(a_k)_{k=0}^n \in \R^{n+1}$ löst $a_0 +a_1x_i^1 + \dotsb a_nx_i^n = f_i$ für $i=0,\dotsc,n$.
		Das ist äquivalent zu $Aa=f$ mit
		\[
		A= \begin{pmatrix} 1 & x_0 & x_0^2 & \hdots & x_0^n \\
		\vdots  \\
		1 & x_n & x_n^2 & \hdots & x_n^n\end{pmatrix}, f= \begin{pmatrix}f_0 \\ \vdots \\ f_n\end{pmatrix}
		\]
		Wir zeigen, dass $A$ regulär ist, bzw. $\ker(A) = \{0\}$.<br />
		Sei $\_a \in \R^{n+1}, A\_a=0$, dann erfüllt
		$\_p(x) := \sum_{k=0}^n\_a_k x^k$ die Gleichung $\_p(x_i)=0$.
		$\_p$ ist Polynom von Grad $n$ und hat $n+1$ Nullstellen. Also $\_p=0 \implies \_a_k=0 \implies \_a=0$
	\end{proof}
	\begin{note}
		\begin{enumerate}
			\item 
				Darstellung in Monombasis $p(x)= \sum_{k=0}^n a_kx^k$ heißt \emph{Normalform} des Interpolationsproblems.
			\item
				Die Matrix $A$ aus dem Beweis ist die \emph{Vandermondematrix}, typischerweis schlecht konditionirt und voll besetzt.
				Daher ist das zugehörige LGS $Aa=f$ recht aufwändig zu lösen.
			\item
				Bei Verwendung anderer Basen ist das Interpolationsproblem leichter zu lösen.
			\item
				Allgemeine lineare Interpolation:
				Sei $\{\phi_k\}_{k=0}^n$ linear unabhängig. Dann gilt
				\begin{align*}
					p(x) = \sum_{k=0}^n a_k \phi_k(x) \text{löst die Interpolationsaufgabe}
					\iff a\in \R^{n+1} \text{löst} Aa=f \text{mit} f=(f_i)_{i=0}^n \text{und} A= \begin{pmatrix}\phi_0(x_0) & \hdots & \phi_n(x_0)\\
					\end{pmatrix}
				\end{align*}
		\end{enumerate}
		
	\end{note}
\end{st}

\begin{st}[Lagrange-Form]
	\label{1.2}
	Die \emph{Lagrange-Polynome}
	\[
	L_k^n(x) = \prod_{i=0, i\neq k}^n \f {x-x_i}{x_k-x_i} \qquad k=0,\dotsc,n
	\]
	erfüllen
	\[
	L_k^n(x_i) = \delta_{ik} \qquad i,k=0,\dotsc, n
	\]
	und bilden eine Basis für $\P_n$ und das Interpolationspolynom hat sogenannte <em>Lagrange-Form</em>
	\[
	p(x) = \sum_{k=0}^n f_k L_k^n(x)
	\]
	\begin{proof}
		siehe NLA.		
	\end{proof}
	\begin{note}
		\begin{enumerate}
			\item Wegen $L_k^n(x_i)=\delta_{ik}$ nennt man die $\{L_k\}_{k=0}^n$ eine nodale Basis
			\item Lösen eines LGS entfällt, da zugehörige Matrix $A=I$.
			\item $L_k^n(x)$ recht aufwändig auszuwerten
			\item Die Basen sind nicht hierarchisch, d.h. ${L_k^n} \not\subset \{L_k^{n'}\}$ für $n'\gt n$.
		\end{enumerate}
	\end{note}
\end{st}

\begin{df}[Newton-Polynome, Newton-Form]
	\label{df:1.3}
	Wir nennen
	\[
	N_k^n(x) := \prod_{j=0}^{k-1}(x-x_j) \qquad k=0,\dotsc,n
	\]
	\emph{Newton-Polynome} und die Darstellung des Interpolationspolynoms
	\[
	p(x) = \sum_{k=0}^n a_k N_k^n(x)
	\]
	\emph{Newton-Form} der Interpolierenden
\end{df}

\begin{lem}[Eigenschaften der Newton-Polynome]
	\label{lem:1.4}
	\begin{enumerate}
		\item $N_k^n(x_j) = 0$ für $0\le j \le k$ 
		\item ${N_k^n}_{k=0}^n$ bilden Basis für $\P_n$
		\item Die Basen sind hierarchisch: $\{N_k^n\}\subset \{N_k^{n'}\}$ für $n'\gt n$
	\end{enumerate}
	\begin{proof}
		1. und 3. sind klar.

		$N_k^n \in \P_n$ ist klar, zeige lineare Unabhängigkeit.
		Sei $a\in \R^{n+1}$ mit $0=\sum_{k=0}^n a_k N_k^n(x)$.
		Angenommen $a\neq 0$, wähle $k_0 := \max\{k | a_k \neq 0\}$.
		Also
		\[
			0 \neq N_{k_0}^n = - \f 1{a_{k_0}} \sum_{k=0}^{k_0-1}a_k N_k^n(x)
		\]
		Also hat $N_{k_0}$ Grad $k_0$ und ist Summe von Polynomen niedrigeren Grades, ein Widerspruch.
		Damit ist $a=0$ und $\{N_k^n\}$ linear unabhängig.
	\end{proof}
	\begin{note}
		\begin{itemize}
			\item Die Interpolationsmatrix ist eine untere Dreiecksmatrix
			\item LGS ist durch Vorwärtseinsetzen in $\mathcal O(n^2)$ lösbar
			\item Alternative Berechnung: Dividierte Differenzen (später)
		\end{itemize}
	\end{note}
\end{lem}

\begin{ex*}
	Sei $n=2$.	
	\begin{tabular}{l|rrr}
		$x_i$ & 1 & 2 & 3  \\ \hline
		$f_i$ & 2 & 3 & 6 
	\end{tabular}
	\\
	$N_0^2(x) = 1, N_1^2(x) = (x-1), N_2^2(x) = (x-1)(x-2)$
	Das zugehörige LGS ist
	\[
	\begin{pmatrix}1&0&0\\1&1&0\\1&2&2\end{pmatrix}\begin{pmatrix}a_0\\a_1\\a_2\end{pmatrix} = \begin{pmatrix}2\\3\\6\end{pmatrix}
	\]
	Es ergibt sich $a_0=2,a_1=1, a_2=1$ und damit das Interpolationspolynom
	\[
		p(x) = 2 + 1\cdot(x-1) + 1(x-1)(x-2) = x^2 - 2x+3
	\]
\end{ex*}

\begin{st}[Rekursionsformel]
	\label{st:1.5}
	Sei $p_{ij}\in \P_j$ das Interpolationspolynom zu den Daten $(x_i,f_l)$, $l=i,\dotsc,i+j$.
	Dann gilt für $j\ge 1$
	\[
		p_{i,j}(x) = \f {(x-x_i)p_{i+1,j-1} - (x-x_{i+j}p_{i,j-1}}{x_{i+j}-x_i}
	\]
	\begin{proof}
		Es gilt
		\begin{align*}
			p_{i+1,j-1}(x_i) &= f_l \qquad l=i+1,\dotsc,i+j\\
			p_{i,j-1}(x_i) &= f_l \qquad l=i,\dotsc,i+j-1
		\end{align*}
		Mit
		\[
			q(x) := \f {(x-x_i)p_{i+1,j-1} - (x-x_{i+j}p_{i,j-1}}{x_{i+j}-x_i}
		\]
		gilt $q\in \P_j$ und die Randpunkte werden interpoliert:
		\begin{align*}
			q(x_i) &= p_{i,j-1}(x_i) = f_i\\
			q(x_{i+j} &= p_{i+1,j-1}(x_{i+j}) = f_{i+j}
		\end{align*}
		für die Zwischenstellen $l=i+1,\dotsc,i+j-1$ gilt
		\begin{align*}
			q(x_l) &= \f {(x_l-x_i)p_{i+1,j-1}(x_l) - (x_l-x_{i+j})p_{i,j-1}(x_i)}{x_{i+j}-x_i}\\
			&= \f {(x_l-x_i)f_l - (x_l-x_{i+j}f_l)}{x_{i+j}-x_i} = f_l
		\end{align*}
		Damit ist $q(x)$ ein Interpolationspolynom und wegen der Eindeutigkeit $p_{i,j}=q$.
	\end{proof}
\end{st}

\begin{df}[Dividierten Differenze]
	\label{df:1.6}
	Wir definieren rekursiv
	\begin{align*}
		f[x_i] &:= f_i\\
		f[x_i,\dotsc,x_{i+j}] &:= \f {f[x_{i+1},\dotsc,x_{i+j}] - f[x_i,\dotsc,x_{i+j-1}]}{x_{i+j}-x_i}
	\end{align*}
\end{df}

\begin{st}[Newton-Form über Dividierten Differenzen]
	\label{st:1.7}
	Das Interpolationspolynom hat Darstellung
	\[
		p(x) = f[x_0]N_0^n(x) + f[x_0,x_1]N_1^n(x) + \dotsb + f[x_0,\dotsc,x_n]N_n^n(x)
	\]
	\begin{proof}
		Sei die Newton-Form
		\[
			p(x) := a_0 + a_1(x-x_0) + a_2(x-x_0)(x-x_1) + \dotsb + a_n\prod_{i=0}^{n-1}(x-x_i)
		\]
		gegeben.
		Dann ist
		\[
			p_{0,k}(x) := a_0 + a_1(x-x_0) + \dotsb + a_k\prod_{i=0}^{k-1}(x-x_i)
		\]
		Interpolierende zu den Daten $(x_j,f_j)$ für $j=0,\dotsc,k$, denn
		\begin{align*}
			f_j = p(x_j) &= \underbrace{\sum_{k'=0}^ka_{k'}N_{k'}^n(x_j)}_{p_{0,k(x_j)}} + \sum_{k'=k+1}^n a_{k'}\underbrace{\prod_{i=0}^{k'-1}(xj-x_i)}_{=0}\\
			&= p_{0,k}(x_j)
		\end{align*}
		also ist $a_k$ Koeffizient vor dem höchsten Term in $p_{0,k}$.
		Sei $a_{i,j}$ der Koeffizient des höchsten Terms in $p_{i,j}$.

		Es ergibt sich mit \ref{st:1.5} für den höchsten Koeffizienten:
		\begin{align*}
			a_{i,j} &= \f {a_{i+1,j-1} - a_{i,j-1}}{x_{i+j}-x_i}\\
			a_{i,0} &= f_i
		\end{align*}
		Per Induktion über $j$ folgt leicht, dass
		\[
			a_{i,j} = f[x_i,\dotsc, x_{i+j}]
		\]
		also insbesondere
		\[
			a_k = a_{0,k} = f[x_0,\dotsc, x_k]
		\]
	\end{proof}
\end{st}

\subsection{Neville-Schema}

Die Rekursion aus \ref{df:1.6} kann man als Schema darstellen:

\begin{tabular}{cccc}
$x_0$ & $f[x_0]=f_0$ & $f[x_0,x_1]$ & $f[x_0,x_1,x_2]$ \\
$x_1$ & $f[x_1]=f_1$ & $f[x_1,x_2]$ &  \\
\vdots & \vdots & \vdots\\
$x_n$ & $f[x_n]=f_n$ &\\
\end{tabular}

\begin{ex*}
	Sei $n=2$.	
	\begin{tabular}{l|rrr}
		$x_i$ & 1 & 2 & 3  \\ \hline
		$f_i$ & 2 & 3 & 6 
	\end{tabular}
	
	\fixme[Neville-Schema]
	Es ergibt sich das Interpolationspolynom
	\[
		p(x) = 2+ 1(x-1) + 1(x-1)(x-2) = x^2 -2x+3
	\]
\end{ex*}

\begin{note}
	\begin{itemize}
		\item Falls nach der Interpolation ein neues Datenpaar $(x_{n+1},f_{n+1})$ zur Verfügung steht, kann das alte Neville-Schema wiederverwendet werden.
			Es muss nur unterhalb der Diagonalen jeweils ein neuer Wert berechnet werden.
		\item
			Dividierte Differenzen $f[x_i,\dotsc,x_{i+j}]$ sind symmetrisch bezüglich den Daten, d.h. bei Permutation $\tau: \{i,\dotsc,i+j\} \to \{i,\dotsc,i+j\}$ gilt
			\[
				f[x_{\tau(i)},\dotsc,x_{\tau(i+j)}] = f[x_i,\dotsc,x_{i+j}]
			\]
	\end{itemize}
\end{note}

\subsection{Punkterweiterung der Interpolierenden}

\begin{enumerate}[a)]
	\item 
		Falls die Newton-Form vorliegt, erlaubt das \emph{Horner-Schema} die effiziente Auswertung durch geschickte Klammerung.
		\begin{align*}
			p(x) &= a_0 + a_1(x-x_0) + \dotsb + a_n\prod_{i=0}^{n-1}(x-x_i)
				&= (\dotso(a_n(x-x_{n-1} + a_{n-1})(x-x_{n-2}) + a_n-2) + \dotsb + a_0
		\end{align*}
		Satt $\mathcal(n^2)$ Produkte sind nur  $\O(n)$ Produkte erforderlich.
	\item
		Man kann das Interpolationspolynom auswerten, ohne es vorliegen zu haben.
		Verwende dazu das Neville-Schema für die Rekursion aus Satz \ref{st:1.5} nur für Stelle $x$.
		\begin{tabular}{cccc}
		$x_0$ & $f_0=p_{0,0}(x)$ & $p_{0,1}(x)$ & $p_{0,2}(x)$ \\
		$x_1$ & $f_1=p_{1,0}(x)$ & $p_{1,1}(x)$ &  \\
		\vdots & \vdots & \vdots\\
		$x_n$ & $f_n=p_{n,0}(x)$ &\\
		\end{tabular}
\end{enumerate}

\begin{ex}
	\begin{enumerate}[a)]
		\item 
			Horner-Schema mit Newton-Form
			\[
				p(x) = (1(x-2)+1)(x-1) +2
			\]
			und damit
			\[
				f(4) = 11
			\]
		\item
			Mit dem Neville-Schema
			\begin{tabular}{cccc}
				$x_i$ & $f_i$ &  & \\ \hline
				1 & 2 & 5 & 11\\
				2 & 3 & 9\\
				3 & 6
			\end{tabular}
	\end{enumerate}
\end{ex}

\subsection{Zusammenfassung}

\begin{tabular}{l|c|c}
	 & Lösen des LGS & Punktauswertung\\ \hline
	 Monombasis  & $\mathcal O(n^3)$ & $\mathcal O(n)$ via Horner \\ \hline
	 Lagrange-Basis & $\mathcal O(n)$ & $\mathcal O(n^2)$ \\ \hline
	 Newton-Basis & $\mathcal O(n^2)$ & $\mathcal O(n)$ via Horner
\end{tabular}

\subsection{Anwendung: Richardson-Extrapolation}

Für eine gegebene Funktion $b:(0,\infty)\to \R$ ist $\lim_{h\to 0} b(h)$ gesucht, bzw. eine Approximation.

\begin{seg}{Ansatz:}
Wähle $h_0,\dotsc, h_n \in \R^+$, berechne $b_k := b(h_k)$.
Sei $p(h)$ das Interpolationspolynom zu den Daten $(h_k,b_k)$.

Werte $p(0)$ aus als Approximation von $\lim_{h\to 0} b(h)$.
Nach vorigem Abschnitt b) ist $p(0)$ ohne Interpolationspolynom auswertbar.
Vereinfachung der Rekursion aus \ref{st:1.5} für $x=0$:
\begin{align*}
	p_{i,j}(0) &= \f {(0-h_i)p_{i+1,j-1}(0) - (0-h_{i+j})p_{i,j-1}(0)}{h_{i+j}-h_i} \\
	&= \f {(h_{i+j}-h_i)p_{i+1,j-1}(0)}{h_{i+j}-h_i} - \f {h_{i+j}(p_{i+1,j-1}(0)-p_{i,j-1}(0))}{h_{i+j}-h_i}
	&= p_{i+1,j-1}(0) + \f {p_{i+j,j-1}(0) -p_{i,j-1}(0)}{\f {h_i}{h_{i+j}} - 1}
\end{align*}
\end{seg}

\begin{df}[Richardson-Extrapolation]
	Zu ${h_i}_{i=0}^n \subset \R^+$ definiere
	\begin{align*}
		b_i := b_{i,0} &:= b(h_i)\\
		b_{i,j} &:= b_{i+1,j-1} + \f {b_{i+1,j-1}-b_{i,j-1}}{\f {h_i}{h_{i+j}}-1}
	\end{align*}
	Dann ist $b_{0,n}$ eine Approximation für $\lim_{n\to \infty} b(h)$.
\end{df}

\begin{ex}
	Berechnung von $e=\lim_{n\to \infty} (1 +\f 1n)^n = \lim_{n\to 0} (1+h)^{\f 1n}$.
	D.h. $b(h)=(1+h)^{\f 1h}$.
	Wähle $h_k=2^{-k} \implies b_{k,0}=b(h_k) = (1+2^{-k})^{2^k}$
	\[
		\implies b_{0,0} = 2, \qquad b_{1,0} = \f 94, \qquad b_{2,0} = \f {625}{256} \approx 2,44
	\]
	Neville-Schema für Rekursion aus 1.8, verwende $\f {h_i}{h_{i+j}} = 2^j$.
	\begin{table}[h]
		\centering
		\begin{tabular}{c|ccc}
			$h_0=1$ & $b_{0,0}=2$  & $b_{0,1}=\f 52$ & $b_{0,2}=\f {257}{96}$\\
			$h_1=\f12$ & $b_{1,0}=\f 94$  & $b_{1,1}=\f {337}{128}$ & $b_{0,2}=\f {257}{96}$\\
			$h_2=\f14$ & $b_{2,0}=\f {625}{256}$  \\
		\end{tabular}
	\end{table}
\end{ex}

\begin{note}
	\begin{enumerate}
		\item 
			Die Richardson-Extrapolation lohnt sich insbesondere für Funktionen, deren Auswertung für kleine $h$ teuer ist, z.B. $\mathcal O(\f 1n)$.
	\end{enumerate}
\end{note}

\begin{note}
	Verbesserungsmöglichkeit bei Kenntnis der Form der asymptotischen Entwicklung.
	Falls $b(h) = \sum_{n=0}^\infty a_nh^{qn}$ für $q>1$, dann ist die Interpolation mit Polynomen angebracht, die nur diese Terme $h^{qn}$ enthalten, diese werden anschließend extrapoliert.

	Setze dazu $\_h = h^q$, d.h. $\_{h_{i}}:= h_i^q$ und
	\[
		\_b(\_h) := \sum a_n\_h^n
	\]
	Dann ist $\_b(\_h_i) = b(h_i)$.
	Führe die Richardson-Extrapolation für $\{\_h_i\}$ und $\{\_b(\_h_i)\}$ durch, dannn ist das Ergebnis eine Aproximation für
	\[
		\lim_{\_h\to 0}\_b(\_h) = \lim_{h\to 0}b(h)
	\]
\end{note}

\subsection{Fehleranalysis}

Wir nehmen an, die Daten $\{t_i\}_{i=0}^n$ stammen von Funktionsauswertungen $f_i=f(x_i)$.

Wie gut apporximiert die Interpolierende die Funktion?

\begin{df}
	\label{df:1.9}
	Sei $I\subset \R$ ein abgeschlossenes Intervall.
	Wir definieren
	\[
		C(I) = C^0(I) := \{f: I\to \R : f \text{ stetig}\}
	\]
	als Raum der stetigen Funktionen und
	\[
		C^m(I) := \{f: I\to \R : f,f',\dotsc,f^{(m)} \text{ existieren und stetig}\}
	\]
	als Raum der $m$-mal stetig differenzierbaren Funktionen ($m\in \N$) und
	\[
		C^\infty(I) := \bigcap_{m\in \N}C^m (I)
	\]
	als Raum der unendlich oft stetig differenzierbaren Funktionen.

	Für $f\in C(I)$, $I$ beschränkt, definieren wir die Supremums-Norm
	\[
		\|f\|_\infty := \|f\|_{C(I)} := \sup_{x\in I}|f(x)|
	\]
\end{df}

\begin{note}
	\begin{itemize}
		\item 
			Es gilt $C^\infty(I) \subsetneq \dotsb \subsetneq C^0(I)$.
		\item
			$(C(I), \|\cdot\|_\infty)$ ist normierter Raum für $I$ beschränkt.
		\item
			$(C(I),\|\cdot\|_\infty)$ ist vollständig, d.h. jede Cauchy-Folge konvergiert in $C(I)$.
		\item
			Eine vollständigen, normierten Vektorraum nennen wir Banachraum, $(C(I),\|\cdot\|_\infty)$ Banachraum.
	\end{itemize}
\end{note}

\begin{st}[Punktweise Interpolationsfehler]
	\label{st:1.10}
	Sei $f\in C^{n+1}(\R)$ und $p\in \P_n$ Interpolierende.
	\[
		p(x_i) = f_i \qquad i=0,\dotsc,n \;\land\; x_i\neq x_j \text{ für } i\neq j
	\]
	Dann gilt für alle $x\in \R$
	\[
		p(x) - f(x) = \f 1{(n+1)!}f^{(n+1)}(\xi) (x-x_0)(x-x_1)\dotsb(x-x_1)
	\]
	mit geeignetem $\xi = \xi(x)\in I = \xi \in [\min(x,x_0,\dotsc,x_n),\max(x,x_0,\dotsc,x_n)]$.

	\begin{proof}
		Für $x=x_i$ ist 
		\[
			p(x) - f(x) = \f 1{(n+1)!}f^{(n+1)}(\xi) (x-x_0)(x-x_1)\dotsb(x-x_1)
		\]
		erfüllt ($0=0$).
		Für $x\neq x_i$, $i=0,\dotsc,n$.
		Setze
		\[
			w(x) := \prod_{i=0}^n (x-x_i) \qquad \text{„Knotenpolynom“}
		\]
		und definiere $g(t):= f(t)-p(t)-\f {w(t)}{w(x)}(f(x)-p(x))$.
		
		Es gilt $g(x_i)=0$, $i=0,\dotsc,n$ (wegen $f(x_i)=p(x_i)$) und $g(x)=0$.
		Also hat $g$ mindestens $n+2$verschiedene Nullstellen in $I$.
		Nach dem Satz von Rolle hat $g'$ mindestens $n+1$ verschiedene Nullstellen, usw.
		
		$g^{(n+1)}$ hat mindestens $1$ Nullstelle $\xi\in I$. 

		Mit $p^{(n+1)}(\xi) = 0$, $w^{(n+1)}(\xi) = (n+1)!$ folgt
		\begin{align*}
			0 = g^{(n+1)}(\xi) &= f^{(n+1)}(\xi) - p^{(n+1)}(\xi) - \f {((n+1)!}{w(x)}(f(x)-p(x))\\
			&= f^{(n+1)}(\xi) - 0 - \f {((n+1)!}{w(x)}(f(x)-p(x))\\
		\end{align*}
		Also 
		\[
			f(x) -p(x) = \f 1{(n+1)!}f^{(n+1)}(\xi) w(x)
		\]
	\end{proof}
\end{st}

\begin{kor}[Fehlerschranke]
	\label{kor:1.11}
	Für alle beschränkte Intervalle $I\subset \R$, mit $\{x_0,\dotsc,x_n\}\subset I$ gilt
	\[
		\|p-f\|_\infty \le \f 1{(n+1)!}\|w\|_\infty \|f^{(n+1)}\|_\infty
	\]
	\begin{proof}
		klar
	\end{proof}
\end{kor}

\begin{st}[Gleichmäßige Konvergenz]
	\label{st:1.12}
	Sei $I=[a,b]$, $f\in C^\infty(I)$ mit $\|f^{(n)}\|_\infty \le M$ für alle $n\in \N$.
	Sei $\Delta_n := \{x_0^n,\dotsc, x_n^n\}\subset I$ eine Menge $n$ disjunkter Stützstellen und $p_n\in \P_n$ eine zugehörige Interpolierende.
	Dann gilt
	\[
		\lim_{n\to \infty} \|p_n -f\| = 0
	\]
	\begin{proof}
		Für $w_n(x):=\prod_{i=0}^n(x-x_i^n)$ gilt $|w_n(x)| \le (b-a)^{n+1}$.
		Aus \ref{kor:1.11} folgt
		\[
			\|p_n-f\|_\infty \le \f {\|w_n\|_\infty}{(n+1)!}\|f^{(n+1)}\|_\infty \le \f{(b-a)^{n+1}}{(n+1)!}M \to 0
		\]
	\end{proof}
\end{st}

\begin{note}
	Vergleiche Stoer-Bulirsch.
	\begin{itemize}
		\item
			ohne Einschränkungen an $(\Delta_n)_{n\in \N}$ oder $f\in C(I)$. kann man \emph{keine} gleichmäßige Konvergenz und nicht einmal punktweise Konvergenz erwarten.
			Das Problem ist, dass $f^{(n+1)}(\xi)$ kann schneller mit $n$ wachsen, als $\left(\f {w(x)}{(n+1)!}\right)^{-1}$.	
		\item
			Für jedes $f\in C(I)$ existiert Folge $(\Delta_n)_{n\in \N}$ sodass $p_n\to f$ gleichmäßig.
		\item
			Satz von Faber: Zu jeder Folge $(\Delta_n)_{n\in \N}$ gibt es ein $f\in C([a,b])$ so dass $p_n\not\to f$ gleichmäßig.
		\item
			In der Praxis kommt es zu häufig Oszillationen von $p_n$, vor allem am Rand von $[\min\{x_0,\dotsc, x_n\},\max\{x_0,\dotsc,x_n\}]$.
	\end{itemize}
\end{note}

\begin{ex}[Runge]
	Sei $f(x)= \f 1{1+x^2}$ mit $I=[-5,5]$ und $x_k^n := -5+k*k+h_n$ ($k=0,\dotsc,n$) mit $h_n:= \f {10}n$ äquidistanten Stützstellen.

	Sei $p_n\in \P_n$ mit $p_n(x_k^n) = f(x_k^n)$, dann ergibt sich \fixme[Oszillationen].
	
	Man kann zeigen, dass $\tilde x \approx 3,68$ existiert, so dass in $(-\tilde x,\tilde x)$ punktweise und nicht punktweise Konvergenz für $|x|\ge \tilde x$. Sogar: $\|p_n-f\|_\infty \to \infty$.
\end{ex}

\subsection{Optimale Wahl von Stützstellen}

\begin{seg}{Idee}
Finde Stützstellen $\{x_0,\dotsc,x_n\}\subset I := [-1,1]$, so dass $\|w\|_\infty$ minimal wird.
Dann ist die Schranke in \ref{1.11} am kleinsten..
\end{seg}

\begin{df}[Tschebyscheff-Polynome]
	Wir definieren Tschebyscheff Polynome auf $I$, durch
	\begin{align*}
		T_0(x) &:= 1\\
		T_1(x) &:= x\\
		T_{n+1}(x) &:= 2xT_n - T_{n-1}(x)
	\end{align*}
	und normierte Tschebyscheff Polynome
	\[
		\hat T_n(x) := 2^{1-n} T_n(x)
	\]
\end{df}

\begin{ex}
	\begin{align*}
		T_2(x) &= 2x^2 - 1\\
		T_3(x) &= 4x^3 - 3x\\
		\hat T_2(x) &= x^2 -\f 12\\
		\hat T_3(x) &= x^3 -  \f 34 x
	\end{align*}
\end{ex}

\begin{st}
	\label{1.14}
	Für $n\in \N_0$ gilt
	\begin{enumerate}[i)]
		\item 
			$T_n\in \P_n$
		\item
			Die $\hat T_n$ sind normiert.
		\item
			Für $x\in [-1,1]$ gilt
			\[
				T_n(x) = \cos(n\cos^{-1}(x))
			\]
		\item
			$|T_n(x)| \le 1$
		\item
			$T_n(\cos(\f {j\pi}n)) = (-1)^j$ Extrema für $j=0,\dotsc,n$.
		\item
			$T_n(\cos(\pi \f {2j-1}{2n})) = 0$ Nullstellen für $j=1,\dotsc,n$.
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item klar
			\item klar
			\item
				Ein Additionstheorem liefert
				\[
					\cos(\alpha+\beta) = \cos(\alpha)\cos(\beta) - \sin(\alpha)\sin(\beta)
				\]
				Also
				\begin{align*}
					\cos((n+1)\theta) &= \cos(n\theta) \cos(\theta) - \sin(n\theta) \sin(\theta)\\
					\cos((n-1)\theta) &= \cos(n\theta) \cos(\theta) + \sin(n\theta) \sin(\theta)
				\end{align*}
				und damit
				\[
					\cos((n+1)\theta) + \cos((n-1)\theta) = 2 \cos(n\theta)\cos(\theta)
				\]
				Setze $\theta:= \cos^{-1}(x)$, dann ergibt sich
				\begin{align*}
					F_{n+1}(x) = \cos((n+1)\cos^{-1}(x)) &= 2\cos(n\cos^{-1}(x)) \cos(\cos^{-1}x) - \cos((n-1)\cos^{-1}(x))\\
					&= 2x \underbrace{\cos(n\cos^{-1}(x))}_{F_n(x)} - \underbrace{\cos((n-1)\cos^{-1}(x))}_{F_{n-1}(x)}
				\end{align*}
				d.h. $F_n(x):= \cos(n\cos^{-1}(x))$ erfüllt die Rekursion der Tschebyscheff Polynome aus Definition \ref{1.13}.
				Außerdem  $F_0(x)=1, F_1(x)=x$, also $F_n(x)=T_n(x)$.
			\item
				klar mit iii)
			\item
				Durch Nachrechnen mit iii)
			\item
				Durch Nachrechnen mit iii)
		\end{enumerate}
	\end{proof}
\end{st}

\begin{st}[Optimalität]
	\label{1.15}
	\begin{enumerate}[i)]
		\item 
			Sei $p\in \P_n$ normiert auf $[-1,+1] =: I$, dann gilt
			\[
				\|p\|_\infty \ge 2^{1-n}
			\]
		\item
			$\displaystyle \|\hat T_n \|_\infty = 2^{1-n}$
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[i)]
			\item 
				Wir nehmen an, ein normiertes $p\in \P_n$ mit $|p(x)|<2^{1-n}$ für alle $x\in I$.
				Sei $x_j= \cos(\f {j\pi}{n})$.
				Mit \ref{1.15} und der Definition von $\hat T_n$ folgt
				\[
					(-1)^j p(x_j) \le |p(x_j)| < 2^{1-n} = 2^{1-n}\underbrace{(-1)^j}_{T_n(x_j)}(-1)^j = \hat T(x_j)(-1)^j
				\]
				und
				\[
					(-1)^j(\hat T_n(x_j)-p(x_j)) > 0 \qquad j=0,\dotsc,n
				\]
				also hat $\hat T_n -p$ mindestens $n$ verschiedene Nullstellen. $\hat T_n-p\in \P_{n-1}$, da beide normiert sind, ein Widerspruch.
				Also gilt
				\[
					|p(x)| \ge 2^{1-n} \qquad \forall p\in \P_n \text{ normiert}
				\]
			\item
				aus i) folgt wegen $\hat T_n$ normiert, dass
				\[
					\|\hat T_n\|_\infty \ge 2^{1-n}
				\]
				Mit Satz \ref{1.14} iv) ist $|T_n(x)| \le 1$ also $|\hat T_n(x)| \le 2^{1-n}$ für alle $x\in I$.
				Also 
				\[
					\|\hat T_n\|_\infty = 2^{1-n}
				\]
		\end{enumerate}
	\end{proof}
\end{st}

\section{Numerische Integration}	


\section{Nichtlineare Gleichungsysteme}

\section{Optimierung}



\end{document}

\chapter{Restringierte Optimierung}



\section{Lineare Optimierung}


\subsection{Motivation}

Zur Schweinefütterung stehen zwei Möglichkeiten zur Verfügung:
 Soja zu 1€ pro Einheit, enthält 2 Proteiene und 4 Fett; Kartoffeln zu 2€ pro Einheit, enthält 2 Proteine und 2 Fett.

Futter soll $\ge 10$ Proteine und $\le 12$ Fett besitzen.
Dies führt auf die Minimierungsaufgabe
\[
	x + 2y \to \min!
	\udN
	\begin{cases}
		2x + 2y &\ge 10 \\
		4x + 2y &\le 12 \\
		x &\ge 0 \\
		y &\ge 0
	\end{cases}.
\]

\coursetimestamp{11}{12}{2013}

Jede Ungleichung kann in die Form
\[
	\sum_{j} a_{ij} x_j \le b_i
\]
gebracht werden.
Jede Ungleichung kann in die Form
\begin{align*}
	\sum a_{ij} x_j + \xi &= b_i \\
	\xi &\ge 0
\end{align*}
gebracht werden.
Mit $x_j = x_j' - x_j'', x_j' \ge 0, x_j'' \ge 0$
kann für jede Variable $x_j \ge 0$ gefordert werden.

\begin{df}[Lineares Optimierungsproblem, Normalform] \label{3.1}
	Sei $A \in \R^{n\times n}, b\in \R^m, c \in \R^n$.
	Das Minimierungsproblem
	\[
		f(x) := c^T x \to \min!
		\udN
		Ax = b \land x \ge 0
	\]
	heißt \emph{lineares Optimierungsproblem (engl. linear program) in Normalform}.
	Wir nennen $f$ \emph{Zielfunktion} und die Menge
	\[
		K := \{x \in \R^n : Ax = b \land x \ge 0 \}
	\]
	\emph{zulässiger Bereich}.
\end{df}

\begin{conv*}
	Im Folgenden nutzen wir die Subskript-Notation $x = (x_1, \dotsc, x_n)^T \in \R^n$ für Komponenteneinträge eines Vektors und schreiben für $x_1, \dotsc, x_n \ge 0$ auch kurz $x \ge 0$.
\end{conv*}

\begin{nt} \label{3.2}
	\begin{itemize}
		\item
			Der zulässige Bereich $K$ kann einelementig sein.
			In diesem Fall ist dieses Element die eindeutige Lösung.
		\item
			Auch für nicht-leere zulässige Bereiche muss keine Lösung existieren.
			Betrachte beispielsweise das nach unten unbeschränkte Problem: $n=m=1, A = 0, b=0, c=-1$.
		\item
			Ist der zulässige Bereich $K$ nicht-leer und beschränkt, so existiert eine Lösung.
	\end{itemize}
\end{nt}

\subsection{Geometrische Interpretation}

\begin{df} \label{3.3}
	\begin{enumerate}[(a)]
		\item
			Zu gegebenen Punkten $y^1, \dotsc, y^n \in \R^n$ heißt
			\[
				\sum_{i=1}^m \lambda_i y^i
			\]
			mit $\lambda_i \in [0,1]$ und $\sum_{i=1}^n \lambda_i = 1$ \emph{Konvexkombination}.
			Die Menge aller Konvexkombinationen
			\[
				S(y^1, \dotsc, y^m) := \Set{
					\sum_{i=1}^m \lambda_i y^i |
					\lambda_i \in [0,1], \sum_{i=1}^n \lambda_i = 1
				}
			\]
			heißt \emph{der von $y^1,\dotsc, y^m$ aufgespannte Simplex}.

			\begin{note}
				Damit ist $S(y^1, y^2) = [y^1, y^2]$ die Verbindungsstrecke aus \ref{2.1}.
			\end{note}
		\item
			Für $\emptyset \neq K \subset \R^n$ nennen wir $x \in K$ eine \emph{Ecke von $K$}, falls
			\[
				\forall y^1, y^2 \in \R^n, x \in [y^1, y^2] \subset K : x = y^1 \lor x = y^2.
			\]
	\end{enumerate}
\end{df}

Betrachte stets $A \in \R^{m\times n}, b\in \R^m, c \in \R^n$ und
\[
	K:= \{ x \in \R^n : Ax = b, x \ge 0 \}.
\]
$K$ ist offenbar konvex, d.h. $\forall y^1, y^2 \in K : [y^1, y^2] \subset K$.

\begin{ex} \label{3.4}
	Es gilt $0 \in K \iff b = 0$.
	Für $0 \in K$ ist $x = 0$ Ecke von $K$, denn für alle $y^1, y^2 \in K$ mit
	\[
		0 = (1-\lambda) y^1 + \lambda y^2,
		\qquad \lambda \in [0,1]
	\]
	ist $y^1 = 0$ oder $y^2 = 0$.
\end{ex}

\begin{df} \label{3.5}
	Teilmengen $I \subset \{1, \dotsc, n\}$ nennen wir \emph{Indexmengen} und bezeichnen die Anzahl der Elemente in $I$ mit $|I|$.

	Seien $A = (a_{ij}) \in \R^{m\times n}, v \in \R^n$.
	Für $I \neq \emptyset$ setzen wir
	\begin{align*}
		A_I &:= (a_{ij})_{\substack{i=1,\dotsc,m \\ j \in I}}
			\in \R^{m\times |I|}, \\
		v_I &:= (v_j)_{j\in I}
			\in \R^{|I|}.
	\end{align*}
	Definiere für $x \in \R^n$ eine zugehörige Indexmenge
	\[
		I_x = \Big\{ j \in \{1, \dotsc, n\} : x_j > 0 \Big\}
		\subset \{1, \dotsc, n\}
	\]
	Für $x \neq 0$ schreiben wir auch
	\[
		A_x := A_{I_x}
	\]
	und $v_x$ statt $v_{I_x}$.
	\begin{note}
		Es gilt offenbar für $0 \neq x \in K$
		\[
			b = Ax = A_x x_x.
		\]
		Es gilt auch
		\[
			A_x v_x = A v
		\]
		für alle $v \in \R^n$ mit $I_v \subset I_x$.
	\end{note}
\end{df}

\begin{lem} \label{3.6}
	\begin{enumerate}[(a)]
		\item
			Ist $0 \neq x \in K$ keine Ecke von $K$, so existieren $y^1, y^2 \in K$ mit $y^1 \neq x \neq y^2$ und $y^1 \neq y^2$, so dass
			\[
				x \in [y^1, y^2], \quad I_{y^1}, I_{y^2} \subset I_x.
			\]
		\item
			$0 \neq x \in K$ ist genau dann eine Ecke von $K$, wenn $A_x$ injektiv ist, also $\rg(A_x) = |I_x|$.
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[(a)]
			\item
				Ist $x \in K$ keine Ecke, dann existieren $y^1, y^2 \in K, y^1 \neq y^2$ und für $\lambda \in (0,1)$
				\[
					x = (1-\lambda) y^1 + \lambda y^2.
				\]
				Für jedes $j$ gilt
				\[
					y_j^1 > 0 \implies x_j > 0 \qquad
					y_j^2 > 0 \implies x_j > 0
				\]
				also $I_{y^1}, I_{y^2} \subset I_x$.
			\item
				Außerdem ist
				\[
					A_x (y_x^1 - y_x^2)
					= Ay^1 - Ay^2
					= b - b = 0.
				\]
				Wegen $y^1 \neq y^2 \implies y_x^1 \neq  y_x^2$.
				Damit ist $A_x$ nicht injektiv, wenn $x$ keine Ecke ist.

				Sei $A_x$ nicht injektiv, dann existiert $0 \neq y_x \in \R^{|I_x|}$ mit $A_x y_x = 0$, den wir durch Nullen zu $y \in \R^n$ fortsetzen.
				Wähle $\eps > 0$ so klein, dass $\eps |y_j| < x_j$ für alle $j \in I_x$.
				Setze
				\[
					y^1 := x - \eps y,
					\quad
					y^2 := x + \eps y
				\]
				Offenbar ist $x \in [y^1, y^2]$ und $y^1 \neq x, y^2 \neq x$.
				Aus $Ay = A_x y_x = 0$ folgt
				\[
					Ay^1 = Ax = b = Ax = Ay^2.
				\]
				und wegen der Wahl von $\eps$ ist
				\[
					y^1, y^2 \ge 0.
				\]
		\end{enumerate}
	\end{proof}
\end{lem}

\coursetimestamp{16}{12}{2013}
\begin{kor} \label{3.7}
	Ein Punkt $x \in K$ mit nichtleerer Indexmenge $I_x \subset \{1, \dotsc, n\}$ ist eine Ecke von $K$ genau dann wenn $A_{I_x} \tilde x = b$ mit $\tilde x \in \R^{|I_x|}$ genau eine Lösung mit nicht-negativen Komponenten besitzt.

	In diesem Fall ergibt sich die Ecke $x \in \R^n$ durch Nullfortsetzung aus $\tilde x \in \R^{|I_x|}$.

	Insbesondere existieren nur endlich viele Ecken.
	\begin{proof}
		\begin{segnb}[„$\implies$“]
			Sei also $x \in K$ eine Ecke mit nichtleerer Indexmenge $I_x \subset \{1, \dotsc, n\}$.
			Dann gilt
			\[
				A_{I_x} x_{I_x} = A_x x_x = Ax = b
			\]
			und wegen \ref{3.6} b) ist $x_x$ auch die einzige Lösung von $A_{I_x} \tilde x = b$ mit nichtnegativen Komponenten.
		\end{segnb}
		\begin{segnb}[„$\impliedby$“]
			Ist nun also $A_{I_x} \tilde x = A_x \tilde x = b$ eindeutig lösbar, so ist insbesondere $A_x$ injektiv, besitzt also linear unabhängige Spalten.
			Bezeichne $x \in \R^n$ die Nullfortsetzung von $\tilde x$, dann ist $I_x \subset \{1, \dotsc, n\}$ und wenn die Komponenten von $\tilde x$ nicht-negativ sind, so ist $x \in \R^n$ ein zulässiger Punkt und nach \ref{3.6} b) eine Ecke von $K$.
		\end{segnb}
	\end{proof}
\end{kor}

\begin{lem} \label{3.8}
	\begin{enumerate}[(a)]
		\item
			Jeder Punkt $x \in K$ mit minimaler Indexmenge, d.h. $|I_x| \le |I_y|$ für alle $y \in K$, ist eine Ecke.
			Insbesondere besitzt jeder nicht-leere zulässige Bereich mindestens eine Ecke.
		\item
			Existiert auf dem zulässigen Bereich ein Minimum der Zielfunktion $f(x) = c^T x$, so ist einer der Minimierer eine Ecke.
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[(a)]
			\item
				Ist $0 = x \in K$, so ist $I_x = \emptyset$ und $x = 0$ ist wegen \ref{3.4} auch eine Ecke.

				Sei also $x \neq 0$ mit minimaler Indexmenge, aber $x$ sei \emph{keine} Ecke.
				Nach \ref{3.6} a) existieren dann paarweise von $x$ verschiedene $y^1, y^2 \in K$ mit $x \in [y^1, y^2]$ und $I_{y^1}, I_{y^2} \subset I_x$.

				Betrachte $v := y^2 - y^1$ und für ein $\lambda \in \R$ den Punkt $z_\lambda := x + \lambda v$.
				Dann gilt für alle $\lambda \in \R$
				\[
					A z_\lambda
					= A (x + \lambda v)
					= b + \lambda (b-b)
					= b.
				\]
				Wir wollen nun zeigen, dass ein $\_\lambda$ existiert, sodass $z_{\_\lambda} \in K$ ist (d.h. $z_\lambda \ge 0$) und $|I_{z_\lambda}| < |I_x|$ gilt.

				Da $y^1, y^2$ paarweise verschieden waren, gilt $\emptyset \neq I_v \subset I_x$.
				Für jedes $j \in I_v$ setzen wir
				\[
					\lambda_j
					:= - \f {x_j}{v_j}
					= - \f {x_j}{y^2_j - y^1_j}
					\neq 0
				\]
				und wählen $k \in I_v$, sodass $|\lambda_k| \le |\lambda_j|$ für alle $j \in I_v$.
				Wähle $\_\lambda := \lambda_k$ und $z := z_{\_\lambda} = x + \lambda_k v$.
				$z$ erfüllt
				\begin{itemize}
					\item
						$z_k = x_k + \lambda_k v_k = x_k - x_k = 0$
					\item
						$z_j = x_j + \lambda_k v_j \ge 0$ für $j \in I_v$, denn wäre $z_j$ negativ, so müsste die Funktion $\lambda \mapsto x_j + \lambda v_j$ eine betragskleinere Nullstelle $\lambda_j$ als $\lambda_k$ besitzen.
				\end{itemize}
				Damit ist $z \in K$ zulässig.
				Außerdem gilt für $\lambda_k > 0$ dass $x \in [y^1, z]$ und für $\lambda_k < 0$ dass $x \in [z, y^2]$.
				Da also $x$ keine Ecke von $K$ ist, existieren $y, z \in K, \lambda \in (0,1)$, sodass $x = (1-\lambda) y + \lambda z$ und es gilt $|I_z| < |I_i| \le |I_x|$, ein Widerspruch zu $|I_x| \le |I_y|$ für alle $y \in K$.
			\item
				Sei also $x \in K$ ein Minimierer mit (unter den Minimierern) minimaler Indexmenge, d.h.
				\[
					x \in \argmin_{\xi \in K} c^T \xi
					\land \forall x' \in \argmin_{\xi \in K} c^T \xi : |I_x| \le |I_{x'}|.
				\]
				Falls $x$ \emph{keine} Ecke ist, so existieren wie im a)-Teil wieder $y, z \in K, \lambda \in (0, 1)$ mit $x = (1 - \lambda)y + \lambda z$ und $|I_z| < |I_x|$.
				Noch zu zeigen ist: $z$ ist Minimierer von $f$.

				Da $x$ Minimerer von $f$ ist, gilt $c^Tx \le c^Ty, c^Tx \le c^Tz$.
				Es gilt
				\[
					c^T x = (1-\lambda)c^T y + \lambda c^T z
					\le c^Ty
				\]
				und damit auch $c^T z \le c^T y$.
				Analog zeigt man $c^T y \le c^T z$, also $c^T y = c^T z$.
				Schließlich folgt
				\[
					c^T x = (1-\lambda)c^T y + \lambda c^Tz = c^T y = c^T z.
				\]
				Damit ist auch $z$ Minimierer von $f$ auf $K$, ein Widerspruch zu $|I_z| < |I_x|$.
		\end{enumerate}
	\end{proof}
\end{lem}

\begin{nt} \label{3.9}
	\begin{itemize}
		\item
			Das betrachtete Optimierungsproblem ist damit gelöst.
			Es genügt, für jede (der endlich vielen, höchstens $m$-elementigen) Teilmengen von $\{1, \dotsc, n\}$ gemäß \ref{3.7} zu prüfen, ob es eine dazugehörige Ecke $x \in K$ gibt (für $b = 0$ muss noch $x = 0$ als Ecke hinzugenommen werden).
		\item
			Gibt es einen Minimierer, so ist dies eine der Ecken und es müssen nur noch die Werte der Zielfunktion für die endlich vielen Ecken berechnet und verglichen werden.
		\item
			Falls das Minimierungsproblem lösbar ist, erhalten wir so nach endlich vielen Schritten eine Lösung.
	\end{itemize}
\end{nt}

\coursetimestamp{13}{01}{2014}

\subsection{Das Simplexverfahren}

Im folgenden setzen wir voraus, dass alle Ecken nicht-entartet sind, d.h. dass für alle Ecken $x$ von $K$ gilt, dass $|I_x| = m$.

Wir formulieren das Verfahren direkt auf der Indexmenge.
Zu einer Ecke $x$ bezeichnen wir die Indexmenge $I_x$ mit $B \subset \{1, \dotsc, n\}$ (Basisvariablen) und $N := \{1, \dotsc, n\} \setminus B$ (Nichtbasisvariable).
Entsprechend definieren wir $A_B, A_N, v_B$ und $v_N$ zu $A \in \R^{m\times n}, v \in \R^n$.

Für einen Algorithmus, der sich von Ecke zu Ecke „hangelt“ und dabei das Zielfunktional verbessert sind folgende Dinge nötig:
\begin{enumerate}[1.]
	\item
		Bestimmung einer Startecke (genauer: ein $B$, das zu einer Ecke gehört).
	\item
		Wie kommen wir von einer Ecke zu einer neuen Ecke mit besserem Zielfunktionalswert?
	\item
		Wie erkennen wir eine optimale Ecke?
\end{enumerate}

\subsubsection{Das Stoppkriterium}

Gehöre $B \subset \{1, \dotsc, n\}$ zu einer Ecke.
Nach \ref{3.6} ist $A_B \in \R^{m\times |B|}$ injektiv und dank der vereinfachten Voraussetzung $A_B$ invertierbar.
Jedes $x \in K \subset \R^n$ ist eindeutig festgelegt durch $x_B \in \R^n$ und $x_N \in \R^{n-m}$.
Es gilt
\[
	b = Ax = A_Bx_B + A_Nx_N,
\]
also $x_B = A_B^{-1} (b - A_N x_N)$ für alle $x \in K$.

In der zu $B$ gehörigen Ecke $x \in K$ gilt $x_N = 0, x_B = A_B^{-1}b$ und
\[
	c^T x = c_B^T x_B + c_N^T x_N = c_B^T A_B^{-1}b
\]
in jedem anderen zulässigen Punkt $x \in K$ ist
\begin{align*}
	c^Tx
	&= c_B^T x_B + c_N^T x_N \\
	&= c_B^T A_B^{-1}(b - A_Nx_N) + c_N^T x_N \\
	&= c_B^T A_B^{-1}b + (c_N^T - c_B^T A_B^{-1}A_N) x_N.
\end{align*}
$c_N^T - c_B^T A_B^{-1} A_N$ heißt \emph{Vektor der reduzierten Kosten}.
Für $c_N^T c_B^T A_B^{-1} A_N \ge 0$, dann ist $c^T x \ge c_B^T A_B^{-1} b$, für alle $x \in K$ also die zu $B$ gehörige Ecke optimal.

Demnach ist das Stoppkriterium für das Simplexverfahren die Bedingung $c_N^T - c_B^T A_B^{-1} A_N \ge 0$.

\subsection{Der Pivotschritt}

Es gelte
\[
	c_N^T - c_B^T A_B^{-1} A_N \not\ge 0,
\]
etwa $(c_N^T - c_B^T A_B^{-1} A_N)_j < 0$ für ein $j \in \{1, \dotsc, n-m\}$.
Jedes $x_N = (x_k)_{k\in\N}$ mit $x_k = 0$ für $k \neq j$ und $x_j > 0$ führt mit
\[
	x_B := A_B^{-1}(b - A_N x_N)
\]
zu einem $x \in \R^n$ mit geringerem Zielfunktionalswert $c^T x$ und $Ax = b$ und $x$ ist zulässig genau dann, wenn $x_B \ge 0$.

$x_B$ hängt stetig von $x_j \ge 0$ ab.
Für $x_j = 0$ ist $x_B > 0$, d.h. es existiert $\eps > 0$ mit $x_B \ge 0$ für alle $x_j \in [0, \eps]$.

Entweder bleibt $x_B$ positiv für alle $x_j > 0$, dann ist das Zielfunktional nach unten unbeschränkt.
Ansonsten existiert ein kleinstes $\hat x_j > 0$, sodass $\hat x_B \ge 0$ und für alle $x_j > \hat x_j$ gilt $x_B \not\ge 0$.
Ersetze dann den zugehörigen Index in $B$ (der jetzt 0 geworden ist) durch $j$ und definiere so $B'$.
$B'$ ist damit eine Indexmenge einer Ecke mit geringerem Zielfunktionalswert.

\coursetimestamp{15}{01}{2014}

\paragraph{Implementierung}

Sei \oBdA $N = \{1, \dotsc, n-m \}, B = \{n-m+1, \dotsc, n\}$.
Bezeichne mit $(\hat a_{kl})$ die Einträge von $\hat A := A_B^{-1} A_N \in \R^{m \times (n-m)}$ und mit $\hat b_k$ die Einträge von $\hat b := A_B^{-1}b \in \R^m$.

Für die zu $B$ gehörige Ecke $x$ gilt
\[
	x_N = 0
	\quad\land\quad
	x_B = A_B^{-1} b = \hat b.
\]
Vektor der reduzierten Kosten
\[
	c_N^T - c_B^T A_B^{-1} A_N
\]
mit
\begin{align*}
	c_N &= (c_k)_{k=1}^{n-m}, &
	c_B &= (c_{n-m+k})_{k=1}^{m}
\end{align*}
Der $j$-te Eintrag von $c_n^T - c_B^T A_B^{-1} A_N$ ist
\[
	c_j - \sum_{k=1}^m c_{n-m+k} \hat a_{kj}
\]
also existiert $j \in \{1, \dotsc, n-m \}$ mit
\begin{align*}
	&c_j - \sum_{k=1}^m c_{n-m+k} \hat a_{kj} < 0 \\
	&\qquad \iff c_N^T - c_B^T A_B^{-1} A_N \not\ge 0.
\end{align*}
Sei $j$ ein solcher Index.
Aus $x_B = A_B^{-1} (b - A_N x_N)$ folgt
\[
	x_{n-m+k} = \hat b_k - \sum_{l=1}^{n-m} \hat a_{kl} x_l
\]
für alle $k = 1, \dotsc, m$.
Wähle jetzt $x_l = 0$ für alle $l = 1, \dotsc, j-1, j+1, \dotsc, n-m$, also
\[
	x_{n-m+k} = \hat b_k - \hat a_{kj} x_j.
\]
Da die Ecke zu $B$ zulässig und nicht-entartet war, muss $\hat b = x_B > 0$ gelten.
Falls $\hat a_{kj} \le 0$ für alle $k \in \{1, \dotsc, m\}$, dann ist das Zielfunktional nach unten unbeschränkt.
Sei also $\hat a_{kj} > 0$ für mindestens ein $k \in \{1, \dotsc, m\}$.
Dann gilt
\[
	0 = x_{n-m+k}
	= \hat b_k - \hat a_{kj} x_j
	\quad\iff\quad
	x_j = \f {\hat b_k}{\hat a_{kj}}.
\]
Wähle also gemäß (a)
\[
	\hat k := \argmin_{\substack{k=1,\dotsc, m \\ \hat a_{kj} > 0}} \f {\hat b_k}{\hat a_{kj}}.
\]
und ersetze den in $N$ enthaltenen Index $j$ durch $n - m + \hat k$ und den in $B$ enthaltenen Index $n - m + \hat k$ durch $j$.

\begin{nt} \label{3.10}
	Ohne die Sortierungsannahme ist der $j$-te Eintrag von $c_N^T - c_B^T A_B^{-1} A_N$ gerade
	\[
		c_{N(j)} - \sum_{k=1}^m c_{B(k)} \hat a_{kj},
	\]
	wobei $N(j)$ das $j$-te Element von $N$ und $B(j)$ das $j$-te Element von $B$ bezeichne.

	Die Elemente von $x_B = A_B^{-1} (b - A_N x_N)$ sind
	\[
		x_{B(k)} = \hat b_k - \sum_{l=1}^{n-m} \hat a_{kl} x_{N(l)}.
	\]
	Mit $x_{N(l)} = 0$ für alle $l \neq j$ folgt
	\[
		x_{B(k)} = \hat b_k - \hat a_{kj} x_{N(j)}.
	\]
	Wähle also
	\[
		\hat k := \argmin_{\substack{k=1,\dotsc,m \\ \hat a_{kj > 0}}} \f {\hat b_{k}}{\hat a_{kj}}
	\]
	und ersetze das $\hat k$-te Element von $B$ durch das $j$-te Element von $N$ und vice versa.
\end{nt}

\subsection{Bestimmung einer Startecke}

Sei \oBdA $b \ge 0$ (evtl. durch Negieren der Zeilen von $A$).
Betrachte das Ersatzproblem:
suche $(x^T,y^T)^T \in \R^n \times \R^m$ mit
\begin{align*}
	\begin{pmatrix}
		0 & \cdots & 0 & 1 & \cdots & 1
	\end{pmatrix}
	\begin{pmatrix}
		x \\ y
	\end{pmatrix}
	\to \min! \\
	\udN
	\begin{pmatrix}
		A & I_{m\times m}
	\end{pmatrix}
	\begin{pmatrix}
		x \\ y
	\end{pmatrix}
	= b
	\quad \land \quad
	\begin{pmatrix}
		x \\ y
	\end{pmatrix}
	\ge 0.
\end{align*}
Ist der zulässig Bereich des ursprünglichen Bereichs nicht leer, also $K = \{x \in \R^n : Ax = b, x \ge 0 \} \neq \emptyset$, dann sind die Minimierer des Ersatzproblems genau diese zulässigen Punkte.

Wir setzen voraus, dass auch das Ersatzproblem keine entarteten Ecken besitzt.
$(0, b^T)$ ist Ecke des Ersatzproblems, da er höchstens $m$ Nicht-Null-Einträge enthält.
Die Minimierung des Ersatzproblems liefert
\begin{enumerate}[(a)]
	\item
		eine zulässige Ecke von $k$, falls das Minimum von der Form $(x^T, 0)^T$ ist,
	\item
		oder $K = \emptyset$, falls das Minimum nicht die Form $(x^T, 0)^T$ hat.
\end{enumerate}

\coursetimestamp{20}{01}{2014}

\section{Restringierte nichtlineare Optimierung}


Betrachte
\[
	f(x) \to \min! \udN g(x) \le 0, h(x) = 0.
\]
mit
\begin{itemize}
	\item
		$f: \R^n \to \R$
	\item
		Ungleichungsrestriktionen $g: \R^n \to \R^m$
	\item
		Gleichungsrestriktionen $h: \R^n \to \R^p$
	\item
		$f,g,h$ seine stes stetig differenzierbar
\end{itemize}

\begin{df} \label{3.11}
	Definiere den \emph{zulässigen Bereich} als
	\[
		X = \Set{ x \in \R^n | g(x) \le 0, h(x) = 0 }.
	\]
	Für zulässige Punkte $x \in X$ heißt
	\[
		I_x := \Set{ j \in \{1, \dotsc, m\} | g_j(x) < 0 }
	\]
	die \emph{Indexmenge inaktiver Ungleichungsnebenbedingungen}.
\end{df}

\subsection{Optimalitätsbedingungen}

Die notwendige Optimalitätsbedingung für unrestringierte Probleme war $\nabla f(\hat x) = 0$.

Jedes $d \in \R^n$ mit $\nabla f(\hat x)^T d < 0$ ist eine Abstiegsrichtung und es gilt
\[
	f(\hat x + sd) < f(\hat x)
\]
für hinreichend kleine $s$ (siehe auch \ref{2.20}, \ref{2.21}).
Die Bedingung $\nabla f(\hat x) = 0$ bedeutet also: es gibt keine Abstiegsrichtung.

\paragraph{Tangentialkegel}

\begin{ex} \label{3.12}
	Das Problem
	\[
		f(x) := x \to \min! \udN  x\in \R, x \ge 0
	\]
	besitzt offenbar ein Minimum bei $\hat x = 0$, aber $f'(\hat x) = 1$, d.h. $d = -1$ ist Abstiegsrichtung, ($\hat x + sd$ gehört aber für kein $s > 0$ zum zulässigen Bereich).
\end{ex}

\begin{df} \label{3.13}
	\begin{enumerate}[(a)]
		\item
			Eine Menge $K \subset \R^n$ heißt \emph{Kegel}, falls $\forall x \in K, \lambda \ge 0 : \lambda x \in K$.
		\item
			Für $\emptyset \neq X \subset \R^n$ heißt $d \in \R^n, \|d\| = 1$ \emph{Tangentialrichtung in $x \in X$}, falls eine Folge $(x^k)_{k\in\N}$ existiert mit
			\[
				x^k \to x, x^k \neq x, \lim_{k\to\infty} \f{x^k - x}{\|x^k-x\|} = d
			\]
			Ist $X$ der zulässige Bereich eines Optimierungsproblems, so heißen diese $d$ auch \emph{zulässige Richtungen}.
		\item
			Der von den Tangentialrichtungen aufgespannte Kegel heißt \emph{Tangentialkegel}:
			\[
				T(X, x) := \{ \lambda d : \lambda \ge 0, d \text{ Tangentialrichtung} \} \subset \R^n.
			\]
	\end{enumerate}
	\begin{note}
		In der Literatur wird für einen Kegel auch manchmal die Abgeschlossenheit der Addition gefordert.
	\end{note}
\end{df}

\begin{ex} \label{3.14}
	\begin{enumerate}[(a)]
		\item
			Ist $X$ offen, dann ist für alle $x \in X$, $T(X, x) = \R^n$.
		\item
			Ist $x$ isolierter Punkt von $X$, so ist $T(X, x) = \emptyset$.
		\item
			Auch für Häufungspunkte $x$ einer Menge $X$ kann $T(X, x) = \emptyset$ gelten (Übung).
	\end{enumerate}
\end{ex}

\begin{st} \label{3.15}
	Sei $f: \R^n \to \R$ stetig differenzierbar und $X \subset \R^n$ eine beliebige Menge.
	In jedem lokalen Minimum $\hat x \in X$ von $f$ in $X$ gilt
	\[
		\nabla f(\hat x)^T d \ge 0
	\]
	für alle $d \in T(X, \hat x)$.
	\begin{proof}
		Sei \oBdA $\|d\| = 1$.
		Sei $(x^k)_{k\in\N} \subset X$ mit $x^k \to \hat x, x^k \neq \hat x, \lim_{k\to \infty} \f{x^k-\hat x}{\|x^k - \hat x\|} = d$.

		Für hinreichend große $k$ gilt
		\[
			f(x^k) \ge f(\hat x)
		\]
		und damit
		\begin{align*}
			0 \le \f {f(x^k) - f(\hat x)}{\|x^k - \hat x\|}
			&= \f{\nabla f(\hat x)^T (x^k - \hat x) + o(x^k - \hat x)}{\|x^k - \hat x\|} \\
			&\to \nabla f(\hat x)^T d.
		\end{align*}
	\end{proof}
\end{st}

\paragraph{Der linearisierte Tangentialkegel}

Betrachte wieder
\[
	f(x) \to \min! \udN g(x) \le 0, h(x) = 0.
\]

\begin{df} \label{3.16}
	Zu $x \in x$ definieren wir den linearisierten Tangentialkegel durch
	\[
		T_l(g,h,x) := \{d \in \R^n : \nabla h_i(x)^T d = 0, \nabla g_i(x)^T d \le 0 \forall i \in \{1,\dotsc,p\}, j \not\in I_x \}
	\]
\end{df}

\begin{lem} \label{3.17}
	Es gilt $T(X, x) \subsetneq T_l(g,h,x)$.
\coursetimestamp{22}{01}{2014}
	\begin{proof}
		Es gilt offenbar $\lambda T_l(g,h,x) \subset T_l(g,h,x)$ für alle $\lambda \ge 0$, also ist $T_l(g,h,x)$ ein Kegel.
		Sei $d \in T(X,x)$ und \oBdA (da $T_l(g,h,x)$ Kegel) $\|d\| = 1$.
		Sei $(x^k)_{k\in \N} \subset X$ eine Folge mit
		\begin{align*}
			x^k &\to x, &
			x^k &\neq x, &
			\lim_{k\to\infty} \f{x^k - x}{\|x^k-x\|} &= d.
		\end{align*}
		Es gilt
		\[
			0
			= \f{h_i(x^k) - h_i(x)}{\|x^k - x\|}
			= \f{\nabla h_i(x)^T(x^k-x) + \rho_{h_i}(x^k-x)}{\|x^k-x\|}
			\to \nabla h_i(x)^T d
		\]
		für alle $i = 1, \dotsc, p$.
		\[
			0
			\ge \f{g_j(x^k) - g_j(x)}{\|x^k-x\|}
			\to \nabla g_j(x)^T d
			\qquad j \not\in I_x
		\]
		und damit $d \in T_l(g,h,x)$.

		Für das Gegenbeispiel betrachte \ref{3.18}.
	\end{proof}
\end{lem}

\begin{ex} \label{3.18}
	Betrachte
	\begin{align*}
		X &:= \Set{ x \in \R^2 | -1 \le x_1 \le 1, x_2 = 0 } \\
		&:= \Set{ x \in \R^2 | (x_1 + 1)^3 \ge x_2, x_1 \le 1, x_2 = 0 }.
	\end{align*}
	$X$ lässt sich beschreiben durch
	\[
		g(x) = \begin{pmatrix}
			- x_1 - 1
			x_1 - 1
		\end{pmatrix},
		h(x) = x_2,
	\]
	aber auch durch
	\[
		\tilde g(x) = \begin{pmatrix}
			x_2 - (x_1 + 1)^3 \\
			x_1 - 1
		\end{pmatrix},
		\tilde h(x) = x_2.
	\]
	Für $x \in (-1, 0)^T \in X$ gilt
	\[
		T(X, x) = T_l(g,h,x) \subsetneq T_l(\tilde g, \tilde h, x)
	\]
	\begin{proof}
		Siehe Übung
	\end{proof}
\end{ex}

\paragraph{Constraint Qualifications}

\begin{df} \label{3.19}
	Die Bedingung
	\begin{equation*} \label{acq} \tag{ACQ}
		T_l(g,h,x) = T(X,x)
	\end{equation*}
	heißt \emph{Abadic Constraint Qualification (ACQ)} für $x \in X$.

	Gilt (ACQ) in einem lokalen Minimum $\hat x \in X$, so ist nach \ref{3.15}
	\[
		\nabla f(\hat x)^T d \ge 0
	\]
	für alle $d \in T_l(g,h,x)$.
\end{df}

\begin{df} \label{3.20}
	\begin{enumerate}[(a)]
		\item
			Zu einem Kegel $K \neq \emptyset$ definieren wir den \emph{Polarkegel} durch
			\[
				K^0 := \Set{ v \in \R^n | \forall d \in K : v^T d \le 0 }.
			\]
		\item
			Die Bedingung
			\begin{equation} \label{gcq} \tag{GCQ}
				T_l(g,h,x)^0 = T(X,x)^0
			\end{equation}
			heißt Guignard Constraint Qualification (GCQ).
		\item
			Jede Bedingung, die (GCQ) impliziert, heißt \emph{Constraint Qualification (CQ)}.
	\end{enumerate}
\end{df}

\begin{ex} \label{3.21}
	\begin{enumerate}[(a)]
		\item
			Es gilt $\eqref{acq} \implies \eqref{gcq}$, d.h \eqref{acq} ist eine CQ.
		\item
			Die Forderung
			\[
				\text{$g_i$ konkav für alle $i \not\in I_x$ und $h$ affin linear}
			\]
			ist eine CQ.
		\item
			Ein Punkt $x \in X$ heißt \emph{regulär}, wenn die Vektoren
			\[
				\Big\{ \nabla h_i(x), \nabla g_j(x) : i \in \{1, \dotsc, p\}, j \not\in I_x \Big\} \subset \R^n
			\]
			linear unabhängig sind.

			„$x$ ist regulär“ ist eine CQ.
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[(a)]
			\item
				klar
			\item
				siehe Übung
			\item
				siehe Ulbrich, 16.2 % fixme: ref
		\end{enumerate}
	\end{proof}
\end{ex}

\begin{st} \label{3.22}
	In jedem lokalen Minimum $\hat x \in X$ von $f$ in $X$, das eine CQ erfüllt, gilt
	\[
		\nabla f(\hat x)^T d \ge 0
	\]
	für alle $d \in T_l(g,h,\hat x)$.
	\begin{proof}
		Nach \ref{3.15} gilt $-\nabla f(\hat x)^T d \le 0$ für alle $d \in T(X, \hat x)$.
		Dies ist äquivalent zu
		\[
			-\nabla f(\hat x) \in T(X,\hat x)^0 = T_l (g,h,\hat x)^0
		\]
		und äquivalent zu $\nabla f(\hat x)^T d \ge 0$ für alle $d \in T_l(g, h, \hat x)$.
	\end{proof}
\end{st}

\subsection{Karush-Kuhn-Tucker Bedingungen}


\begin{lem}[Trennungssatz von Hahn-Banach] \label{3.23}
	Sei $\emptyset \neq K \subset \R^n$ abgeschlossen und konvex und sei $x \in K$.
	Dann existieren $\nu \in \R^n$ und $r \in \R$ mit
	\begin{align*}
		\nu^T x &> r, &
		\forall y \in K : \nu^T y \le r.
	\end{align*}
	Ist $K$ ein abgeschlossener, konvexer Kegel, dann gilt die Aussage mit $r = 0$.
	\begin{proof}
		Es existiert $\eta \in K$ mit $\|x - \eta\| = \inf_{y\in K} \|x-y\| > 0$.

		Wähle $\nu := x - \eta$ und $r := \nu^T \eta$.
		Dann ist
		\[
			\nu^T x := \nu^T (x- \eta) + \nu^T \eta = \|\nu\|^2 + r > r.
		\]
		Zeige für $y \in K$ noch $\nu^T y \le r$.
		Da $K$ konvex, ist $[y, \eta] \subset K$.
		Es gilt
		\[
			\|t(y - \eta) - \nu\|^2
			 = \|ty + (1-t)\eta - x \|^2
			 \ge \|\eta - x\|^2
			= \|\nu\|^2
		\]
		und damit
		\[
			t^2 \|y-\eta\|^2 - 2t(y-\eta)^T \nu \ge 0.
		\]
		Für $t \to 0$ folgt $(y-\eta)^T \nu \le 0$, also $y^T \nu \le r$.
	\end{proof}
\end{lem}



